[{"id":0,"href":"/about/","title":"About","section":"Rick's Blog","content":"Email: tianjiawei@gmail.com\n"},{"id":1,"href":"/docs/MySQL/InnoDB/2_source/","title":"2 Source","section":"Inno Db","content":"typora-root-url: ../../../../static\n在MySQL源代码中，每个模块都有自己单独的目录存放，里面按照模块名0子模块名.cc来组织。所有头文件都放在include目录下，同时include目录下还有*.ic的文件，这个文件中存放定义的内联函数。\n如果要在*.ic中使用宏UNIV_INLINE定义内联，需要#include \u0026ldquo;univ.i\u0026rdquo;，即\n#include \u0026#34;univ.i\u0026#34;UNIV_INLINE return_value function foo(param1, ...) ``// 函数声明或实现 { ``... } 注意，这种风格只是适用于c函数，对于ic文件中的类成员函数定义，还是需要手动写inline  univ.i中UNIV_INLINE的宏定义\n#ifndef UNIV_MUST_NOT_INLINE /* Definition for inline version */ #define UNIV_INLINE static inline #else /* !UNIV_MUST_NOT_INLINE *//* If we want to compile a noninlined version we use the following macro definitions: */ #define UNIV_NONINL #define UNIV_INLINE #endif /* !UNIV_MUST_NOT_INLINE */阅读源码层次\n推荐从下至上进行逐层阅读\n最下一层是基础管理模块：\n File Manager主要封装了InnoDB对于文件的各类操作，如读、写、异步I/O等。 Concurrency Manager模块主要封装了引擎内部使用的各类mutex和latch。 Common Utility模块用于一些基本数据结构与算法的定义，如链表、哈希表等。  图中间虚线标注的部分为InnoDB的内核实现，也就是InnoDB存储引擎中的事务、锁、缓冲区、日志、存储管理、资源管理、索引、change buffer模块，这部分是整个存储引擎的核心。\n图最上面的两层是接口层，通过这些接口实现server层与存储引擎的交互。InnoDB存储引擎可以不依赖MySQL数据库，而作为一个嵌入式数据库存在，因此还存在嵌入式的API接口。\n详细的目录（模块）说明如下：\n   目录 说明 文件      ut 基本数据结构和算法 ut0byteut0crc32ut0dbgut0listut0lstut0memut0newut0rbtut0rndut0vecut0wqueue 内存双向链表 ib_list内存双向链表 ut_list   mem 内存管理 mem0mem 内存管理   os 进程控制 univos0atomicos0eventos0fileos0procos0thread 定义了POD、编译器hint、代码段宏Atomic Read-Modify-WriteOS Event getpid、大页分配内存PSI key、进程控制原语   sync 同步机制 ut0mutexsync0arrsync0debugsync0rwsync0syncsync0policy 定义了mutex的宏、mutex_init()和mutex_destroy()、MutexMonitor MutexDebug   log 日志及恢复 log0loglog0recv 重做日志恢复   mtr mini-transaction mtr0typemtr0mtrmtr0logdyn0buf mtr相关定义mtr基本操作mtr日志操作mtr_buf_t   fsp 存储管理 fsp0filefsp0fspfsp0spacefsp0sysspace 数据文件物理文件结构与实现表空间系统表空间   fil 文件管理 fil0filos0file 文件内存数据结构及相关文件操作底层文件操作实现   fut  fut0lstfut0fut 磁盘双向链表 flst   data 逻辑记录 data0datadata0typedata0types 逻辑记录逻辑记录的操作逻辑记录数据结构   rem 物理记录 rem0recrem0cmprem0types 物理记录物理记录的比较物理记录数据结构   page 索引页 page0curpage0pagepage0typespage0sizepage0zip 索引页中记录的定位、插入、删除索引页的维护类型定义   lock 锁 lock0locklock0iterlock0prdtlock0waitlock0typeslock0priv 锁模式   btr B+树 btr0btrbtr0bulkbtr0curbtr0pcurbtr0sea    buf 缓冲池 buf0buddybuf0bufbuf0checksumbuf0dblwrbuf0dumpbuf0flubuf0lrubuf0rea    dict 数据字典 dict0bootdict0creadict0dictdict0loaddict0memdict0statsdict0stats_bg    ibuf change buffer ibuf0ibuf    row  row0extrow0ftsortrow0importrow0insrow0logrow0mergerow0mysqlrow0purgerow0quiescerow0rowrow0selrow0truncrow0uninsrow0umodrow0undorow0updrow0vars    trx 事务 trx0i_strx0purgetrx0rectrx0rolltrx0rsegtrx0systrx0trxtrx0undo    handler  ha_innodbha_innoparthandler0alteri_s    read  read0read    api  api0apiapi0misc    eval  eval0evaleval0misc    ha  ha0haha0storagehash0hash    mach  mach0data    pars  lexyypars0grmpars0lexpars0optpars0parspars0sym    que  que0que    srv  srv0concsrv0monsrv0srvsrv0start    usr  usr0sess    gis  gis0geogis0rtreegis0sea    fts  fts0astfts0blexfts0configfts0ftsfts0optfts0parsfts0pluginfts0quefts0sqlfts0tlex        目录 文件 说明     fut fut0fut File-based utilities   fu0lst File-based list utilities    ha ha0ha The hash table with external chains   ha0storage Hash storage    hash0hash The simple hash table utility    mem mem0mem The memory management   ut ut0byte Byte utilities        ut0crc32 CRC32    ut0dbg Debug utilities for Innobase.    ut0list A double-linked list    ut0mem Memory primitives    ut0new Instrumented memory allocator.    ut0rbt Red-Black tree implementation    ut0rnd Random numbers and hashing    ut0ut Various utilities for Innobase.    ut0vec A vector of pointers to data items    ut0wqueue work queue.          db0err Error Codes\neval\nFile Name What Name Stands For Size Comment Inside File --------- -------------------- ------ ------------------- eval0eval.c Evaluating/Evaluating 17,061 SQL evaluator eval0proc.c Evaluating/Procedures 5,001 Executes SQL procedures The evaluating step is a late part of the process of interpreting an SQL statement \u0026mdash; parsing has already occurred during \\pars (PARSING).\nThe ability to execute SQL stored procedures is an InnoDB feature, but MySQL handles stored procedures in its own way, so the eval0proc.c program is unimportant.\n代码风格 #  InnoDB的代码缩进风格更接近于K\u0026amp;R风格：所有的非函数语句块（if、switch、for、while、do），起始大括号放在行尾，而把结束大括号放在行首。函数开头的左花括号放到最左边。\n此外，每个文件都包含一段简短说明其功能的注释开头，同时每个函数也注释说明函数的功能，需要哪些种类的参数，参数可能值的含义以及用途。最后，对于变量的声明，使用下画线以分隔单词，坚持使用小写，并把大写字母留给宏和枚举常量。\n"},{"id":2,"href":"/docs/MySQL/InnoDB/","title":"Inno Db","section":"MySQL","content":"InnoDB存储引擎 #  从InnoDB的发展历史作为起点，了解InnoDB一路走来的历程。\n然后在全局视角下先了解整体的模块构成及其功能。\n再按照以下的路径深入细节：\n 概览 源码结构 基本数据结构与算法 os records 索引页 storage management 同步机制 缓冲池 数据字典 B+树索引 change buffer lock 事务处理 mini-transaction redo log 服务管理 row  一路下来，应该已经对各个模块的功能和细节了然于心，然后再回头再重温一下整体架构，做到融会贯通。\n"},{"id":3,"href":"/docs/MySQL/Server/8.0_net_optimize/","title":"8.0 Net Optimize","section":"Server","content":"（转载自阿里内核月报）\nAdministrative Connection Management #  如果MySQL连接连接被打满，有时甚至连root也无法登录去kill。\n对于这个问题，目前已有的解法有：\n 各家的线程池提供了extra_port。 Alibaba RDS MySQL的做法是把connection的个数拆分成不同的使用目的，例如系统维护账户占用一部分，用户账户占用一部分，两者不互相影响。  MySQL 8.0提供了administrative-connection-interface的机制（由facebook贡献），即提供单独的network interface并且可以单独配置一个pthread用于listen。\n参数如下：\n admin_address admin_port create_admin_listener_thread：是否创建一个单独的listener线程来监听admin的链接请求（默认OFF，建议打开）  worklog：WL#12138: Add Admin Port\n代码\nMultiple addresses for the –bind-address #  bind-address支持绑定多个网络地址。比如：\n// The server listens on the 198.51.100.20 IPv4 address and the 2001:db8:0:f101::1 IPv6 address. bind_address=198.51.100.20,2001:db8:0:f101::1 worklog：WL#11652: Support multiple addresses for the \u0026ndash;bind-address command option\n代码\nconnect/disconnect performance #  目前MySQL里是使用一个全局大锁（LOCK_thd_list、LOCK_thd_remove）来保护thd_list。\n优化的思路其实很简单直接：分区，将LOCK_thd_list、LOCK_thd_remove根据thread id来分成8个分区（hardcode）来减少冲突，负面影响就是PS的监控数据需要聚合。\nworklog：WL#9250: Split LOCK_thd_list and LOCK_thd_remove mutexes\n代码\ntransfer metadata optional #  MySQL的结果集格式如下，点查下metadata在结果集包中的占比很大。\nRESULTSET contains a bunch of packets: - metadata; - EOF_PACKET if not CLIENT_DEPRECATE_EOF flag set; - data rows; - OK_PACKET (or EOF_PACKET if not CLIENT_DEPRECATE_EOF flag set) or ERR_PACKET MySQL 8.0提供了resultset_metadata，可以有选择的不传输metadata，以提升TPS和吞吐。\nC client driver代码示例\n阿里的同学之前做过相应的对比测试：\nAfter porting twitter's patch ( Great thanks to Davi Arnaut) to MySQL5.6.16, I slightly changed it to make protocol_mode support more options: 0/METADATA_FULL: return all metadata, default value. 1/METADATA_REAL_COLUMN: only column name; 2/METADATA_FAKE_COLUMN: fake column name ,use 1,2...N instead of real column name 3/METADATA_NULL_COLUMN: use NULL to express the metadata information 4/METADATA_IGNORE: ignore metadata information, just for test.. CREATE TABLE `test_meta_impact` ( `abcdefg1` int(11) NOT NULL AUTO_INCREMENT, `abcdefg2` int(11) DEFAULT NULL, `abcdefg3` int(11) DEFAULT NULL, `abcdefg4` int(11) DEFAULT NULL, ...... ...... `abcdefg40` int(11) DEFAULT NULL, PRIMARY KEY (`abcdefg1`) ) ENGINE=InnoDB AUTO_INCREMENT=229361 DEFAULT CHARSET=utf8 mysqlslap --no-defaults -uxx --create-schema=test -h$host -P $port --number-of-queries=1000000000 --concurrency=100 --query='SELECT * FROM test.test_meta_impact where abcdefg1 = 2' METADATA_FULL : 3.48w TPS, Net send 113M METADATA_REAL_COLUMN: 7.2W TPS, Net send 111M METADATA_FAKE_COLUMN: 9.2W TPS , Net send 116M METADATA_NULL_COLUMN: 9.6w TPS , Net send 115M METADATA_IGNORE: 13.8w TPS, Net send 30M worklog：WL#8134: Make metadata information transfer optional\n代码\nMySQL protocol support async #  目前的MySQL C client APIs是synchronous，即在MySQL server处理请求返回完成前，只能等待。MySQL 8.0增加了protocol的异步支持，增加以下异步函数（原有api+_nonblocking）：\n mysql_real_connect_nonblocking mysql_send_query_nonblocking mysql_real_query_nonblocking mysql_store_result_nonblocking mysql_next_result_nonblocking mysql_fetch_row_nonblocking mysql_free_result_nonblocking  worklog：WL#11381: Add asynchronous support into the mysql protocol\n代码\nC client driver代码示例\n"},{"id":4,"href":"/docs/MySQL/Server/8.0_resource_group/","title":"8.0 Resource Group","section":"Server","content":"（转载自阿里内核月报）\nＭySQL8.0增加了一个新功能resource group，可以对不同的用户进行资源控制，例如对用户线程和后台系统线程给予不同的CPU优先级。\n用户可以通过SQL接口创建不同的分组，这些分组可以作为sql的hit，也可以动态的绑定过去。本文主要简单介绍下用法，至于底层如何实现的，其实比较简单：创建的分组被存储到系统表中；在linux系统底层通过CPU_SET来绑定CPU，通过setpriority来设置线程的nice值\nworklog: WL#9467: Resource Groups\n创建resource group #  系统自带两个resource group（不可修改）：\n FOREGROUND (FG) - \u0026ldquo;user\u0026rdquo; threads BACKGROUND (BG) - \u0026ldquo;system\u0026rdquo; threads (internal Engine threads, e.g. \u0026ldquo;purge\u0026quot;in InnoDB, etc.)  mysql\u0026gt; SELECT * FROM INFORMATION_SCHEMA.RESOURCE_GROUPS\\G *************************** 1. row *************************** RESOURCE_GROUP_NAME: USR_default RESOURCE_GROUP_TYPE: USER RESOURCE_GROUP_ENABLED: 1 VCPU_IDS: 0-63 THREAD_PRIORITY: 0 *************************** 2. row *************************** RESOURCE_GROUP_NAME: SYS_default RESOURCE_GROUP_TYPE: SYSTEM RESOURCE_GROUP_ENABLED: 1 VCPU_IDS: 0-63 THREAD_PRIORITY: 0 2 rows in set (0.00 sec) 只有超级账户启动mysqld才能设置thread priority，否则只能降低而不能提升优先级。（-20最高，20最低）\n对于system threads，cpu priority只能从-20 ~ 0，user threads在0 ~ 19之间，这样就保证了系统线程的优先级肯定比用户线程高。\n使用resource group #  有两种方式来使用resource group，一种是SET RESOURCE GROUP，一种是通过SQL HINT的方式。\n设置当前session：\nmysql\u0026gt; SET RESOURCE GROUP test_user_rg; Query OK, 0 rows affected (0.00 sec) 也可以指定hint的方式来设置：\nmysql\u0026gt; select /* + RESOURCE_GROUP(test_user_rg) */ * from sbtest1 where id \u0026lt;10; 还可以通过thread id来设置其他运行中的session，注意这里的thread id不是THD id，而是通过pthread id（performance_schema.threads表）。\nmysql\u0026gt; SELECT THREAD_ID, TYPE FROM performance_schema.threads WHERE PROCESSLIST_ID = 26\\G *************************** 1. row *************************** THREAD_ID: 71 TYPE: FOREGROUND 1 row in set (0.00 sec) mysql\u0026gt; SET RESOURCE GROUP test_user_rg for 71; Query OK, 0 rows affected (0.00 sec) 可以看到，通过resource group，我们可以为任何线程指定不同的计算资源。\n"},{"id":5,"href":"/docs/MySQL/Server/connection_handler/","title":"Connection Handler","section":"Server","content":"概述 #  在MySQL中，对于client发来的请求，其处理流程分为建链和请求处理两部分，这两个阶段分别称为connection phase和command phase。\nMySQL的server-client protocol交互如下：\n从上图中可以看出，connection phase负责连接的建立，而日常的query处理，则称为command phase，command phase的结束，以COM_QUIT query的到来作为标志。\n一般典型的交互过程是connect，query，query，query\u0026hellip; quit，其中query可以是dml、ddl、multi-statement或是prepared statement。\n下面我们先看一下connection phase。\n建链 #  connection phase用于在client-server间建立连接，而建链分为TCP建链和应用建链。\nTCP建链是指TCP socket的listen、accept。\n应用建链是在TCP建链的基础上，通过应用层协议进行认证：server发送handshake（initial handshake）、客户端回username/pwd（handshake response），server回应是否通过认证（OK/Error）。\n我们接下来首先看一下connection phase，即在MySQL中如何处理TCP建链和应用建链的。\nTCP建链 #  TCP连接处理分为两步：\n 初始化，创建conn_mgr和conn_handler，acceptor和listener 监听建链，由acceptor+listener负责 对已建链连接进行线程分发处理，由conn_mgr+conn_handler负责  整体流程如下图所示：\n代码实现\nmysqld_main init_common_variables Connection_handler_manager::init() // 初始化conn_mgr和conn_handler  network_init()\t// 初始化网络 \tset_ports();\t// 设置port  // 初始化acceptor、listener \tMysqld_socket_listener *mysqld_socket_listener= new (std::nothrow) Mysqld_socket_listener(bind_addr_str, mysqld_port, back_log, mysqld_port_timeout, unix_sock_name); Connection_acceptor\u0026lt;Mysqld_socket_listener\u0026gt; *mysqld_socket_acceptor= new (std::nothrow) Connection_acceptor\u0026lt;Mysqld_socket_listener\u0026gt;(mysqld_socket_listener); mysqld_socket_acceptor-\u0026gt;init_connection_acceptor(); ... mysqld_socket_acceptor-\u0026gt;connection_event_loop();\t// 监听、接受、处理连接 监听建链 #  MySQL的连接方式支持多种方式，常见的有：socket、TCP/IP、named_pipe和shared_memory。因为我们一般都在Unix-like系统上编程，所以这里只展开讨论socket，其余连接方式的处理类似。\n连接处理分为分为监听（listen）和接受（accept）两部分：\n listener：Mysqld_socket_listener acceptor：Connection_acceptor  我们先看一下listener\nlistener #  Mysqld_socket_listener用于处理监听（listen）和建链（accept），包括：\n 监听信息 ：ip、port、backlog、socket、socket_map 处理 ：POLL（tcp socket、unix sock file） 状态信息 ：错误（select、accept、tcpwrap产生的状态）  Mysqld_socket_listener\n   方法 说明     ctor/dtor /   setup_listener 建链准备（初始化listen socket，POLL、socket_map）   listen_for_connection_event 建链处理（处理poll/select，accept，创建channel_info）   close_listener 关闭连接（socket_shutdown、socket_close、unlink_socket_file）    acceptor #  Connection_acceptor是一个模板类，根据type展开不同的listener（Mysqld_socket_listener、Named_pipe_listener、Shared_mem_listener），负责将listener TCP监听、建链的连接（channel_info）交给conn_mgr处理。\n核心函数如下：\n/** Connection acceptor loop to accept connections from clients. */ void connection_event_loop() { Connection_handler_manager *mgr= Connection_handler_manager::get_instance(); while (!abort_loop) { Channel_info *channel_info= m_listener-\u0026gt;listen_for_connection_event(); if (channel_info != NULL) mgr-\u0026gt;process_new_connection(channel_info); } } Connection_acceptor\n   方法 说明     ctor/dtor 传入listener   init_connection_acceptor 调用listener-\u0026gt;setup_listener()   connection_event_loop 调用listener监听建链，然后交给conn_mgr处理连接   close_listener 调用listener-\u0026gt;close_listener    LibWrap #  TCP Wrappers作为服务程序安全增强工具，提供 IP 层存取过滤控制，扩展了 inetd (xinetd ) 对服务程序的控制能力，其作用相当于给 xinetd 增加了一道防火墙。最常用的场景如下：通过配置/etc/hosts.allow和/etc/hosts.deny ，以允许或阻止指定客户端对指定服务的访问。\n处理连接 #  经过上面的处理后，用户的建链请求已经经过listen+accept，下面交给conn_mgr+conn_handler。\n在MySQL中，为了支持多种的连接处理方式（单线程only-once、多线程1:1、线程池m:n），通过Connection_handler基类来定义连接处理所需要的函数，具体的处理方式则由子类实现。\nconn_mgr和conn_handler的关系：\nconnection manager #  conn_mgr采用单例模式，进行全局的连接资源管理，这些资源包括：\n conn_handler的创建 将lisenter创建的连接（channel_info）转交给conn_handler 连接相关计数：当前、历史、中止、错误 提供callback接口用于在相关连接线程等待时进行回调  Connection_handler_manager\n   方法 说明     ctor/dtor 由init调用   init 根据thread_handler的配置，实例化conn_handler,调用conn_mgr ctor实例化conn_mgr，注册callbacks   destroy_instance 销毁conn_handler和conn_mgr   get_instance 返回conn_mgr instance   process_new_connection 移交连接   wait_till_no_connection 关闭MySQL时等待连接清零   load_connection_handlerunload_connection_handler 为企业版线程池（plugin）准备的钩子    这里的核心函数为process_new_connection。需要注意一点：channel_info连接信息的所有权转移，由listener转移给conn_handler。\nvoid Connection_handler_manager::process_new_connection(Channel_info* channel_info) { // 连接控制  if (abort_loop || !check_and_incr_conn_count()) { channel_info-\u0026gt;send_error_and_close_channel(ER_CON_COUNT_ERROR, 0, true); delete channel_info; return; } // 转交连接  if (m_connection_handler-\u0026gt;add_connection(channel_info)) { inc_aborted_connects(); delete channel_info; } } conn_mgr的生命周期：初始化和销毁的时机分别为MySQL启动和停止，代码如下：\n// 初始化 mysqld_main init_embedded_erver init_common_variables get_options Connection_handler_manager::init() // 停止 lib_sql.cc end_embedded_server unireg_clear cleanup Connection_handler_manager::destroy_instance(); mysqld.cc mysqld.cc mysqld_main unireg_abort clean_up Connection_handler_manager::destroy_instance(); connection handler #  Connection_handler\n   方法 说明     ctor/dtor /   add_connection 处理连接   get_max_threads 获取conn_handler可以创建的最大线程数    从上面的conn_handler类图可以看到，Connection_handler一共有三个子类，这里主要看Per_thread_connection_handler。\nPer_thread_connection_handler的功能是新起一个线程（handle_connection）1:1处理连接，即进行应用协议的处理。\n请求处理 #  请求分发 #  MySQL请求处理的详细过程如下：\n// 监听socket事件  mysqld_socket_acceptor-\u0026gt;connection_event_loop() { Connection_handler_manager *mgr= Connection_handler_manager::get_instance(); while (!abort_loop) { Channel_info *channel_info= m_listener-\u0026gt;listen_for_connection_event(); if (channel_info != NULL) mgr-\u0026gt;process_new_connection(channel_info); } } Channel_info* Mysqld_socket_listener::listen_for_connection_event() { int retval= poll(\u0026amp;m_poll_info.m_fds[0], m_socket_map.size(), -1); // POLL  int retval= select((int) m_select_info.m_max_used_connection, \u0026amp;m_select_info.m_read_fds, 0, 0, 0); // 或者SELECT  for (uint i= 0; i \u0026lt; m_socket_map.size(); ++i) { if (m_poll_info.m_fds[i].revents \u0026amp; POLLIN) { listen_sock= m_poll_info.m_pfs_fds[i]; is_unix_socket= m_socket_map[listen_sock]; break; } } MYSQL_SOCKET connect_sock; connect_socket= mysql_socket_accept(key_socket_client_connection, listen_sock, (struct sockaddr *)(\u0026amp;cAddr), \u0026amp;length); Channel_info* channel_info= new (std::nothrow) Channel_info_tcpip_socket(connect_sock); return channel_info; } void Connection_handler_manager::process_new_connection(Channel_info* channel_info) { check_and_incr_conn_count(); // 检查max_connections  m_connection_handler-\u0026gt;add_connection(channel_info); } // One_thread_connection_handler 一个线程处理所有连接  // Per_thread_connection_handler 一个线程处理一个连接  bool Per_thread_connection_handler::add_connection(Channel_info* channel_info) { // 检查thread cache是否有空闲  check_idle_thread_and_enqueue_connection(channel_info); // 没有空闲，创建用户线程  mysql_thread_create(key_thread_one_connection, \u0026amp;id, \u0026amp;connection_attrib, handle_connection, (void*) channel_info); } extern \u0026#34;C\u0026#34; void *handle_connection(void *arg) { my_thread_init(); // 线程初始化  for (;;) { THD *thd= init_new_thd(channel_info); // 初始化THD对象  thd_manager-\u0026gt;add_thd(thd); if (thd_prepare_connection(thd)) { // connection phase  lex_start(thd); // 初始化sqlparser  rc= login_connection(thd); check_connection(thd); acl_authenticate(thd, COM_CONNECT); // auth认证  thd-\u0026gt;send_statement_status(); prepare_new_connection_state(thd); // 准备接受QUERY  } else {\t// command phase  while (thd_connection_alive(thd)) // 判活  { if (do_command(thd)) // 处理query sql/sql_parser.c  break; } end_connection(thd); } close_connection(thd, 0, false, false); thd-\u0026gt;release_resources(); // 进入thread cache，等待新连接复用  channel_info= Per_thread_connection_handler::block_until_new_connection(); } my_thread_end(); my_thread_exit(0); } command phase #  具体SQL处理流程：\nbool do_command(THD *thd) { // 新建连接，或者连接没有请求时，会block在这里等待网络读包  NET *net= thd-\u0026gt;get_protocol_classic()-\u0026gt;get_net(); my_net_set_read_timeout(net, thd-\u0026gt;variables.net_wait_timeout); net_new_transaction(net); rc= thd-\u0026gt;get_protocol()-\u0026gt;get_command(\u0026amp;com_data, \u0026amp;command); dispatch_command(thd, \u0026amp;com_data, command); } int Protocol_classic::get_command(COM_DATA *com_data, enum_server_command *cmd) { read_packet(); // 网络读包  my_net_read(\u0026amp;m_thd-\u0026gt;net); raw_packet= m_thd-\u0026gt;net.read_pos; *cmd= (enum enum_server_command) raw_packet[0]; // 获取命令号  parse_packet(com_data, *cmd); } bool dispatch_command(THD *thd, const COM_DATA *com_data, enum enum_server_command command) { switch (command) { case COM_QUERY: alloc_query(thd, com_data-\u0026gt;com_query.query, com_data-\u0026gt;com_query.length); // 从网络读Query并存入thd-\u0026gt;query  mysql_parse(thd, \u0026amp;parser_state); // 解析  } } // sql/sql_parse.cc void mysql_parse(THD *thd, Parser_state *parser_state) { mysql_reset_thd_for_next_command(thd); lex_start(thd); parse_sql(thd, parser_state, NULL); // 解析SQL语句  mysql_execute_command(thd, true); // 执行SQL语句  LEX *const lex= thd-\u0026gt;lex; TABLE_LIST *all_tables= lex-\u0026gt;query_tables; trans_commit_implicit(thd); // 隐式提交 sql/transaction.cc  switch (lex-\u0026gt;sql_command) { case SQLCOM_INSERT: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_DELETE: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_UPDATE: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_SELECT: { res= select_precheck(thd, lex, all_tables, first_table); // 检查privileges  res= execute_sqlcom_select(thd, all_tables); } case SQLCOM_COMMIT: // 显式提交  { trans_commit(thd); ha_commit_trans(thd, TRUE); Transaction_ctx *trn_ctx= thd-\u0026gt;get_transaction(); tc_log-\u0026gt;commit(thd, all)); // MYSQL_BIN_LOG::commit sql/binlog.cc  ordered_commit(thd, all, skip_commit); } ... } } 网络模型 #  MySQL对于网络处理模型做了非常好的抽象分层。\n网络处理模型 #  MySQL对于网络通信的封装层次如下：\n| Channel_info\t连接 | THD\t线程 | Protocol\t应用协议 | NET\t网络缓冲 | VIO\t网络I/O | SOCKET\tsocket fd Channel_info封装了连接信息。\nTHD封装了线程相关的数据结构。\nProtocol封装了应用协议，一共有5种，其中2种最常用，统称为classic protocol：\n PROTOCOL_TEXT：用于plain SQL PROTOCOL_BINARY：用于prepared statement，也称为prepared statement protocol。  NET封装了网络缓冲区，包括buffer、packet、read/write点位。\nVIO封装了网络I/O，包括sockaddr等待。\nSOCKET封装了socket fd，里面只有fd信息。\n这些对象的创建时机如下：\n创建对象的函数调用链如下：\nmysqld_socket_acceptor-\u0026gt;connection_event_loop() Mysqld_socket_listener::listen_for_connection_event() mysql_socket_accept new Channel_info_tcpip_socket // accept \u0026amp; 封装 Channel_Info  Connection_handler_manager::process_new_connection() Per_thread_connection_handler::add_connection // 交给具体的conn_handler处理连接  create pthread(handle_connection) for loop init_new_thd // 初始化VIO, THD, Protocol和VIO  channel_info-\u0026gt;create_thd() delete channel_info thd_manager-\u0026gt;add_thd // 加thd  thd_prepare_connection // connection phase  login_connection while thd_connection_alive // command phase  do_command dispatch_command create_thd() create_and_init_vio mysql_socket_vio_new // malloc，初始化VIO  VIO malloc vio_init THD malloc // malloc，初始化THD，Protocol  thd-\u0026gt;get_protocol_classic()-\u0026gt;init_net // 初始化NET  my_net_init my_net_local_init // 设置net_read_timeout, net_write_timeout 从上面我们可以看出，TCP建链后主线程只封装了Channel_nfo用于存放连接的信息，后续的THD、NET、VIO等信息的创建和初始化都（connections and disconnects）是在用户线程完成的。通过这种方式，主线程可以更高效的accept新的连接请求，从而优化在短连接场景下的性能。\n参见Improving connect/disconnect performance和WL#6606: Offload THD initialization and network initialization to worker thread。\n短连接的性能优化效果如下：\n我们先看一下Channel_info。\nChannel_info 连接 #  Channel_info对象封装了连接信息，以区分处理不同的连接方式：local、TCP/IP、named pipes和shared memory，并负责整个网络模型层次中各个对象的初始化。类和类关系图如下：\nChannel_info_local_socket\nChannel_info_shared_mem\nChannel_info_named_pipe\nChannel_info_tcpip_socket\n属性\n   属性 说明     prior_thr_create_utime 连接的创建时间    方法\n   方法 说明     ctor/dtor /   create_thd 创建THD   create_and_init_vio 创建并初始化VIO（只针对local、TCP/IP）   send_error_and_close_channel 发送错误包并关闭socket   prior_thr_create_utime getter/setter Per_thread_connection_handler::add_connection时设置    THD 线程 #  THD\nProtocol 应用协议 #  Protocol\nNET 网络缓冲 #  NET\nVIO 网络 #  VIO\nSOCKET socket fd #  SOCKET最简单，直接代码说话。\n/** An instrumented socket. */ struct st_mysql_socket { /** The real socket descriptor. */ my_socket fd; /** The instrumentation hook. Note that this hook is not conditionally defined, for binary compatibility of the @c MYSQL_SOCKET interface. */ struct PSI_socket *m_psi; }; /** An instrumented socket. @c MYSQL_SOCKET is a replacement for @c my_socket. */ typedef struct st_mysql_socket MYSQL_SOCKET; thread cache #  pthread复用可以通过thread_cache_size配置：默认值为8 + (max_connections / 100)。\nMySQL提供如下status可以查看thread的数量信息：\n Threads_cached：缓存的 thread数量 Threads_connected：已连接的thread数量 Threads_created：建立的thread数量 Threads_running：running状态的 thread 数量  Threads_created = Threads_cached + Threads_connected\nThreads_running \u0026lt;= Threads_connected\n创建pthread新连接非常消耗资源，特别是在短连接频繁场景下，如果又没有其他组件实现连接池，通过观察Connections/Threads_created的比例，适当提高 thread_cache_size，可以降低新建连接的开销。\nmysql\u0026gt; show status like 'Thread%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 1 | | Threads_connected | 1 | | Threads_created | 2 | | Threads_running | 1 | +-------------------+-------+ 4 rows in set (0.00 sec) auth连接限制 #  除了参数 max_user_connections 限制每个用户的最大连接数，还可以对每个用户制定更细致的限制。以下四个限制保存在mysql.user表中：\n MAX_QUERIES_PER_HOUR 每小时最大请求数（语句数量） MAX_UPDATES_PER_HOUR 每小时最大更新数（更新语句的数量） MAX_CONNECTIONS_PER_HOUR 每小时最大连接数 MAX_USER_CONNECTIONS 这个用户的最大连接数  GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...] resource_option: { | MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count } ALTER USER 'jeffrey'@'localhost' WITH MAX_QUERIES_PER_HOUR 90; 源码分析 #  typedef struct user_resources { uint questions; /* MAX_QUERIES_PER_HOUR */ uint updates; /* MAX_UPDATES_PER_HOUR */ uint conn_per_hour; /* MAX_CONNECTIONS_PER_HOUR */ uint user_conn; /* MAX_USER_CONNECTIONS */ /* Values of this enum and specified_limits member are used by the parser to store which user limits were specified in GRANT statement. */ enum {QUERIES_PER_HOUR= 1, UPDATES_PER_HOUR= 2, CONNECTIONS_PER_HOUR= 4, USER_CONNECTIONS= 8}; uint specified_limits; } USER_RESOURCES; ACL_USER #  ACL_USER 是保存用户认证相关信息的类 USER_RESOURCES 是它的成员属性\nclass ACL_USER :public ACL_ACCESS { public: USER_RESOURCES user_resource; ... } ACl_USER 对象保存在数组 acl_users 中，每次mysqld启动时，从mysql.user表中读取数据，初始化 acl_users，初始化过程在函数 acl_load 中\n调用栈如下：\nmain() mysqld_main() acl_init(opt_noacl); acl_reload(thd); acl_load(thd, tables); USER_CONN #  保存用户资源使用的结构体，建立连接时，调用 get_or_create_user_conn 为 THD 绑定 USER_CONN 对象：\n// 请求第一次处理时 acl_authenticate() if ((acl_user-\u0026gt;user_resource.questions || acl_user-\u0026gt;user_resource.updates || acl_user-\u0026gt;user_resource.conn_per_hour || acl_user-\u0026gt;user_resource.user_conn || global_system_variables.max_user_connections) \u0026amp;\u0026amp; get_or_create_user_conn(thd, (opt_old_style_user_limits ? sctx-\u0026gt;user().str : sctx-\u0026gt;priv_user().str), (opt_old_style_user_limits ? sctx-\u0026gt;host_or_ip().str : sctx-\u0026gt;priv_host().str), \u0026amp;acl_user-\u0026gt;user_resource)) -------\u0026gt; thd-\u0026gt;set_user_connect(uc); 每个用户第一个连接创建时，建立一个新对象，存入 hash_user_connections。\n第二个连接开始，从 hash_user_connections 取出 USER_CONN 对象和 THD 绑定。\n同一个用户的连接，THD 都和同一个 USER_CONN 对象绑定。\ntypedef struct user_conn { /* hash_user_connections hash key: user+host key */ char *user; char *host; /* Total length of the key. */ size_t len; ulonglong reset_utime; uint connections; uint conn_per_hour, updates, questions; USER_RESOURCES user_resources; } USER_CONN; 资源限制在源码中的位置\n   资源名称 函数     MAX_USER_CONNECTIONS check_for_max_user_connections()   MAX_CONNECTIONS_PER_HOUR check_for_max_user_connections()   MAX_QUERIES_PER_HOUR check_mqh()   MAX_UPDATES_PER_HOUR check_mqh()    调用链\nhandle_connection thd_prepare_connection(thd) login_connection check_connection acl_authenticate check_for_max_user_connections do_command dispatch_command mysql_parse check_mqh 权限存储与管理 #  MySQL用户权限信息都存储在以下系统表中，用户权限的创建、修改和回收都会同步更新到系统表中。\n   系统表 存储的权限信息     mysql.user 用户权限   mysql.db 库权限   mysql.tables_priv 表权限   mysql.columns_priv 列权限   mysql.procs_priv 存储过程和UDF的权限信息   mysql.proxies_priv proxy权限    mysql.db存储是库的权限信息，并不存储实例有哪些库（ls查找目录）。  information_schema表的查询接口：\n USER_PRIVILEGES SCHEMA_PRIVILEGES TABLE_PRIVILEGES COLUMN_PRIVILEGES  权限缓存 #  用户在连接数据库的过程中，为了加快权限的验证过程，系统表中的权限会缓存到内存中。\n mysql.user → acl_users mysql.db → acl_dbs mysql.tables_priv和mysql.columns_priv → column_priv_hash mysql.procs_priv → proc_priv_hash和func_priv_hash  另外acl_cache缓存db级别的权限信息。例如执行use db时，会尝试从acl_cache中查找并更新当前数据库权限（thd-\u0026gt;security_ct→db_access）。\n权限更新 #  以grant select on test.t1为例:\n 更新系统表mysql.user，mysql.db，mysql.tables_priv 更新缓存acl_users，acl_dbs，column_priv_hash 清空acl_cache  flush privileges #  重新从系统表中加载权限信息来构建缓存。\nMariaDB Role体系 #  从MairaDB 10.0.5开始，MariaDB开始提供Role（角色）的功能，补全了大家一直吐槽的MySQL不能像 Oracle 一样支持角色定义的功能。\n一个角色就是把一堆的权限捆绑在一起授权，这个功能对于有很多用户拥有相同权限的情况可以显著提高管理效率。在有角色之前，这种情况只能为每个用户都做一大堆的授权操作，或者是给很多个需要相同权限的用户提供同一个账号去使用，这又会导致你要分析用户行为的时候不知道哪个操作是哪个具体用户发起的。\n有了角色，这样的管理就太容易了。例如，可以把权限需求相同的用户赋予同一个角色，只要定义好这个角色的权限就行，要更改这类用户的权限，只需要更改这个角色的权限就可以了，变化会影响到所有这个角色的用户。\n使用方法 #  创建角色需要使用CREATE ROLE语句，删除角色使用DROP ROLE语句。然后再通过GRANT语句给角色增加授权，也可以把角色授权给用户，然后这个角色的权限就会分配给这个用户。同样，REVOKE语句也可以用来移除角色的授权，或者把一个用户移除某个角色。\n一旦用户连接上来，他可以执行SET ROLE语句来把自己切换到某个被授权的角色下，从而使用这个角色的权限。通过CURRENT_ROLE函数可以显示当前用户执行在哪个角色下，没有就是NULL。\n只有直接被授予用户的角色才可以使用SET ROLE语句，间接授予的角色并不能被SET ROLE设置。例如角色B被授予角色A，而角色A被授予用户A，那么用户A只能 SET ROLE 角色A，而不能设置角色B。（角色B-\u0026gt;角色A-\u0026gt;用户A）\n从MariaDB 10.1.1开始，可以利用SET DEFAULT ROLE语句来给某个用户设置默认的角色。当用户链接的时候，会默认使用这个角色，其实就是连接后自动做了一个SET ROLE语句。\n创建一个角色并给他赋权:\nCREATE ROLE journalist; GRANT SHOW DATABASES ON *.* TO journalist; GRANT journalist to hulda; 这里hulda并不马上拥有SHOW DATABASES权限，他还需要先执行一个SET ROLE语句启用这个角色：\n// 一开始只能看到IS库 SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | +--------------------+ // 当前用户没有对应的角色 SELECT CURRENT_ROLE; +--------------+ | CURRENT_ROLE | +--------------+ | NULL | +--------------+ // 启用角色 SET ROLE journalist; SELECT CURRENT_ROLE; +--------------+ | CURRENT_ROLE | +--------------+ | journalist | +--------------+ SHOW DATABASES; +--------------------+ | Database | +--------------------+ | ... | | information_schema | | mysql | | performance_schema | | test | | ... | +--------------------+. SET ROLE NONE; 角色也可以授权给另一个角色（角色累加）：\nCREATE ROLE writer; GRANT SELECT ON db1.* TO writer; GRANT writer TO journalist; 但是只能SET ROLE直接给用户的角色。像这里hulda只能SET ROLE journalist，而不能SET ROLE writer，并且只要启用了journalist角色，hulda也自动获得了writer角色的权限：\nSELECT CURRENT_ROLE; +--------------+ | CURRENT_ROLE | +--------------+ | NULL | +--------------+ SHOW TABLES FROM data; Empty set (0.01 sec) // 启用角色 SET ROLE journalist; SELECT CURRENT_ROLE; +--------------+ | CURRENT_ROLE | +--------------+ | journalist | +--------------+ // 叠加了wirter角色，可以访问db1中的表 SHOW TABLES FROM db1; +------------------------------+ | Tables_in_db1 | +------------------------------+ | set1 | | ... | +------------------------------+ 限制 #  角色和视图、存储过程\n当用户设置启用了一个角色，从某种意义上说他有两个身份的权限集合（用户本身和他的角色）但是一个视图或者存储过程只能有一个定义者。所以，当一个视图或者存储过程通过SQL SECURITY DEFINER创建时，只能指定CURRENT_USER或者CURRENT_ROLE中的一个。所以有些情况下，你创建了一个视图，但是你却可能没法使用它。\nCREATE ROLE r1; GRANT ALL ON db1.* TO r1; GRANT r1 TO foo@localhost; GRANT ALL ON db.* TO foo@localhost; SELECT CURRENT_USER; +---------------+ | current_user | +---------------+ | foo@localhost | +---------------+ SET ROLE r1; CREATE TABLE db1.t1 (i int); CREATE VIEW db.v1 AS SELECT * FROM db1.t1; SHOW CREATE VIEW db.v1; +------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ | View | Create View | character_set_client | collation_connection | +------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ | v1 | CREATE ALGORITHM=UNDEFINED DEFINER=`foo`@`localhost` SQL SECURITY DEFINER VIEW `db`.`v1` AS SELECT `db1`.`t1`.`i` AS `i` from `db1`.`t1` | utf8 | utf8_general_ci | +------+------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ CREATE DEFINER=CURRENT_ROLE VIEW db.v2 AS SELECT * FROM db1.t1; SHOW CREATE VIEW db.b2; +------+-----------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ | View | Create View | character_set_client | collation_connection | +------+-----------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ | v2 | CREATE ALGORITHM=UNDEFINED DEFINER=`r1` SQL SECURITY DEFINER VIEW `db`.`v2` AS select `db1`.`t1`.`a` AS `a` from `db1`.`t1` | utf8 | utf8_general_ci | +------+-----------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+ 相关文件 #  源代码文件按照文章顺序整理：\n源代码文件按照文章顺序整理： sql/conn_handler/named_pipe_connection.h\tNamed_pipe_listener sql/conn_handler/named_pipe_connection.cc\tChannel_info_named_pipe sql/conn_handler/shared_memory_connection.h\tShared_mem_listener sql/conn_handler/shared_memory_connection.cc\tChannel_info_shared_mem sql/conn_handler/socket_connection.h\tMysqld_socket_listener sql/conn_handler/socket_connection.cc\tChannel_info_local_socket Channel_info_tcpip_socket TCP_socket Unix_socket sql/conn_handler/channel_info.h\tChannel_info sql/conn_handler/channel_info.cc sql/conn_handler/connection_acceptor.h\tConnection_acceptor sql/conn_handler/connection_handler_manager.h\tConnection_handler_manager sql/conn_handler/connection_handler_manager.cc sql/conn_handler/connection_handler.h\tConnection_handler sql/conn_handler/connection_handler_impl.h\tPer_thread_connection_handler One_thread_connection_handler sql/conn_handler/plugin_connection_handler.h\tPlugin_connection_handler sql/conn_handler/connection_handler_one_thread.cc sql/conn_handler/connection_handler_per_thread.cc "},{"id":6,"href":"/docs/MySQL/Server/invisible_index/","title":"Invisible Index","section":"Server","content":"是否使用索引对性能影响很大。在数据库运维过程中，索引的删除总是慎重的，一方面是因为实际的DDL变更代价，一方面是评估对性能的影响。\n为了减少删除索引的代价，MySQL 8.0引入了invisible index，用于解决这个问题。\n即在索引上增加一个invisible的属性，用于optimizer可以将其忽略掉。\n从这里也可以看到，引入的带动在server层，不涉及到存储引擎，所以所有的存储引擎都适用。\nMySQL的实现基本参考的是Oracle 12c。\nMySQL 8.0.0 Release Notes\n Optimizer Notes  MySQL now supports invisible indexes. An invisible index is not used by the optimizer at all, but is otherwise maintained normally. Indexes are visible by default. Invisible indexes make it possible to test the effect of removing an index on query performance, without making a destructive change that must be undone should the index turn out to be required. This feature applies to InnoDB tables, for indexes other than primary keys. To control whether an index is invisible explicitly for a new index, use a VISIBLE orINVISIBLE keyword as part of the index definition for CREATE TABLE, CREATE INDEX, or ALTER TABLE. To alter the invisibility of an existing index, use a VISIBLE or INVISIBLE keyword with the ALTER TABLE ... ALTER INDEX operation. For more information, see Invisible Indexes.     MySQL 8.0.1 Release Notes\n Optimizer Notes  Previously, invisible indexes were supported only for the InnoDB storage engine. Invisible indexes are now storage engine neutral (supported for any engine). (Bug #23541244)   Bugs Fixed  Index hints applied to invisible indexes produced no error. (Bug #24660093, Bug #82960)     MySQL 8.0.11 Release Notes\n Bugs Fixed  Row-based replication used the wrong set of indexes on the slave. (Bug #88847, Bug #27244826)     WorkLog:\n WL#8697 Support for INVISIBLE indexes WL#10891: optimizer_switch to see invisible indexes  MySQL Server团队、PERCONA和阿里数据库团队的解读\n MySQL 8.0: Invisible Indexes Thoughts on MySQL 8.0 Invisible Indexes AliSQL · 特性介绍 · 支持 Invisible Indexes  其中背后的思想是以下两点：\n soft delete staged rollout  应用场景主要有：\n 线上的产品库数据规模上不同于线下库，数据访问方式可能不同，这对于索引评估至关重要 可以立即看到效果（ALTER TABLE algorithm = replace） 只有某个会话想用到某个索引 使用FORCE INDEX的风险：如果没有索引命中，可能会导致全表扫描的高昂代价  在MySQL 8.0中元信息不再存储在frm里，而在backport到5.7时需要处理frm，阿里的做法是复用了一个不存在frm里的flag HA_SORT_ALLOWS_SAME，我们是采用了一个D flag来标记。\nbackport changes：\n1. 增加配置项 reset_frm_visibility_enabled sql/mysqld.h sql/mysqld.cc sql/sys_vars.cc 在开启enable下扫描解析frm sql/mysqld.cc 2. frm操作 增加 sql/sql_reset_index_visibility.h sql/sql_reset_index_visibility.cc 修改 sql/CMakeLists.txt 扫描：find_all_frms reset（兼容性）：reset_frm_index [128] == D set to d 只扫描除了mysql/performance_schema/sys之外的目录 系统目录全集：https://dev.mysql.com/doc/refman/5.7/en/data-directory.html 3. 词法 sql/lex.h VISIBLE INVISIBLE 4. 语法 sql/share/ ER_PK_INDEX_CANT_BE_INVISIBLE sql/sql_yacc.yy sql/sql_alter.h sql/sql_alter.cc sql/sql_table.cc alter_commands { THD *thd= YYTHD; LEX *lex= thd-\u0026gt;lex; if (!lex-\u0026gt;m_sql_cmd) { /* Create a generic ALTER TABLE statment. */ lex-\u0026gt;m_sql_cmd= new (thd-\u0026gt;mem_root) Sql_cmd_alter_table(); if (lex-\u0026gt;m_sql_cmd == NULL) MYSQL_YYABORT; } } alter_commands alter_command_list alter_list alter_list_item keywords VISIBLE_SYM INVISIBLE_SYM key_visibility %type \u0026lt;NONE\u0026gt; key_visibility END_OF_INPUT key_visibility: VISIBLE_SYM { Lex-\u0026gt;key_create_info.is_visible= true; } | INVISIBLE_SYM { Lex-\u0026gt;key_create_info.is_visible= false; } key algo normal_key_options: /* empty */ {} | normal_key_opts | key_visibility ; fulltext_key_options: /* empty */ {} | fulltext_key_opts | key_visibility ; spatial_key_options: /* empty */ {} | spatial_key_opts | key_visibility ; ALTER TABLE ALTER INDEX VISIBLE/INVISIBLE | ALTER INDEX_SYM ident key_visibility { LEX *lex= Lex; Alter_index_visibility *aiv= new Alter_index_visibility($3.str, lex-\u0026gt;key_create_info.is_visible); if (aiv == NULL) MYSQL_YYABORT; lex-\u0026gt;alter_info.alter_index_visibility_list.push_back(aiv); lex-\u0026gt;alter_info.flags|= Alter_info::ALTER_INDEX_VISIBILITY; } 增加Alter_index_visibility类，并在Alloc_Info中配置为成员变量, ctor reset() 并实现ctor PK不能设置invisible create index add_create_index() 5. key_create_info增加is_visible属性 sql/handler.h bool is_visible sql/handler.cc default_key_create_info 6. 在alter table时设置new_key.is_visible，把old_key，new_key放入index_altered_visibility_buffer的KEY_PAIR + Alter_inplace_info.add_altered_index_visibility() +uint index_altered_visibility_count +KEY_PAIR *index_altered_visibility_buffer mysql_alter_table fill_alter_inplace_info 7. 读取frm文件中的key visible flag sql/unireg.cc pack_keys_visib mysql_create_frm sql/table.cc open_binary_frm 8. 在表元信息中增加变量和方法 在TABLE_SHARE中增加key_map visible_indexes bit(1) 当开启thd.OPTIMIZER_SWITCH_USE_INVISIBLE_INDEXES，返回交集 +TABLE_SHARE::usable_indexes sql/table.h sql/sql_tmp_table.cc sql/table.cc 9. SQL优化器调用usable_indexes TABLE_LIST::process_index_hints 赋值、判断 setup_ftfuncs Item_func_match::fix_index() Rows_log_event::decide_row_lookup_algorithm_and_key() search_key_in_table sql/table.cc sql/item_func.h sql/item_func.cc sql/log_event.cc storage/myisam/ha_myisam.cc 更新setup_ftfuncs的调用，增加thd参数 sql_command接入 sql/sql_base.h sql/sql_base.cc sql/sql_update.cc sql/sql_delete.cc sql/sql_resolver.cc setup_ftfuncs fix_index 10. key增加is_visible属性 标记是否对query optimizer可见 OPTIMIZER_SWITCH_USE_INVISIBLE_INDEXES show create table show table status mysql_prepare_create_table +check_promoted_index sql/sql_const.h sql/key.h sql/sql_show.cc sql/sql_table.cc 11. DDL mysql_alter_Table fill_alter_inplace_info 分配ha_alter_info-\u0026gt;index_altered_visibility_buffer 主键不允许设置invisible index mysql_prepare_alter_table 12. 增加优化器优化开关 optimizer_switch_names https://github.com/xiaoboluo768/qianjinliangfang/wiki/optimizer_switch sql/sys_vars.cc "},{"id":7,"href":"/docs/MySQL/Server/io_cache/","title":"Io Cache","section":"Server","content":"设计理念：\n 对于碎片化的IO会减少实际文件IO读写 写缓冲：对于写，可以合并IO操作，并在写缓冲满时自动调用write写入文件 读缓冲：对于读，多余读出来的数据可以给下次读使用 对于文件设备支持字符设备和块设备 支持多种IO模式，比如缓冲IO/direct IO/aio 对文件操作进行了抽象：4k对齐（按照块粒度读取IO效率最高），缓冲封装在文件操作内部  IO_CACHE初始化\n// 1. seek 如果文件不支持seek，则seek_offset要求为0 // 2. 设置最小对齐块min_cache（16k/8k） // 3. 缩小cachesize：READ_CACHE/SEQ_READ_APPEND下，如果读取范围足够小，则重置cachesize为该范围，并关闭aio // 4. cachesize按照min_cache对齐，如果为SEQ_READ_APPEND，则申请2块，malloc如果失败则缩小3/4重试(128k-\u0026gt;96k)，少于等于min_cache则报错返回  int init_io_cache_ext(IO_CACHE *info, File file, size_t cachesize, enum cache_type type, my_off_t seek_offset, pbool use_async_io, myf cache_myflags, PSI_file_key file_key) { size_t min_cache; my_off_t pos; my_off_t end_of_file= ~(my_off_t) 0; info-\u0026gt;file= file; info-\u0026gt;file_key= file_key; info-\u0026gt;type= TYPE_NOT_SET; // 直到创建append_buffer_lock后才设置type  info-\u0026gt;pos_in_file= seek_offset; info-\u0026gt;pre_close = info-\u0026gt;pre_read = info-\u0026gt;post_read = 0; info-\u0026gt;arg = 0; info-\u0026gt;alloced_buffer = 0; info-\u0026gt;buffer=0; info-\u0026gt;seek_not_done= 0; if (file \u0026gt;= 0) { pos= mysql_file_tell(file, MYF(0)); // 定位文件位置  if ((pos == (my_off_t) -1) \u0026amp;\u0026amp; (my_errno() == ESPIPE)) // 文件不支持seek，再试一次，如果还不行就失败，这种情况下传入的seek_offset要求为0  { info-\u0026gt;seek_not_done= 0; DBUG_ASSERT(seek_offset == 0); } else info-\u0026gt;seek_not_done= MY_TEST(seek_offset != pos); } info-\u0026gt;disk_writes= 0; info-\u0026gt;share=0; min_cache=use_async_io ? IO_SIZE*4 : IO_SIZE*2; // 设置最小对齐块（aio使用16k，否则使用8k）  if (type == READ_CACHE || type == SEQ_READ_APPEND) { // 如果读取范围足够小（end-seek），则将cachesize设小一点，并且不使用aio  if (!(cache_myflags \u0026amp; MY_DONT_CHECK_FILESIZE)) // 假设文件不会增长  { end_of_file= mysql_file_seek(file, 0L, MY_SEEK_END, MYF(0));// 计算文件大小(end)  /* Need to reset seek_not_done now that we just did a seek. */ info-\u0026gt;seek_not_done= end_of_file == seek_offset ? 0 : 1; // 文件大小和查找位置不相同，设置为seek_not_done  if (end_of_file \u0026lt; seek_offset) // 查找位置 \u0026gt; 文件大小，end = seek  end_of_file=seek_offset; if ((my_off_t) cachesize \u0026gt; end_of_file-seek_offset+IO_SIZE*2-1) { cachesize= (size_t) (end_of_file-seek_offset)+IO_SIZE*2-1; use_async_io=0; /* No need to use async */ } } } cache_myflags \u0026amp;= ~MY_DONT_CHECK_FILESIZE; if (type != READ_NET \u0026amp;\u0026amp; type != WRITE_NET) { cachesize= ((cachesize + min_cache-1) \u0026amp; ~(min_cache-1)); // cachesize按照min_cache对齐  for (;;) { size_t buffer_block; /* Unset MY_WAIT_IF_FULL bit if it is set, to prevent conflict with MY_ZEROFILL. */ myf flags= (myf) (cache_myflags \u0026amp; ~(MY_WME | MY_WAIT_IF_FULL)); if (cachesize \u0026lt; min_cache) cachesize = min_cache; // 最小为一个min_cache  buffer_block= cachesize; if (type == SEQ_READ_APPEND) // 申请2块  buffer_block *= 2; if (cachesize == min_cache) flags|= (myf) MY_WME; if ((info-\u0026gt;buffer= (uchar*) my_malloc(key_memory_IO_CACHE, buffer_block, flags)) != 0) { info-\u0026gt;write_buffer=info-\u0026gt;buffer; if (type == SEQ_READ_APPEND) // write_buffer指向第二块  info-\u0026gt;write_buffer = info-\u0026gt;buffer + cachesize; info-\u0026gt;alloced_buffer=1; // 标记已malloc  break; } if (cachesize == min_cache) DBUG_RETURN(2); // 最小申请容量为min_cache，如果还失败就报错返回  cachesize= (cachesize*3/4 \u0026amp; ~(min_cache-1)); // 如果malloc失败，缩小cachesize到原大小的3/4，直到申请成功  } } info-\u0026gt;read_length=info-\u0026gt;buffer_length=cachesize; info-\u0026gt;myflags=cache_myflags \u0026amp; ~(MY_NABP | MY_FNABP); info-\u0026gt;request_pos= info-\u0026gt;read_pos= info-\u0026gt;write_pos = info-\u0026gt;buffer; if (type == SEQ_READ_APPEND) { info-\u0026gt;append_read_pos = info-\u0026gt;write_pos = info-\u0026gt;write_buffer; info-\u0026gt;write_end = info-\u0026gt;write_buffer + info-\u0026gt;buffer_length; mysql_mutex_init(key_IO_CACHE_append_buffer_lock, \u0026amp;info-\u0026gt;append_buffer_lock, MY_MUTEX_INIT_FAST); } if (type == WRITE_CACHE) info-\u0026gt;write_end= info-\u0026gt;buffer+info-\u0026gt;buffer_length- (seek_offset \u0026amp; (IO_SIZE-1)); else info-\u0026gt;read_end=info-\u0026gt;buffer; // 初始状态下, 读缓冲为空  /* End_of_file may be changed by user later */ info-\u0026gt;end_of_file= end_of_file; info-\u0026gt;error=0; info-\u0026gt;type= type; init_functions(info); // 初始化读写函数read_func/write_func  DBUG_RETURN(0); } static void init_functions(IO_CACHE* info) { enum cache_type type= info-\u0026gt;type; switch (type) { case READ_NET: break; case SEQ_READ_APPEND: info-\u0026gt;read_function = _my_b_seq_read; info-\u0026gt;write_function = 0; // 若使用出core  break; default: info-\u0026gt;read_function = info-\u0026gt;share ? _my_b_read_r : _my_b_read; info-\u0026gt;write_function = _my_b_write; } setup_io_cache(info); } IO_CACHE读取\n读取流程： 1. 现从读缓冲中读取所有缓冲，拷贝到用户缓冲区 2. 计算文件读写位置：实际读取位置为 pos_infile + buffer，并计算该位置相对于4k对齐的余量（余数diff） 3. 如果读取大于8k，从实际文件中读取（只读取到4k对齐的位置，余量交由#4进行），并直接拷贝到用户缓冲区 4. 从文件读取\u0026lt;=缓冲区大小的块（1. 未4k对齐的，读取4k-diff大小 2. 如果文件剩余大小小于缓冲区，一次性读取全部内容），放入读缓冲中 5. 从读缓冲中拷贝用户需要的剩余字节给用户缓冲区，并推进read_pos和pos_in_file int _my_b_read[IO_CACHE *info, uchar *Buffer, size_t Count) { size_t length; size_t diff_length; size_t left_length; size_t max_length; my_off_t pos_in_file; /* If the buffer is not empty yet, copy what is available. */ if ((left_length= (size_t) (info-\u0026gt;read_end-info-\u0026gt;read_pos))) // 如果读缓冲有数据可读（end-pos），则将现有缓冲区的数据拷贝给用户  { memcpy(Buffer,info-\u0026gt;read_pos, left_length); Buffer+=left_length; Count-=left_length; } pos_in_file=info-\u0026gt;pos_in_file+ (size_t) (info-\u0026gt;read_end - info-\u0026gt;buffer); 计算读取的位置 pos = 文件pos - 读缓冲长度，以保证文件pos始终和读缓冲起始点对齐 // 通过设置seek_not_done设为1暗示在IO_CACHE上进行过针对磁盘的flush/write操作，所以需要重新定位  if (info-\u0026gt;seek_not_done) { if ((mysql_file_seek(info-\u0026gt;file, pos_in_file, MY_SEEK_SET, MYF(0)) != MY_FILEPOS_ERROR)) { info-\u0026gt;seek_not_done= 0; //如果没有错误，设置为seek已完成  } else { // 如果seek失败，且错误号为ESPIPE，则意味着文件为pipe/socket/FIFO，不再重试，直接返回错误  DBUG_ASSERT(my_errno() != ESPIPE); info-\u0026gt;error= -1; DBUG_RETURN(1); } } diff_length= (size_t) (pos_in_file \u0026amp; (IO_SIZE-1)); // 如果之前读取到一半中断过，则找到IO_SIZE对齐后的多余字节(diff)  // 如果要读取的大小(Count+diff)超过2个IO_SIZE，读取文件，并且不填充读缓冲  if (Count \u0026gt;= (size_t) (IO_SIZE+(IO_SIZE-diff_length))) { /* Fill first intern buffer */ size_t read_length; if (info-\u0026gt;end_of_file \u0026lt;= pos_in_file) // 如果读取点在文件尾部之外，则返回错误，并通过error告知读取到的部分内容  { info-\u0026gt;error= (int) left_length; DBUG_RETURN(1); } // 计算按照IO_SIZE对齐后需要读取的大小（减去多余字节），进行实际的文件读取，并进行4k对齐  length=(Count \u0026amp; (size_t) ~(IO_SIZE-1))-diff_length; if ((read_length= mysql_file_read(info-\u0026gt;file, Buffer, length, info-\u0026gt;myflags)) != length) { /* If we didn\u0026#39;t get, what we wanted, we either return -1 for a read error, or (it\u0026#39;s end of file), how much we got in total. */ // 返回-1或者部分读到的数据  info-\u0026gt;error= (read_length == (size_t) -1 ? -1 : (int) (read_length+left_length)); DBUG_RETURN(1); } Count-=length; Buffer+=length; pos_in_file+=length; left_length+=length; diff_length=0; } // 填充读缓冲  // 计算除4k对齐外还需要读取的字节数 = 读缓冲长度-对齐外剩余字节（diff）  max_length= info-\u0026gt;read_length-diff_length; // 如果文件剩余长度小于读缓冲，则读取文件所有剩余长度  if (info-\u0026gt;type != READ_FIFO \u0026amp;\u0026amp; max_length \u0026gt; (info-\u0026gt;end_of_file - pos_in_file)) max_length= (size_t) (info-\u0026gt;end_of_file - pos_in_file); // 如果文件无剩余内容可读，则返回错误  if (!max_length) { if (Count) { /* We couldn\u0026#39;t fulfil the request. Return, how much we got. */ info-\u0026gt;error= (int)left_length; DBUG_RETURN(1); } length=0; } else if ((length= mysql_file_read(info-\u0026gt;file, info-\u0026gt;buffer, max_length, info-\u0026gt;myflags)) \u0026lt; Count || length == (size_t) -1) // 遇到读取错误，没有读到所需的大小（Count）或者读到EOF（-1）  { /* We got an read error, or less than requested (end of file). If not a read error, copy, what we got. */ if (length != (size_t) -1) // 遇到读取错误，将已读到的内容拷贝给用户Buffer  memcpy(Buffer, info-\u0026gt;buffer, length); info-\u0026gt;pos_in_file= pos_in_file; // 设置文件偏移量  info-\u0026gt;error= length == (size_t) -1 ? -1 : (int) (length+left_length); // error设置为-1（EOF）或已读到的字节数  info-\u0026gt;read_pos=info-\u0026gt;read_end=info-\u0026gt;buffer; // 读缓冲重置为空  DBUG_RETURN(1); } info-\u0026gt;read_pos=info-\u0026gt;buffer+Count; info-\u0026gt;read_end=info-\u0026gt;buffer+length; info-\u0026gt;pos_in_file=pos_in_file; memcpy(Buffer, info-\u0026gt;buffer, Count); DBUG_RETURN(0); } "},{"id":8,"href":"/docs/MySQL/Server/log/","title":"Log","section":"Server","content":"日志 #  MySQL server层日志有：\n general log slow log error log  日志的存储方式（log_output）有：\n 文件 表  所有日志的配置参数如下：\nlog_output general log，slow log存放在何处（文件/表） log_timestamps general log, slow log, error log写入文件中的时间戳所用时区（UTC/SYSTEM），写入日志表中的不在此列。 general_log 是否开启general log sql_log_off 当前session是否关闭general log general_log_file general log文件名 slow_query_log 是否开启slow log slow_query_log_file slow log文件名 long_query_time slow query的判断时间（秒） log_queries_not_using_indexes 是否记录没有使用index的query log_throttle_queries_not_using_indexes log_queries_not_using_indexes配额（每分钟） log_slow_admin_statements 是否记录慢管理语句（ALTER TABLE, ANALYZE TABLE, CHECK TABLE, CREATE INDEX, DROP INDEX, OPTIMIZE TABLE, and REPAIR TABLE） log_error error log文件名 log_error_verbosity error, warning, and note messages log_warnings deprecated，用log_error_verbosity替代 log_syslog log_syslog_facility log_syslog_include_pid log_syslog_tag log-slow-slave-statements 记录从库上回放时的慢SQL（SBR/MIXED） mysqld startup options --log-isam[=file_name] 记录所有MyISAM的变化（debug） --log-raw 记录query rewrite plugin改写前的SQL 日志实现机制 #  general log和slow log #  实现 #  如下图所示：\nlogger生命周期 #  // mysqld启动 query_logger.set_handlers(LOG_NONE | LOG_FILE | LOG_NONE); // 设置log event handler list query_logger.init(); // new log event handler // 运行时 query_logger.general_log_write // 写general log log_slow_statement // 写slow log log_slow_do // 记录query rewrite plugin改写前的SQL query_logger.slow_log_write // mysqld关闭 query_logger.cleanup(); 日志格式 #  general log日志格式\nthd-\u0026gt;current_utime() thd-\u0026gt;thread_id() enum_server_command thd-\u0026gt;query().str slow log日志格式\n# Time: thd-\u0026gt;current_utime() # User@Host: sctx-\u0026gt;priv_user().str [thd-\u0026gt;security_context()-\u0026gt;user()] @ thd-\u0026gt;security_context()-\u0026gt;host() [thd-\u0026gt;security_context()-\u0026gt;ip()] Id: thd-\u0026gt;thread_id() # Query_time: current_utime - thd-\u0026gt;start_utime Lock_time: thd-\u0026gt;utime_after_lock - thd-\u0026gt;start_utime Rows_sent: thd-\u0026gt;get_sent_row_count() Rows_examined: thd-\u0026gt;get_examined_row_count() use thd-\u0026gt;db().str SET timestamp=thd-\u0026gt;current_utime() thd-\u0026gt;query().str thd-\u0026gt;utime_after_lock由thd_storage_lock_wait设置  示例\ngeneral log 2021-05-27T10:16:16.864244+08:00 154 Query select now() slow log # Time: 2021-03-23T10:41:29.249524+08:00 # User@Host: root[root] @ localhost [] Id: 34 # Query_time: 11.409186 Lock_time: 0.000117 Rows_sent: 11 Rows_examined: 7499929 use device_manager; SET timestamp=1616467289; select distinct cmd_param_key from device_task_info; slow log细节 #  slow_log 是在语句执行完后记录的，因为加锁时间和返回记录数这些信息，在执行之后才知道，general_log 记录是在语句解析完执行前。\n判断query是否记入slow_log的函数为log_slow_applicable()：\n warn_no_index：没有用到索引，并且log_queries_not_using_indexes打开（这种情况下还可能触发 throttle 导致不记录） log_this_query：被标记为慢SQL：(thd-\u0026gt;server_status \u0026amp; SERVER_QUERY_WAS_SLOW) 或者 warn_no_index 或者扫描行数大于min_examined_row_limit  log_slow_applicable()\nbool warn_no_index= ((thd-\u0026gt;server_status \u0026amp; (SERVER_QUERY_NO_INDEX_USED | SERVER_QUERY_NO_GOOD_INDEX_USED)) \u0026amp;\u0026amp; opt_log_queries_not_using_indexes \u0026amp;\u0026amp; !(sql_command_flags[thd-\u0026gt;lex-\u0026gt;sql_command] \u0026amp; CF_STATUS_COMMAND)); bool log_this_query= ((thd-\u0026gt;server_status \u0026amp; SERVER_QUERY_WAS_SLOW) || warn_no_index) \u0026amp;\u0026amp; (thd-\u0026gt;get_examined_row_count() \u0026gt;= thd-\u0026gt;variables.min_examined_row_limit); 在SQL执行结束时，会先调用thd-\u0026gt;update_server_status()判断是否是慢SQL，如果判断为真则将server_status |= SERVER_QUERY_WAS_SLOW，逻辑如下：\n/** Update server status after execution of a top level statement. Currently only checks if a query was slow, and assigns the status accordingly. Evaluate the current time, and if it exceeds the long-query-time setting, mark the query as slow. */ void update_server_status() { ulonglong end_utime_of_query= current_utime(); if (end_utime_of_query \u0026gt; utime_after_lock + variables.long_query_time) server_status|= SERVER_QUERY_WAS_SLOW; } utime_after_lock表示的是拿到锁的时间点，server层通过THD::set_time_after_lock()设置，引擎层（InnoDB）如果发生锁等待，会通过thd_storage_lock_wait()累加。因此一个SQL如果一直处于等待中而没有执行完，是不会记入慢SQL的。\nquery_time和lock_time的计算逻辑如下：\nQuery_logger::slow_log_write if (thd-\u0026gt;start_utime) { query_utime= (current_utime - thd-\u0026gt;start_utime); lock_utime= (thd-\u0026gt;utime_after_lock - thd-\u0026gt;start_utime); } 同理，rows_sent通过THD::inc_sent_row_count()一直累加。而rows_examined会在累加后由JOIN::exec()重置。\nJOIN::exec() { ``... ``/* ``Initialize examined rows here because the values from all join parts ``must be accumulated in examined_row_count. Hence every join ``iteration must count from zero. ``*/ ``examined_rows= 0; ``... } 我们go through了一遍，从上面我们可以得知，参与判断为慢SQL的变量有：\n thd-\u0026gt;server_status  thd-\u0026gt;utime_after_lock - thd-\u0026gt;start_utime   SQL类型为CF_STATUS_COMMAND m_examined_row_count  在用户SQL的执行中，会在每个SQL语句执行前，把server_status, start_utime, m_sent_rows_count重置，调用链如下：\nmysql_parse() THD::reset_for_next_command() error log #  实现 #  // mysqld使用 sql_print_error sql_print_warning sql_print_information { va_list args; DBUG_ENTER(\u0026quot;sql_print_warning\u0026quot;); va_start(args, format); error_log_print(INFORMATION_LEVEL, format, args); va_end(args); DBUG_VOID_RETURN; } // plugin使用 my_plugin_log_message { va_start(args, format); my_snprintf(format2, sizeof (format2) - 1, \u0026quot;Plugin %.*s reported: '%s'\u0026quot;, (int) plugin-\u0026gt;name.length, plugin-\u0026gt;name.str, format); error_log_print(lvl, format2, args); va_end(args); } 二者都是调用error_log_print，然后通过buffer IO将错误信息写入文件或者syslog。\n从上面可以看出，mysql的所有日志都是buffer IO，其中general log，slow log通过IO_CACHE缓冲，error log通过std::ostringstream缓冲，然后在某些时刻调用flush_error_log_messages()进行flush。  "},{"id":9,"href":"/docs/MySQL/Server/lower_case_table_names/","title":"Lower Case Table Names","section":"Server","content":"在MySQL中，表是和操作系统中的文件对应的，而文件名在有的操作系统下是区分大小写的（比如linux），有的是不区分大小写（比如Windows），表名与文件名的大小写对应关系，MySQL 是通过lower_case_table_names这个变量来控制的。\n这个变量的有效取值是0，1，2，按照官方文档 的解释：\n 0：表在文件系统存储的时候，对应的文件名是按建表时指定的大小写存的，MySQL 内部对表名的比较也是区分大小写的 1：表在文件系统存储的时候，对应的文件名都小写的，MySQL 内部对表名的比较是转成小写的，即不区分大小写 2：表在文件系统存储的时候，对应的文件名是按建表时指定的大小写存的，但是 MySQL 内部对表名的比较是转成小写的，即不区分大小写  0适用于区分大小写的系统，1都适用，2适用于不区分大小写的系统。\n如果在开始使用MySQL选定了一个合适的值后，就不要改变，不然的话在之后使用中就会出现问题。\nMGR用因为要根据db+table+pk生成write_set，还有一个bug fix。\n"},{"id":10,"href":"/docs/MySQL/Server/mariadb_maxscale_proxy_protocol/","title":"Mariadb Maxscale Proxy Protocol","section":"Server","content":"（转载自阿里内核月报）\nMariaDB MaxScale 是一款数据库中间件，可以按照语义分发语句到多个数据库服务器上。它对应用程序透明，可以提供读写分离、负载均衡和高可用服务。\nproxy protocol 是 MaxScale 引入，为了解决使用proxy作为中间件连接mysql时，mysql获取client ip用 client通过proxy中间价连接mysql时，mysql server接收到认证报文中，包含的是proxy节点的IP。如果mysql.user表中指定了client的IP，无法通过认证。\n一种传统处理方式是在proxy节点上保存client的ip和密码，用于client的认证，而在mysql server上使用另一套ip和密码用于proxy 节点到mysql server的认证。\nMariaDB MaxScale引入了proxy protocol来解决client ip透传的问题。proxy节点获取到client ip后，把client ip包装在proxy protocol报文中发给server，server解析到了proxy protocol报文，再用报文中的ip替换proxy节点的ip\n因为 proxy protocol 本身不包含鉴权，同时可能影响数据库的鉴权行为，所以在数据库 server 端设置了一个ip白名单，只有白名单ip发送的 proxy protocol 报文才会被接受。\n报文格式 #  proxy protocol 有两个版本 v1 和 v2 通过报文签名区分\nv1 #  v1 报文是字符串形式 “PROXY %s %s %s %d %d”\n   字段 内容     签名 PROXY   %s.1 address family   %s.2 client address   %s.3 server address   %d.1 client port   %d.2 server port    v2 #     字段 内容     报文签名12字节 \\x0D\\x0A\\x0D\\x0A\\x00\\x0D\\x0A\\x51\\x55\\x49\\x54\\x0A   ver_cmd 1 字节 version 高四位 0x2 « 4， command 0x1(PROXY) 0x0 (LOCAL)   family 1 字节 IPV4 0x11，IPV6 0x21，AF_UNIX 0x21   length 2字节 addr 部分的数据长度，IPV4 12， IPV6 36， AF_UNIX 216   addr 以IPV4为例 client地址4字节，server地址4字节，client port 2字节， server port 2字节    客户端改造 #  MaxScale 使用场景下，Proxy Protocol Header 由 MaxScale 发送，这里暂不分析\nMariaDB 为了测试，也改造了 client，可以通过 mysql_option 设置 Proxy Protocol\n使用方法：在mysql_real_connect之前，调用 mysql_option\nmysql_optionsv(mysql, MARIADB_OPT_PROXY_HEADER, header_data, header_lengths);\n第一个参数是 MYSQL 句柄，第二个参数是 PROXY_HEADER 标志位，第三个参数是 Proxy Protocol Header，第四个参数是 Header length\nProxy Protocol Header 按照上述规则传入即可\n/* st_mysql_options_extension 结构体对应 mysql-\u0026gt;options-\u0026gt;extension */ struct st_mysql_options_extension { ... /* */ char *proxy_header; size_t proxy_header_len; } int mysql_optionsv(MYSQL *mysql,enum mysql_option option, ...) { ... switch (option) { case MARIADB_OPT_PROXY_HEADER: { size_t arg2 = va_arg(ap, size_t); /* 把 proxy_header 和 proxy_header_len 保存到extension中 */ OPT_SET_EXTENDED_VALUE(\u0026amp;mysql-\u0026gt;options, proxy_header, (char *)arg1); OPT_SET_EXTENDED_VALUE(\u0026amp;mysql-\u0026gt;options, proxy_header_len, arg2); } break; } } /* mysql_real_connect 过程中发送 proxy header 发送时机在建连后，接收挑战码之前 这样在鉴权时可以使用 proxy header 中保存的 ip port */ MYSQL *mthd_my_real_connect(MYSQL *mysql, const char *host, const char *user, const char *passwd, const char *db, uint port, const char *unix_socket, unsigned long client_flag) { ... /* 如果extension中保存了 proxy_header，则把header发送给server */ if (mysql-\u0026gt;options.extension \u0026amp;\u0026amp; mysql-\u0026gt;options.extension-\u0026gt;proxy_header) { char *hdr = mysql-\u0026gt;options.extension-\u0026gt;proxy_header; size_t len = mysql-\u0026gt;options.extension-\u0026gt;proxy_header_len; if (ma_pvio_write(pvio, (unsigned char *)hdr, len) \u0026lt;= 0) { ma_pvio_close(pvio); goto error; } } } 服务端改造\n我们知道，server 通过对比自己的 net-\u0026gt;pkt_nr 和 client 发送过来的 net-\u0026gt;pkt_nr 来保证包没有乱序。而proxy header 没有包含正确的 pkt_nr，就可以通过判断 pkt_nr 乱序来识别 proxy header。\n对于一对client和server线程，pkt_nr是共享的。\n假设 server 发送的第一个包 pkt_nr == 1，下一个包是 client 回包 pkt_nr == 2，那么server再发给client 的包 pkt_nr == 3 以此类推。\n/* 正常的 mysql 都包含4字节 header，前三字节是size，第四字节是 pkt_nr */ my_bool my_net_write(NET *net, const uchar *packet, size_t len) { ... /* 如果大于最大包长度，则分成多个包发送 */ while (len \u0026gt;= MAX_PACKET_LENGTH) { const ulong z_size = MAX_PACKET_LENGTH; /* 前三个字节保存size */ int3store(buff, z_size); /* 第四字节保存pkt_nr */ buff[3]= (uchar) net-\u0026gt;pkt_nr++; if (net_write_buff(net, buff, NET_HEADER_SIZE) || net_write_buff(net, packet, z_size)) { MYSQL_NET_WRITE_DONE(1); return 1; } packet += z_size; len-= z_size; } /* Write last packet */ int3store(buff,len); buff[3]= (uchar) net-\u0026gt;pkt_nr++; ... } /* my_real_read 里面处理 proxy header */ static ulong my_real_read(NET *net, size_t *complen, my_bool header __attribute__((unused))) { retry: /* 收到的包不满足 pkt_nr 约束，可能是 proxy header，跳转到 packets_out_of_order */ if (net-\u0026gt;buff[net-\u0026gt;where_b + 3] != (uchar) net-\u0026gt;pkt_nr) goto packets_out_of_order; packets_out_of_order: /* 判断是否是 proxy protocol header 判断标准是报文前几位是否符合 proxy protocol header 的 signature 前面已经提到 v1 signature 五字节：PROXY v2 signature 12字节：\\x0D\\x0A\\x0D\\x0A\\x00\\x0D\\x0A\\x51\\x55\\x49\\x54\\x0A */ if (has_proxy_protocol_header(net) \u0026amp;\u0026amp; net-\u0026gt;thd \u0026amp;\u0026amp; ((THD *)net-\u0026gt;thd)-\u0026gt;get_command() == COM_CONNECT) { /* Proxy information found in the first 4 bytes received so far. Read and parse proxy header , change peer ip address and port in THD. */ /* 处理 proxy header，修改 THD 中的 ip port */ if (handle_proxy_header(net)) { /* error happened, message is already written. */ len= packet_error; goto end; } /* proxy protocol header 处理成功，跳到函数头重新读取 */ goto retry; } static int handle_proxy_header(NET *net) { /* 判断 proxy protocol header 的来源 ip 是否在白名单中 白名单是一个全局变量 proxy_protocol_networks 白名单 ip 以逗号分隔保存在全局变量中 */ if (!is_proxy_protocol_allowed((sockaddr *)\u0026amp;(thd-\u0026gt;net.vio-\u0026gt;remote))) { /* proxy-protocol-networks variable needs to be set to allow this remote address */ my_printf_error(ER_HOST_NOT_PRIVILEGED, \u0026#34;Proxy header is not accepted from %s\u0026#34;, MYF(0), thd-\u0026gt;main_security_ctx.ip); return 1; } /* 根据格式解析 proxy protocol header，格式在上文中已做介绍 解析结果保存在 peer_info 中 */ if (parse_proxy_protocol_header(net, \u0026amp;peer_info)) { /* Failed to parse proxy header*/ my_printf_error(ER_UNKNOWN_ERROR, \u0026#34;Failed to parse proxy header\u0026#34;, MYF(0)); return 1; } /* Change peer address in THD and ACL structures.*/ /* 将 peer_info 中的信息转存到 thd 中 */ return thd_set_peer_addr(thd, \u0026amp;(peer_info.peer_addr), NULL, peer_info.port, false); } "},{"id":11,"href":"/docs/MySQL/Server/mdl/","title":"Mdl","section":"Server","content":"历史 #  为了解这个著名的bug#989：\nDML和DDL如果并发执行，binlog序错乱：主库并发执行了DDL（T → T'）和DML（T’），但是提交到了binlog中顺序是DDL+DML，这样在从库上回放时，先执行DDL，后执行DML，但此时表结构已经变为T‘，DML执行失败  因此，在MySQL 5.5中，引入了MDL（Meta Data Lock）来控制DDL和DML的并发，保证元数据的一致性。\n设计思理：\n WL#3726：DDL locking for all metadata objects WL#4284：Transactional DDL locking  谈到MDL，需要先谈谈MySQL的锁定机制设计以及thr_lock\nMySQL locking #  为了保证数据的一致性和完整性，数据库系统普遍存在封锁机制，而封锁机制的优劣直接关系到数据库系统的并发处理能力和性能，所以封锁机制的实现也成为了各种数据库的核心技术之一。\nMySQL的封锁机制有三种：\n row-level locking：InnoDB、NDB Cluster table-level locking：MyISAM、MEMORY、MERGE、CSV等非事务存储引擎 page-level locking：BerkeleyDB  MySQL采用如此多样的封锁机制是由其产品定位和发展历史共同决定的。首先，MySQL的产品定位是通过plugin机制可以接入多个存储引擎。在早期的存储引擎（MyISAM和MEMORY）设计中，设计原则建立在\u0026quot;任何表在同一时刻都只允许单个线程（无论读写）对其访问\u0026quot;之上。随后，MySQL3.23修正了之前的假设：MyISAM支持Concurrent Insert：如果没有hole，可以多个读线程并发读同一张表，同时多个写线程以队列的形式进行尾部insert。之后，BerkeleyDB和InnoDB的引入也挑战了之前的设计假设，要求page-level、和row-level locking。此时，之前的设计方式已经和存储引擎所提供的能力不相衬了。因此MySQL做出了改变，允许存储引擎自己改变MySQL 通过接口传入的锁定类型（也就是上面的3种）而自行决定该怎样封锁数据。\nMySQL加锁的顺序\nSQL → open table → 加MDL锁 → 加表锁 → 加InnoDB锁 → SQL执行 → 释放MDL锁 → 释放表锁 → 释放InnoDB锁\n其中表锁是通过thr_lock提供的，在MySQL 5.7.5中，thr_lock被MDL替换：\nScalability for InnoDB tables was improved by avoiding THR_LOCK locks. As a result of this change, DML statements for InnoDB tables that previously waited for a THR_LOCK lock will wait for a metadata lock:\n Explicitly or implicitly started transactions that update any table (transactional or nontransactional) will block and be blocked by LOCK TABLES \u0026hellip; READ for that table. This is similar to how LOCK TABLES \u0026hellip; WRITE works. Tables that are implicitly locked by LOCK TABLES now will be locked using metadata locks rather than THR_LOCK locks (for InnoDB tables), and locked using metadata locks in addition to THR_LOCK locks (for all other storage engines). Implicit locks occur for underlying tables of a locked view, tables used by triggers for a locked table, or tables used by stored programs called from such views and triggers. Multiple-table updates now will block and be blocked by concurrent LOCK TABLES \u0026hellip; READ statements on any table in the update, even if the table is used only for reading. HANDLER \u0026hellip; READ for any storage engine will block and be blocked by a concurrent LOCK TABLES \u0026hellip; WRITE, but now using a metadata lock rather than a THR_LOCK lock.   相关WorkLog：WL#6671: Improve scalability by not using thr_lock.c locks for InnoDB tables\nTable Locking #  MySQL的表级锁定最开始使用thr_lock，主要分为两种类型，一种是读锁定，另一种是写锁定。table lock模块为每个表维护了四个队列来表示这两种锁定：\n granted read/write waiting read/write  如下： • Current read-lock queue (lock-\u0026gt;read) • Pending read-lock queue (lock-\u0026gt;read_wait) • Current write-lock queue (lock-\u0026gt;write) • Pending write-lock queue (lock-\u0026gt;write_wait)\n对于DQL，锁类型一般是TL_READ；对于普通的DML，是TL_WRITE_ALLOW_WRITE\n代码路径：\nlock_tables mysql_lock_tables thr_multi_lock thr_lock MDL locking #  MDL Locking通过多层次、多粒度的locking，在满足一致性和完整性的前提下保证并发性能。\n锁的获取 #  等待中的锁，按照锁优先级编排锁的获取。默认写者优先，但读者在配置了max_write_lock_count（一般不会出现）后，可以优先处理。\nMDL锁是依次申请的（one by one），并进行死锁检测：\n DML：按照语句中table的出现顺序获取 DDL：按照table的字母序申请  比如\nRENAME TABLE tbla TO tbld, tblc TO tbla; // 按照a-\u0026gt;c-\u0026gt;d顺序申请MDL锁（name order） RENAME TABLE tbla TO tblb, tblc TO tbla; // 按照a-\u0026gt;b-\u0026gt;c顺序申请MDL锁（name order） 这样可以保证表按同序申请 但如果DML、DDL并发执行，则先后顺序有差异，见官方文档中的示例。\n锁的释放 #  为了保证事务的可序列化，需要保证保证DML和DDL的互斥，MDL锁只能在事务提交后释放（S2PL）。\n锁的状态变更 #  观测\n可以通过performance_schema.metadata_locks查看MDL信息\nUPDATE performance_schema.setup_consumers SET ENABLED = 'YES' WHERE NAME ='global_instrumentation'; UPDATE performance_schema.setup_instruments SET ENABLED = 'YES' WHERE NAME ='wait/lock/metadata/sql/mdl'; select* from performance_schema.metadata_locks MDL状态转换如下图所示：\n死锁检测 #  造成死锁是因为：\n 访问共享资源 多个线程访问 互斥冲突采用等待策略 不能按同一固定顺序请求锁  从当前线程开始，转换到全局锁列表，然后在深度遍历，当wait_for和当前THD相同时，形成环；然后递归返回时，通过权重确定victim。\nctx::find_deadlock while (1) { Deadlock_detection_visitor dvisitor(this); // ctx  ctx::visit_subgraph(); 当前线程是否有锁等待MDL_context::m_waiting_for，有的话就沿着ticket搜下去，没有就退出。 ticket-\u0026gt;accept_visitor 搜索视角的转换，从 MDL_context 经过 MDL_ticket 进入到 MDL_lock lock-\u0026gt;visit_subgraph 核心逻辑 } visit_subgraph\n先给搜索深度加1，然后判断是否超过最大搜索深度（MAX_SEARCH_DEPTH= 32），超过就无条件认为有死锁，退出； 遍历当前锁的ticket链表，看ticket对应的线程是否和死锁检测的发起线程是同一个，如果是则说明有回路，退出（相当于做了一层的广度搜索）；\n从头开始遍历当前锁的ticket链表，对每个ticket对应的线程，递归调用MDL_context::visit_subgraph（深度搜索）。 整个死锁检测逻辑是一个加了深度限制的深搜，中间同时多了一层广搜。\nDeadlock_detection_visitor 是死锁检测中重要的辅助类，主要负责：\n 记录死锁检测的起始线程 记录被选做victim的线程 在检测到死锁，深搜一层层退出的时候，会依次检查回路上各线程的死锁权重，选择权重最小的做为最终的victim（权重由锁的类型决定）  死锁权重：\n DDL：100 USER_LEVEL_LOCK：50 DML：0  即优先回滚DDL\nMDL子系统 #  锁子系统的核心功能：\n 由用户侧提交锁请求 提供一个中心化的锁管理（锁对象、granted、waiting、停等通知、死锁检测） 通过ticket关联用户和锁对象  锁子系统交互 #  在MDL子系统中，体现在THD和MDL锁子系统的交互上，如下图所示：加锁是用户线程（THD）向MDL子系统申请并获得对应锁的ticket的过程，加锁成功标志是MDL模块返回一个对应的ticket，大致逻辑如下：\n 解析SQL语句，根据语义对每一个表对象设置TABLE_LIST.mdl_request，如对普通的select语句 TABLE_lsit.mdl_request.type 就是MDL_SHARED_READ（st_select_lex::set_lock_for_tables()） 用户线程在打开每个表之前，会请求和这个表对应的MDL锁，通过 thd→mdl_context.acquire_lock()接口将mdl_request请求发给MDL子系统 MDL模块根据请求类型和已有的锁来判断请求能否满足，如果可以就返回一个ticket；如果不可以就等待，等待结果可以是成功（别的线程释放了阻塞的MDL锁）、失败（超时、连接被kill或者被死锁检测选为victim） 用户线程根据MDL模块的返回结果，决定继续往下走还是报错退出。  DL锁并不是对表加锁，而是在加表锁前的一个预检查，如果能拿到MDL锁，下一步加相应的表锁。  MDL子系统内部加锁流程\nMDL子系统内部类关系图\n生命周期 #  整个锁系统各个对象的生命周期如下：\n数据结构 #  MDL_key 锁对象标识 #  Metadata lock object key用于标识MDL对象，由三元组组成：namespace、db、table，作为全局锁表（MDL_map）、锁请求（MDL_request）、锁对象（MDL_lock）的成员变量。\n其数据结构（POD）如下：ctor namespace, db, name *MDL_key reset 置空 is_equal bit equal cmp memcmp 禁止拷贝 MDL lock提供多粒度锁，分为scoped lock和object lock，namespace层次如下：![MySQL_MDL_key namespace](/MySQL_MDL_key namespace.png)\nnamespace的详细说明（这里颠倒了枚举定义的顺序，按scoped lock和object lock顺序论述）：\n   enum_mdl_namespace 说明     enum_mdl_namespace 说明   GLOBAL 全局唯一，防止DDL和写操作的过程中执行 set golbal_read_only =on 或flush tables with read lock;IX+S(EXPLICIT) 显式释放   COMMIT 全局唯一，执行flush table with read lock后，防止在途写事务的提交IX+S(EXPLICIT) 显式释放   TABLESPACE 表空间锁   SCHEMA 库锁   TABLE 表锁   FUNCTION 用于UDF   PROCEDURE 用于SP   TRIGGER 用于TRIGGER   EVENT 用于event-scheduler   USER_LEVEL_LOCK 用于test sync，GET_LOCK(str,timeout)，RELEASE_LOCK(str)   LOCKING_SERVICE plugin service    其中具体的应用场景：\nMDL_key::TABLESPACE X/IX TRX 用于 CREATE or ALTER TABLESPACE DISCARD or IMPORT TABLESPACE CREATE TABLE LIKE MDL_key::SCHEMA X/IX TRX 用于 lock_schema_name GLOBAL+IX+STMT SCHEMA+X+TRX lock_object_name GLOBAL+IX+STMT SCHEMA+IX+TRX OBJECT+X+TRX lock_table_names SCHEMA+IX+TRX MDL_key::GLOBAL IX STMT S EXPLICIT MDL_key::COMMIT IX STMT IX EXPLICIT S EXPLICIT 示例：\n锁的申请由上到下，由大到小，由意向到具体：\nsession A: begin transaton and select\nsession B: ALTER TABLE \u0026hellip;\n   GLOBAL IX STMT     SCHEMA test IX TRX   TABLE test t SU TRX    session A: commit\nsession B:\n   TABLE test t X TRX            锁信息 #  在MDL_request和MDL_ticket中，都需要描述锁信息（key+type+duration）\ntype表示MDL锁的类型，enum_mdl_type\n   锁类型 简写 说明     MDL_INTENTION_EXCLUSIVE IX 意向X锁，只用于scoped lock   MDL_SHARED S    MDL_SHARED_HIGH_PRIO SH    MDL_SHARED_READ SR    MDL_SHARED_WRITE SW    MDL_SHARED_WRITE_LOW_PRIO SWLP    MDL_SHARED_UPGRADABLE SU    MDL_SHARED_READ_ONLY SRO    MDL_SHARED_NO_WRITE SNR    MDL_SHARED_NO_READ_WRITE SNRW    MDL_EXCLUSIVE X     * MDL_INTENTION_EXCLUSIVE IX // 意向X锁，只用于scope 锁\n* MDL_SHARED S // 只能读metadata，当能读写数据，如检查表是否存在时用这个锁\n* MDL_SHARED_HIGH_PRIO SH // 高优先级S锁，可以抢占X锁，只能读metadata，不能读写数据，用于填充INFORMATION_SCHEMA，或者show create table时\n* MDL_SHARED_READ SR // 可以读表数据，select语句，lock table xxx read 都用这个\n* MDL_SHARED_WRITE SW // 可以更新表数据，insert，update，delete，lock table xxx write, select for update，\n* MDL_SHARED_UPGRADABLE SU // 可升级锁，可以升级为SNW或者X锁，ALTER TABLE第一阶段会用到\n* MDL_SHARED_NO_WRITE SNW // 可升级锁，其它线程能读metadata，数据可读不能读，持锁者可以读写，可以升级成X锁，ALTER TABLE的第一阶段\n* MDL_SHARED_NO_READ_WRITE SNRW // 可升级锁，其它线程能读metadata，数据不能读写，持锁者可以读写，可以升级成X锁，LOCK TABLES xxx WRITE\n* MDL_EXCLUSIVE X // 排它锁，禁止其它线程的所有请求，CREATE/DROP/RENAME TABLE\n MDL_INTENTION_EXCLUSIVE(IX) 意向排他锁，锁定一个范围，用在GLOBAL/SCHEMA/COMMIT粒度。 MDL_SHARED(S) 用在只访问元数据信息，不访问数据。例如CREATE TABLE t LIKE t1; MDL_SHARED_HIGH_PRIO(SH) 也是用于只访问元数据信息，但是优先级比排他锁高，用于访问information_schema的表。例如：select * from information_schema.tables; MDL_SHARED_READ(SR) 访问表结构并且读表数据，例如：SELECT * FROM t1; LOCK TABLE t1 READ LOCAL; MDL_SHARED_WRITE(SW) 访问表结构且写表数据， 例如：INSERT/DELETE/UPDATE t1 … ;SELECT * FROM t1 FOR UPDATE;LOCK TALE t1 WRITE MDL_SHARED_WRITE_LOW_PRIO(SWLP) 优先级低于MDL_SHARED_READ_ONLY。语句INSER/DELETE/UPDATE LOW_PRIORITY t1 …; LOCK TABLE t1 WRITE LOW_PRIORITY。 MDL_SHARED_UPGRADABLE(SU) 可升级锁，允许并发update/read表数据。持有该锁可以同时读取表metadata和表数据，但不能修改数据。可以升级到SNW、SNR、X锁。用在alter table的第一阶段，使alter table的时候不阻塞DML，防止其他DDL。 MDL_SHARED_READ_ONLY(SRO) 持有该锁可读取表数据，同时阻塞所有表结构和表数据的修改操作，用于LOCK TABLE t1 READ。 MDL_SHARED_NO_WRITE(SNW) 持有该锁可以读取表metadata和表数据，同时阻塞所有的表数据修改操作，允许读。可以升级到X锁。用在ALTER TABLE第一阶段，拷贝原始表数据到新表，允许读但不允许更新。 MDL_SHARED_NO_READ_WRITE(SNRW) 可升级锁，允许其他连接读取表结构但不可以读取数据，阻塞所有表数据的读写操作，允许INFORMATION_SCHEMA访问和SHOW语句。持有该锁的的连接可以读取表结构，修改和读取表数据。可升级为X锁。使用在LOCK TABLE WRITE语句。 MDL_EXCLUSIVE(X) 排他锁，持有该锁连接可以修改表结构和表数据，使用在CREATE/DROP/RENAME/ALTER TABLE 语句。  duration表示MDL锁的持有时长，enum_mdl_duration\n   锁时长 说明     MDL_STATEMENT 语句结束自动释放   MDL_TRANSACTION 事务结束自动释放   MDL_EXPLICIT 显示申请，显示释放（unlock tables）    锁矩阵 #  MDL的锁矩阵根据scoped lock和object lock分为两类，其中有根据加锁策略分为granted matrix和waiting matrix。\nMDL_scoped_lock只使用3种锁类型：IX、S、X\nscoped_lock granted matrix\n | Type of active | Request | scoped lock | type | IX S X | ---------+----------------+ IX | + - - | S | - + - | X | - - - | scoped_lock waiting matrix\n | Pending | Request | scoped lock | type | IX S X | ---------+---------------+ IX | + - - | S | + + - | X | + + + | MDL_object_lock使用除IX外的所有锁类型\nobject_lock granted matrix\nRequest | Granted requests for lock | type | S SH SR SW SWLP SU SRO SNW SNRW X | ----------+---------------------------------------------+ S | + + + + + + + + + - | SH | + + + + + + + + + - | SR | + + + + + + + + - - | SW | + + + + + + - - - - | SWLP | + + + + + + - - - - | SU | + + + + + - + - - - | SRO | + + + - - + + + - - | SNW | + + + - - - + - - - | SNRW | + + - - - - - - - - | X | - - - - - - - - - - | SU -\u0026gt; X | - - - - - 0 0 0 0 0 | SRO -\u0026gt; X | - - - - 0 0 0 0 0 0 | SNW -\u0026gt; X | - - - 0 0 0 0 0 0 0 | SNRW -\u0026gt; X | - - 0 0 0 0 0 0 0 0 | 0：不可能出现的情况，比如对于SU锁来说其和自身是不兼容的，不可能有2个线程对同一个对象都持有SU锁，所以就不存在当一个线程进行锁升级时，另一个线程持有SU\nobject_lock waiting matrix\nRequest | Pending requests for lock | type | S SH SR SW SWLP SU SRO SNW SNRW X | ----------+--------------------------------------------+ S | + + + + + + + + + - | SH | + + + + + + + + + + | SR | + + + + + + + + - - | SW | + + + + + + + - - - | SWLP | + + + + + + - - - - | SU | + + + + + + + + + - | SRO | + + + - + + + + - - | SNW | + + + + + + + + + - | SNRW | + + + + + + + + + - | X | + + + + + + + + + + | SU -\u0026gt; X | + + + + + + + + + + | SRO -\u0026gt; X | + + + + + + + + + + | SNW -\u0026gt; X | + + + + + + + + + + | SNRW -\u0026gt; X | + + + + + + + + + + | SH 比 X 锁的优先级还高，正是其高优先级(high priority)的体现。\nMDL_map 全局锁信息 #  static，在MySQL启动时初始化，并预分配两个全局唯一的scoped lock（GLOBAL+COMMIT），MDL_map是MDL子系统的内部对象，外部不可见。\n作为全局MDL锁的存储，其必定会成为热点，之前采用partition mutex的方式解决热点，后改进采用xxx。并且采用lazy random re-org的方式进行内存整理。\nMDL_context #  作为THD和MDL子系统的交互接口，提供锁的管理功能（申请、释放、升级、clone、回滚、死锁检测）。\nMDL_context的生命周期如下（和THD同生命周期）：\n声明定义 class THD { MDL_context mdl_context; }; 初始化 THD::THD mdl_context.init(this); 使用：申请锁 acquire lock thd-\u0026gt;mdl_context.acquire_lock(\u0026amp;mdl_request, thd-\u0026gt;variables.lock_wait_timeout)) 释放 THD::release_resources if (!cleanup_down) THD::cleanup mdl_context.release_transactional_locks(); mdl_context::release_locks_stored_before(MDL_STATEMENT, NULL); mdl_context::release_locks_stored_before(MDL_TRANSACTION, NULL); mdl_context.destroy(); MDL_context.m_tickets[]中存储了3个ticket链表：\n STMT TRX EXPLICIT  其中STMT和TRX属于automiatc release，EXPLICIT属于manual release，有以下4种锁使用explicit显示锁：\nLOCK TABLES locks User-level locks HANDLER locks GLOBAL READ LOCK locks 数据结构\nm_tickets 指针数组（里面是3个ticket链表，分别代表STMT,TRX,EXPLICIT） m_owner 指向THD m_wait 实现锁等待（MDL_wait） m_waiting_for 当前线程正在等待的锁（MDL_wait_for_subgraph*） find_ticket 在当前线程的ticket链表中查找一个ticket(和request-\u0026gt;key同一对象\u0026amp;强度\u0026gt;=request-\u0026gt;type) clone_ticket clone ticket mdl_savepoint 生成savepoint rollback_to_savepoint MDL锁回滚到某个savepoint release_locks_stored_before 释放ticket链表上在某个ticket之前所有ticket release_lock 释放单个MDL锁（全局和自己） release_locks release_all_locks_for_name 把当前线程对某个对象加的所有MDL锁都释放掉 acquire_lock 申请锁 acquire_locks 一次性申请多个X锁，要么全部成功要么全部失败，用于RENAME, DROP和其他DDL语句 try_acquire_lock 尝试申请锁，失败就返回out_ticket，没有死锁检测 upgrade_shared_lock 升级共享锁 owns_equal_or_stronger_lock 当前线程是否已持有更强的锁 find_lock_owner 找到持有锁的第一个owner has_lock 当前线程是否在savepoint之前持有指定的锁 has_locks 当前线程是否没有持有任何锁（3个链表都为空） has_locks_waited_for set_explicit_duration_for_all_locks 在LOCK TABLES时使用，因为trx lock生命周期长于explicit，所以将stmt和trx链表中的锁移到explicit链表中 set_transaction_duration_for_all_locks 上面的反操作 set_lock_duration 将当前线程的某个ticket在链表中移动 release_statement_locks 释放所有语句范围的锁 release_transactional_locks 释放所有事务范围的锁 get_deadlock_weight 死锁时拿一个权重值，以此来判断对应线程是否要做为victim set_force_dml_deadlock_weight set_needs_thr_lock_abort get_needs_thr_lock_abort find_deadlock 检测是否有死锁 visit_subgraph 和死锁检测相关 acquire_locks：一次性申请多个X锁，要么全部成功要么全部失败，用于RENAME, DROP和其他DDL语句 如果后续的锁grant失败，会通过savepoint将前面已经申请到的锁也rollback。\n比如drop table test.t1这个DDL会一次加3个锁：\n GLOBAL，MDL_INTENTION_EXCLUSIVE test 库, MDL_INTENTION_EXCLUSIVE test.t1 表，MDL_EXCLUSIVE  MDL_context::acquire_lock\n主加锁函数，调试MDL锁相关问题时，给这个函数加断点比较有效。先调用MDL_context::try_acquire_lock_impl，如果加锁失败就进入等待加锁逻辑：\n 将MDL_context::try_acquire_lock_impl返回的ticket放进MDL_lock的等待队列 触发一次死锁检测 进入等待，等待又分为2种：  定时检查等待: 如果当前请求的锁是比较高级的（对于MDL_object_lock是比MDL_SHARED_NO_WRITE类型更高，对于MDL_scoped_lock是MDL_SHARED类型），就会每秒给其它持有当前锁的线程（并且这些连接持有的锁等级比较低）发信号，通知其释放锁，然后再检查是否锁已拿到 一直等待，直到超时   检查步骤3的等待结果，可以是GRANTED（拿到锁）、VICTIM（被死锁检测算法选为受害者）、TIMEOUT（加锁超时）、KILLED（连接被kill）。拿到锁返回成功，其它返回失败  MDL_context::upgrade_shared_lock\n锁升级，从共享锁升级到互斥锁，实现方式是重新申请一个目标锁，拿到新的ticket后替换老的ticket，用在alter table和create table场景中。\n如create table test.t1(id int) engine = innodb，会先拿test.t1的MDL_SHARED共享锁，检查表是否存在，如果不存在就把锁升级到MDL_EXCLUSIVE锁，然后开始建表。\n对于alter table test.t1 add column name varchar(10), algorithm=copy;，alter用copy到临时的方式来做。整个过程中MDL顺序是这样的：\n 刚开始打开表的时候，用的是 MDL_SHARED_UPGRADABLE 锁； 拷贝到临时表过程中，需要升级到 MDL_SHARED_NO_WRITE 锁，这个时候其它连接可以读，不能更新； 拷贝完在交换表的时候，需要升级到是MDL_EXCLUSIVE，这个时候是禁止读写的。  所以在用copy算法alter表过程中，会有2次锁升级。\nMDL_ticket::downgrade_lock 和MDL_context::upgrade_shared_lock对应的锁降级，从互斥锁降级到共享锁，实现比较简单，直接把锁类型改为目标类型（不用重新申请）。\n对于alter table test.t1 add column name varchar(10), algorithm=inplace，如果alter使用inplace算法的话，整个过程中MDL加锁顺序是这样的：\n 和copy算法一样，刚开始打开表的时候，用的是 MDL_SHARED_UPGRADABLE 锁； 在prepare前，升级到MDL_EXCLUSIVE锁； 在prepare后，降级到MDL_SHARED_UPGRADABLE（其它线程可以读写）或者MDL_SHARED_NO_WRITE（其它线程只能读不能写），降级到哪种由表的引擎决定； 在alter结束后，commit前，升级到MDL_EXCLUSIVE锁，然后commit。  可以看到inplace有2次锁升级，1次降级，不过在alter最耗时的阶段是有可能降级到MDL_SHARED_UPGRADABLE的，对其它线程的影响小。\nMDL_request #  MDL_request用于描述THD当前SQL的MDL锁请求语义（请求什么对象什么类型多长时间的MDL锁），负责THD → MDL的交互数据，由6元组组成：MDL_request是POD，通过MDL_REQUEST_INIT宏来完成初始化\nMDL_request mdl_request; MDL_REQUEST_INIT(\u0026amp;mdl_request, mdl_type, db, name, MDL_EXCLUSIVE, MDL_TRANSACTION); MDL_ticket #  MDL子系统内部对锁请求的表示，可以想象成一张\u0026quot;门票\u0026quot;。\n创建点：由MDL_ticket::create()统一创建ticket\n申请锁时 MDL_context::try_acquire_lock_impl ticket= MDL_ticket::create(this, mdl_request-\u0026gt;type, mdl_request-\u0026gt;duration); MDL_context::clone_ticket ticket= MDL_ticket::create(this, mdl_request-\u0026gt;type, mdl_request-\u0026gt;duration); 也可以复用ticket（MDL_context.find_ticket()）：\n 查找ticket MDL_context.find_ticket()：遍历当前线程的3个ticket链表，查找当前锁对象（key）是否有\u0026gt;=duration的ticket 如果duration不相同，或者为显式锁（explicit），则clone ticket；否则复用  其中#2，因为duration不同，锁的释放时机不同，这会破坏锁的严格性；explicit为manual release，所以必须单独一个；而可以复用锁的场景更为普遍，如下：\nSTART TRANSACTION; insert into t1 values (1); insert into t1 values (2); 第二条insert不需要再走一遍复杂的加锁逻辑，因为第一条insert已经成功拿到t1表的ticket，类型都是MDL_SHARED_WRITE，并且MDL锁时间范围也一样（transaction），这个时候直接用已有的ticket。\nticket有三个用途：\n 向THD描述锁申请的结果：通过挂在MDL_request上的ticket指针表示，有指向则granted/waiting，没有指向则是没有申请到锁资源 作为THD和MDL子系统的桥梁，构建线程的锁等待图，用于死锁检测  其中第二点再展开一下：\n要构建锁线程的锁等待图：\n 连接线程和全局锁对象：作为双方（MDL_context，MDL_lock）的共同链表元素，提供路径可以access 实现锁等待算法：作为MDL_wait_for_subgraph的子类，实现accept_visitor()  内部的数据结构\nm_ctx*, m_lock*, type, duration MDL_lock中的2个ticket链表节点(next,prev) MDL_context中的3个ticket链表节点(next,prev) ctor dtor create destroy 禁止copy has_pending_conflicting_lock 当前ticket的锁类型是否和对应MDL锁的等待队列中的锁冲突 is_upgradable_or_exclusive 是否是可升级锁或者X锁（SU、SNW、SNRW、X） downgrade_lock 锁降级（X/SNR -\u0026gt; ???） has_stronger_or_equal_type 当前ticket对应的锁和指定的锁比较是否更强 granted matrix is_incompatible_when_granted 是否和granted matrix冲突 is_incompatible_when_waiting 是否和waiting matrix冲突 /** Implement MDL_wait_for_subgraph interface. */ accept_visitor get_deadlock_weight() 获取死锁权重 MDL_lock MDL锁对象 #  MDL子系统内部用于描述MDL锁对象的表示。\nm_rwlock 保护MDL_lock锁对象的读写锁（采用读者优先） key 锁标识（MDL_key） m_granted granted ticket链表（Ticket_list） m_waiting waiting ticket链表（Ticket_list） Ticket_list 表示当前锁（key）对应的ticket链表，内部类 add_ticket remove_ticket bitmap 当前ticket list中的所有锁类型（bitmap），用于快速检测要申请的ticket和grant matrix和waiting matrix是否冲突 struct MDL_lock_strategy create destroy remove_ticket 从MDL_lock中的granted/waiting链表释放指定的ticket reschedule_waiters 当锁的ticket释放/降级时，从等待队列中选择ticket看能否grant incompatible_granted_types_bitmap incompatible_waiting_types_bitmap get_incompatible_waiting_types_bitmap_idx switch_incompatible_waiting_types_bitmap_if_needed - has_pending_conflicting_lock // 已经授权的ticket是否和等待队列中的ticket不兼容 - can_grant_lock // 能否加锁，先和等待队列进行优先级比较，然后看和已授权的锁是否兼容 - visit_subgraph // 死锁检测相关 - needs_notification // 是否需要通知其它线程，当前ticket的锁情况 - notify_conflicting_locks // 通知其它线程，有一个高级的锁请求 needs_connection_check needs_hton_notification is_affected_by_max_write_lock_count count_piglets_and_hogs get_unobtrusive_lock_increment is_obtrusive_lock fast_path_granted_bitmap - hog_lock_types_bitmap // 标识哪种锁是高级锁 * m_hog_lock_count // 高级锁可以连接拿得锁的个数，超过这个数目就要给低级锁让路，防止低级锁饿死 m_piglet_lock_count m_current_waiting_incompatible_idx MDL_wait #  锁等待的实现类，作为MDL_context的成员变量\nenum_wait_status 锁等待退出时的状态 timed_wait 锁等待，mutex+cond+timeout MDL_savepoint #  因为explicit锁不会在savepoint rollback释放，所以只需记录THD对应的stmt和trx的ticket point，供MDL_context生成savepoint使用\nm_stmt_ticket 指向创建savepoint前的最后一个stmt ticket m_trans_ticket 指向创建savepoint前的最后一个stmt ticket global read lock #  MySQL为了获得全局一致性的点位，通过FTWRL（FLUSH TABLES WITH READ LOCK）来阻止变化：\n 新的变更进不来（有且只能有一人抢到全局唯一的锁） 已有变更提交不了  做到这样需要：\n 有且只能有一人抢到全局唯一的锁，并进行范围锁定 两把锁：阻止老的，阻止新的  锁定范围\n GLOBAL+S+EXPLICIT（通过Global_read_lock::lock_global_read_lock） COMMIT+S+EXPLICIT（通过Global_read_lock::make_global_read_lock_block_commit）  并清空表缓存（逼迫更新表元信息必须先走open table获取GLOBAL+IX+STMT），以及其他元信息更新入口 阻止元数据修改和表数据更改（S和IX不兼容）：\n 更新表元信息必须先走open table获取GLOBAL+IX+STMT） 事务和xa事务提交（数据变更，写入binlog先获取COMMIT+IX+STMT）  实现：\n当FLUSH TABLES table_list [WITH READ LOCK]时，Lex-\u0026gt;type添加REFRESH_TABLES和REFRESH_READ_LOCK标记\nsql_yacc.cc\nflush_options: table_or_tables { Lex-\u0026gt;type|= REFRESH_TABLES; /* Set type of metadata and table locks for FLUSH TABLES table_list [WITH READ LOCK]. */ YYPS-\u0026gt;m_lock_type= TL_READ_NO_INSERT; YYPS-\u0026gt;m_mdl_type= MDL_SHARED_HIGH_PRIO; opt_flush_lock: /* empty */ {} | WITH READ_SYM LOCK_SYM { TABLE_LIST *tables= Lex-\u0026gt;query_tables; Lex-\u0026gt;type|= REFRESH_READ_LOCK; for (; tables; tables= tables-\u0026gt;next_global) { tables-\u0026gt;mdl_request.set_type(MDL_SHARED_NO_WRITE); tables-\u0026gt;required_type= FRMTYPE_TABLE; /* Don\u0026#39;t try to flush views. */ tables-\u0026gt;open_type= OT_BASE_ONLY; /* Ignore temporary tables. */ } } sql_parse.cc\ncase SQLCOM_FLUSH: reload_acl_and_cache thd-\u0026gt;global_read_lock.lock_global_read_lock(thd); thd-\u0026gt;global_read_lock.make_global_read_lock_block_commit(thd)) flush table cache blocking其他请求\nGlobal_read_lock::lock_global_read_lock GLOBAL+S+EXPLICIT Global_read_lock::make_global_read_lock_block_commit COMMIT+S+EXPLICIT open_table GLOBAL+IX+STMT ha_commit_trans 事务提交 COMMIT+IX+EXPLICIT xa_transaction xa事务提交 prepare COMMIT+IX+STMT commit COMMIT+IX+STMT rollback COMMIT+IX+STMT TODO:\nFAST PATH（unobtrusive） OR SLOW PATH（obtrusive）\nLF_HASH\n"},{"id":12,"href":"/docs/MySQL/Server/memory/","title":"Memory","section":"Server","content":"在MySQL中，为了高效，Server层也有自己的内存管理。\n首先先介绍最简单的单链表内存管理，然后再介绍基于此优化的MEM_ROOT内存管理。\n单链表内存管理 #  在server层中，对于字符集处理，使用单链表的内存管理来处理内存资源。\n正如Monty在comments中写的：\nWed Jun 21 01:34:04 1989 Monty (monty at monty)\n** Added two new malloc-functions: my_once_alloc() and* *my_once_free(). These give easyer and quicker startup.*\n 分配内存、释放全部内存由my_once_alloc、my_once_free负责，my_once_strdup和my_once_memdup则是wrapper，内部封装了alloc+memcpy。\nalloc策略如下：\n 采用单向链表的方式将多个malloc的内存块管理起来 每个内存块采用malloc_chunk + raw的方式，即内存控制块+实际数据使用块两部分 raw部分如果剩余可用，则直接从其中划分，减少malloc开销 如果申请量小且现有的left小于4k，则对齐到4k  从这里可以看出，#3，#4一方面可以减少malloc的系统调用次数和碎片，同时，尽量按照4k对齐申请。\n另外，可以通过my_flags设定分配属性：\n MY_FAE /* Fatal if any error */ 内存分配失败就退出整个进程 MY_WME /* Write message on error */ 记录到日志中 MY_ZEROFILL /* Fill array with zero */ 分配内存后初始化为0  相关代码和示例如下：\nmy_global.h\n#define MY_ALIGN(A,L) (((A) + (L) - 1) \u0026amp; ~((L) - 1)) #define ALIGN_SIZE(A) MY_ALIGN((A),sizeof(double)) my_static.c\n/* from my_malloc */ USED_MEM* my_once_root_block=0; /* pointer to first block */ uint my_once_extra=ONCE_ALLOC_INIT; /* Memory to alloc / block */ my_alloc.h\ntypedef struct st_used_mem { /* struct for once_alloc (block) */ struct st_used_mem *next; /* Next block in use */ unsigned int left; /* memory left in block */ unsigned int size; /* size of block */ } USED_MEM; 示例\naccquire size = 5000 malloc from new chunk size = 5016 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fcb3a801000 next = 0x0 accquire size = 500 malloc from new chunk size = 4088 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fcb3a801000 next = 0x7fcb3b000000 USED_MEM size = 4088 left = 3568 cur = 0x7fcb3b000000 next = 0x0 accquire size = 100 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fcb3a801000 next = 0x7fcb3b000000 USED_MEM size = 4088 left = 3464 cur = 0x7fcb3b000000 next = 0x0 accquire size = 8500 malloc from new chunk size = 8520 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fcb3a801000 next = 0x7fcb3b000000 USED_MEM size = 4088 left = 3464 cur = 0x7fcb3b000000 next = 0x7fcb3a001000 USED_MEM size = 8520 left = 0 cur = 0x7fcb3a001000 next = 0x0 accquire size = 400 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fb5d1000000 next = 0x7fb5d1800000 USED_MEM size = 4088 left = 3064 cur = 0x7fb5d1800000 next = 0x7fb5d1801000 USED_MEM size = 8520 left = 0 cur = 0x7fb5d1801000 next = 0x0 accquire size = 3500 malloc from new chunk size = 3520 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fdced001000 next = 0x7fdced002400 USED_MEM size = 4088 left = 3064 cur = 0x7fdced002400 next = 0x7fdced003400 USED_MEM size = 8520 left = 0 cur = 0x7fdced003400 next = 0x7fdced005600 USED_MEM size = 3520 left = 0 cur = 0x7fdced005600 next = 0x0 accquire size = 3000 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fdced001000 next = 0x7fdced002400 USED_MEM size = 4088 left = 64 cur = 0x7fdced002400 next = 0x7fdced003400 USED_MEM size = 8520 left = 0 cur = 0x7fdced003400 next = 0x7fdced005600 USED_MEM size = 3520 left = 0 cur = 0x7fdced005600 next = 0x0 accquire size = 100 malloc from new chunk size = 4088 ------------------------------- USED_MEM size = 5016 left = 0 cur = 0x7fdced001000 next = 0x7fdced002400 USED_MEM size = 4088 left = 64 cur = 0x7fdced002400 next = 0x7fdced003400 USED_MEM size = 8520 left = 0 cur = 0x7fdced003400 next = 0x7fdced005600 USED_MEM size = 3520 left = 0 cur = 0x7fdced005600 next = 0x7fdced006400 USED_MEM size = 4088 left = 3968 cur = 0x7fdced006400 next = 0x0 相关文件：\nmy_global.h\nmy_static.h\nmy_static.c\nmy_once.c\nMEM_ROOT #  MEM_ROOT封装了USED_MEM对象，在 MySQL的Server层中广泛使用。MEM_ROOT作为类中的一个成员变量，伴随对象的整个生命周期，而且，不同的MEM_ROOT之间互相没有影响。使用MEM_ROOT的类有：THD、String、TABLE、TABLE_SHARE、Query_arena、st_transactions等等。\n在MEM_ROOT中，分配内存的单元是block（复用了USED_MEM对象），并优化了以下方面：\n 初始化时可以预分配内存 将一条单链表拆分为2条单链表（free+used），提升了查找效率 限制内存申请上限，并提供是否关闭错误信息的开关 可以同时申请多块内存 提供了多种回收、释放策略 采用启发式分配算法优化malloc 对free链表中的小块和多次不满足的小块申请，进行了优化  首先看一下MEM_ROOT的数据结构：\ntypedef struct st_mem_root { USED_MEM *free; // 空闲块链表（有可用空间） USED_MEM *used; // 满块链表 USED_MEM *pre_alloc; // 预分配块链表 size_t min_malloc; // 如果内存块过小，则从free移到used size_t block_size; // 初始化的块大小（init_alloc_root指定的大小） unsigned int block_num; // malloc分配的内存块计数 unsigned int first_block_usage; // free链表中的首节点计数，如果某次申请时超过10次且申请的大小\u0026gt;4k，则将该内存块从free移到used size_t max_capacity; // 申请总量（allocated_size+size）限制，0为不限制 size_t allocated_size; // 已分配的内存总量 void (*error_handler)(void); // 错误处理函数 } MEM_ROOT; 内存管理 #  初始化 #  init_alloc_root函数用于初始化MEM_ROOT对象，如果指定了pre_alloc_size，则malloc一块内存并指向到free和pre_alloc链表\n分配 #  alloc_root函数用于进行内存的分配管理，处理流程如下：\n 查看free链表，如果可用空间不足以容纳申请的的大小，且查找次数次数超过10、可用空间小于4k，则将该内存块从free移到used 如果free链表中没有合适的可用块，如果没有合适的块，则malloc，malloc的大小为max(size, block_size)，其中block_size为初始的block_size * block_num的2次幂 如果free链表中存在合适的可用块，则从该块分配，如果分配后剩余空间过小（min_malloc），则将该内存块从free移到used  详细分析日志：\n----------------------------------------------------- init_alloc_root block_size = 1024, pre_alloc_size = 1024 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 0 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 1024 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 1024 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 1024 length = 56 mem_root-\u0026gt;first_block_usage = 0 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 1 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 968 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 968 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 968 length = 56 mem_root-\u0026gt;first_block_usage = 1 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 2 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 912 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 912 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 912 length = 56 mem_root-\u0026gt;first_block_usage = 2 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 3 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 856 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 856 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 856 length = 56 mem_root-\u0026gt;first_block_usage = 3 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 4 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 800 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 800 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 800 length = 56 mem_root-\u0026gt;first_block_usage = 4 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 5 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 744 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 744 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 744 length = 56 mem_root-\u0026gt;first_block_usage = 5 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 6 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 688 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 688 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 688 length = 56 mem_root-\u0026gt;first_block_usage = 6 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 7 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 632 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 632 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 632 length = 56 mem_root-\u0026gt;first_block_usage = 7 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 8 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 576 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 576 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 576 length = 56 mem_root-\u0026gt;first_block_usage = 8 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 9 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 520 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 520 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 520 length = 56 mem_root-\u0026gt;first_block_usage = 9 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 10 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 464 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 464 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 464 length = 56 mem_root-\u0026gt;first_block_usage = 10 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 11 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 408 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 408 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 50 $$$ prev-\u0026gt;left = 408 length = 56 mem_root-\u0026gt;first_block_usage = 11 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 4 first_block_usage = 12 max_capacity = 0 allocated_size = 1040 -------------- mem_root-\u0026gt;used ----------------------- -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 2000 $$$ prev-\u0026gt;left = 352 length = 2000 mem_root-\u0026gt;first_block_usage = 12 $$$ remove from free ==\u0026gt; used -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 5 first_block_usage = 0 max_capacity = 0 allocated_size = 3056 -------------- mem_root-\u0026gt;used ----------------------- USED_MEM size = 2016 left = 0 cur = 0x7fe99d800000 next = 0x7fe99d000000 USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;free ----------------------- -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 500 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 6 first_block_usage = 0 max_capacity = 0 allocated_size = 4048 -------------- mem_root-\u0026gt;used ----------------------- USED_MEM size = 2016 left = 0 cur = 0x7fe99d800000 next = 0x7fe99d000000 USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 992 left = 472 cur = 0x7fe99cd02480 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 5000 $$$ prev-\u0026gt;left = 472 length = 5000 mem_root-\u0026gt;first_block_usage = 0 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 7 first_block_usage = 0 max_capacity = 0 allocated_size = 9064 -------------- mem_root-\u0026gt;used ----------------------- USED_MEM size = 5016 left = 0 cur = 0x7fe99e000000 next = 0x7fe99d800000 USED_MEM size = 2016 left = 0 cur = 0x7fe99d800000 next = 0x7fe99d000000 USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 992 left = 472 cur = 0x7fe99cd02480 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 3000 $$$ prev-\u0026gt;left = 472 length = 3000 mem_root-\u0026gt;first_block_usage = 0 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 8 first_block_usage = 0 max_capacity = 0 allocated_size = 12080 -------------- mem_root-\u0026gt;used ----------------------- USED_MEM size = 3016 left = 0 cur = 0x7fe99e001400 next = 0x7fe99e000000 USED_MEM size = 5016 left = 0 cur = 0x7fe99e000000 next = 0x7fe99d800000 USED_MEM size = 2016 left = 0 cur = 0x7fe99d800000 next = 0x7fe99d000000 USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 992 left = 472 cur = 0x7fe99cd02480 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 alloc_root length = 500 $$$ prev-\u0026gt;left = 472 length = 504 mem_root-\u0026gt;first_block_usage = 0 -------------- mem_root ----------------------------- min_malloc = 32, block_size = 992 block_num = 9 first_block_usage = 2 max_capacity = 0 allocated_size = 14064 -------------- mem_root-\u0026gt;used ----------------------- USED_MEM size = 3016 left = 0 cur = 0x7fe99e001400 next = 0x7fe99e000000 USED_MEM size = 5016 left = 0 cur = 0x7fe99e000000 next = 0x7fe99d800000 USED_MEM size = 2016 left = 0 cur = 0x7fe99d800000 next = 0x7fe99d000000 USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 -------------- mem_root-\u0026gt;free ----------------------- USED_MEM size = 992 left = 472 cur = 0x7fe99cd02480 next = 0x7fe99d002000 USED_MEM size = 1984 left = 1464 cur = 0x7fe99d002000 next = 0x0 -------------- mem_root-\u0026gt;pre_alloc ------------------ USED_MEM size = 1040 left = 352 cur = 0x7fe99d000000 next = 0x0 回收释放 #  free_root函数用于进行内存的回收是否，提供了3种回收策略：\n MY_MARK_BLOCKS_FREE：free、used只是标记（指针重置），并不归还给操作系统 MY_KEEP_PREALLOC：是否保留pre_alloc 释放全部free、used、pre_alloc  空间利用率上来讲，MEM_ROOT的内存管理方式在每个block 上连续分配，内部碎片基本在每个block的尾部，由 min_malloc 成员变量和参数 ALLOC_MAX_BLOCK_USAGE_BEFORE_DROP，ALLOC_MAX_BLOCK_TO_DROP 共同决定和控制，但是 min_malloc 的值是在代码中写死的，有点不够灵活，可以考虑写成可配置的，同时如果写超过申请长度的空间，就很有可能会覆盖后面的数据，比较危险。但相比 PG 的内存上下文，空间利用率肯定是会高很多的。\n从时间利用率上来讲，不提供 free 一个 Block 的操作，基本上一整个 MEM_ROOT 使用完毕才会全部归还给操作系统。\n其他函数 #  strdup_root、safe_strdup_root、strmake_root、memdup_root用于字符串和内存数据的空间申请和复制\nset_memroot_max_capacity：设置max_capacity\nset_memroot_error_reporting：申请不到内存时，是否关闭报错\nmulti_alloc_root：申请多块内存\nreset_root_defaults：重置并复用MEM_ROOT，如果pre_alloc_size不够，则重新分配（free+malloc）\nclear_alloc_root：重置MEM_ROOT中的free、used、pre_alloc指向\nalloc_root_inited：判断MEM_ROOT是否初始化\nclaim_root：PSI使用\n相关文件：\nmy_sys.h\nmy_alloc.h\nmy_alloc.c\n"},{"id":13,"href":"/docs/MySQL/Server/net/","title":"Net","section":"Server","content":"MySQL packet的结构如下：\npacket length (3 bytes) // 数据\npacket number (1 byte) // 保序\ncompression length (3 bytes) optional // 压缩\ncompression packet number （1 byte） optional // 保序\npacket data\n 因为采用3个字节存储包的长度，所以支持的包最大为 MAX_PACKET_LENGTH (256L256L256L-1)。如果数据流超过包最大值（16M），则通过packet number（ne→pkt_nr）保序。\n发包 #  my_net_write #  发包，Write a logical packet with packet header.\nnet_write_buff #  发送缓冲区，Caching the data in a local buffer before sending it.\n每个Net对象有一个buffer(net-\u0026gt;buff)，即将发送的数据被拷贝到这个buffer中。如果buffer未满则进行memcpy，并更新写入点位（net-\u0026gt;write_pos）；满了当Buffer满时需要立刻发出到客户端（net_write_packet）。\nnet_write_packet #  socket写数据，Write a MySQL protocol packet to the network handler.\nnet_write_raw_loop #  Write a determined number of bytes to a network handler. 调用vio_write进行socket写\nnet_flush #  Flush write_buffer if not empty. 也是调用net_write_packet进行socket写数据\n收包 #  my_net_read #  收包\nnet_read_packet #  读单包，Read one (variable-length) MySQL protocol packet. A MySQL packet consists of a header and a payload.\nnet_read_packet_header #  读包头，Read the header of a packet. 校验包头中的packet number，确认保序\n"},{"id":14,"href":"/docs/MySQL/Server/protocol/","title":"Protocol","section":"Server","content":"MySQL client-server protocol：https://dev.mysql.com/doc/internals/en/client-server-protocol.html\nMySQL 5.7重构了Protocol模块\n我们在这里主要聚焦在command phase，即MySQL server和client如何处理query的交互。\n我们从MySQL协议可以知道，在query的交互上，一般采用ping-pong模型，即：\n client-\u0026gt;server：发query server-\u0026gt;client：回包  所以我们在下面详细拆解这两个阶段。\n读取query #  当client发送一条query后，server对query进行以下处理：\n 读包 解析包体 根据命令指派执行  数据报文包括包头+包体，包头已在读包内部验证后丢弃（read_packet），然后包体返回给Protocol_classic::get_command封装为raw_packet。从raw_packet[0]中判断query的命令号，进行报文解析（parse_packet），拆解为COM_DATA（根据命令号封装了不同的struct）。\ndo_command // conn_handler Protocol_classic::get_command // 读包 Protocol_classic::read_packet my_net_read // 处理多包、压缩包 net_read_packet // 将读到的数据填充到NET中 Protocol_classic::parse_packet // 解析包体 dispatch_command // 根据命令指派执行 回包 #  返回的报文类型有：OK Packet，Error Packet和结果集包（Data Packet，EOF Packet）。\nOK Packet #  do_command dispatch_command THD::send_statement_status // 根据这条statement执行的情况确定回包类型 Protocol_classic::send_ok // 回OK包 Error Packet #  do_command dispatch_command THD::send_statement_status // 根据这条statement执行的情况确定回包类型 Protocol_classic::send_error // 回错误包 结果集包 #  结果集包的结构如下:\n   数据 说明     ResultSet Header 列数量   Field 列（多个）   EOF    Row 行（多个）   EOF     Server层的SQL执行器拼装结果集：\n// sql_executor Query_result_send::send_result_set_metadata thd-\u0026gt;send_result_metadata Protocol_classic::start_result_metadata() // 列数量  Protocol_classic::send_field_metadata() // 列（多个）  Protocol_classic::end_result_metadata() // EOF Query_result_send::send_data(List\u0026lt;Item\u0026gt; \u0026amp;items) // 行（多个）  protocol-\u0026gt;start_row(); thd-\u0026gt;send_result_set_row(\u0026amp;items) for loop : items protocl-\u0026gt;store() thd-\u0026gt;inc_sent_row_count(1); protocol-\u0026gt;end_row() Query_result_send::send_eof // EOF  net_send_ok 注意最后两行：send_eof调用了send_ok？？？\nbool Protocol_classic::send_eof(uint server_status, uint statement_warn_count) { DBUG_ENTER(\u0026#34;Protocol_classic::send_eof\u0026#34;); bool retval; /* Normally end of statement reply is signaled by OK packet, but in case of binlog dump request an EOF packet is sent instead. Also, old clients expect EOF packet instead of OK */ #ifndef EMBEDDED_LIBRARY  if (has_client_capability(CLIENT_DEPRECATE_EOF) \u0026amp;\u0026amp; (m_thd-\u0026gt;get_command() != COM_BINLOG_DUMP \u0026amp;\u0026amp; m_thd-\u0026gt;get_command() != COM_BINLOG_DUMP_GTID)) retval= net_send_ok(m_thd, server_status, statement_warn_count, 0, 0, NULL, true); else #endif  retval= net_send_eof(m_thd, server_status, statement_warn_count); DBUG_RETURN(retval); } 这是因为在MySQL 5.7.5中，有一个worklog：WL#7766: Deprecate the EOF packet，认为EOF and OK packets serve the same purpose (to mark the end of a query execution result.\n将ok和eof报文的发送放在了同一块逻辑中，client/server支持一个flag：CLIENT_DEPRECATE_EOF，EOF包说明里也有提到。\n如果我们要自己组装结果集包，则按照如下API组装即可：\nthd-\u0026gt;send_result_metadata protocol-\u0026gt;start_row(); protocol-\u0026gt;store(); protocol-\u0026gt;end_row(); my_eof(thd); 下面是一个实际的组装示例：\nmysql\u0026gt; show sql_filters; +--------+---------+----------+----------+---------+-------------+ | type | item_id | cur_conc | max_conc | key_num | key_str | +--------+---------+----------+----------+---------+-------------+ | SELECT | 4 | 0 | 1 | 2 | +,1,a=1~a=2 | +--------+---------+----------+----------+---------+-------------+ void mysqld_list_sql_filters(THD *thd) { List\u0026lt;Item\u0026gt; field_list; field_list.push_back(new Item_empty_string(\u0026#34;type\u0026#34;, 21)); field_list.push_back(new Item_return_int(\u0026#34;item_id\u0026#34;, 21, MYSQL_TYPE_LONGLONG)); field_list.push_back(new Item_return_int(\u0026#34;cur_conc\u0026#34;, 21, MYSQL_TYPE_LONGLONG)); field_list.push_back(new Item_return_int(\u0026#34;max_conc\u0026#34;, 21, MYSQL_TYPE_LONGLONG)); field_list.push_back(new Item_return_int(\u0026#34;key_num\u0026#34;, 21, MYSQL_TYPE_LONGLONG)); field_list.push_back(new Item_empty_string(\u0026#34;key_str\u0026#34;, SQL_FILTER_STR_LEN)); if (thd-\u0026gt;send_result_metadata(\u0026amp;field_list, Protocol::SEND_NUM_ROWS | Protocol::SEND_EOF)) return; mysql_rwlock_rdlock(\u0026amp;LOCK_filter_list); if (list_one_sql_filter(thd, select_filter_list, \u0026#34;SELECT\u0026#34;) || list_one_sql_filter(thd, update_filter_list, \u0026#34;UPDATE\u0026#34;) || list_one_sql_filter(thd, delete_filter_list, \u0026#34;DELETE\u0026#34;)) { ; } mysql_rwlock_unlock(\u0026amp;LOCK_filter_list); my_eof(thd); } int list_one_sql_filter(THD *thd, LIST *filter_list, const char *type) { CHARSET_INFO *cs= system_charset_info; filter_item *item= NULL; Protocol *protocol= thd-\u0026gt;get_protocol(); while (filter_list) { protocol-\u0026gt;start_row(); item= (filter_item*)filter_list-\u0026gt;data; protocol-\u0026gt;store(type, cs); protocol-\u0026gt;store((longlong)item-\u0026gt;id); protocol-\u0026gt;store(__sync_fetch_and_add(\u0026amp;(item-\u0026gt;cur_conc), 0)); protocol-\u0026gt;store(item-\u0026gt;max_conc); protocol-\u0026gt;store((longlong)item-\u0026gt;key_num); protocol-\u0026gt;store(item-\u0026gt;orig_str, cs); if (protocol-\u0026gt;end_row()) return 1; //no cover line  filter_list= filter_list-\u0026gt;next; } return 0; } MySQL 5.7的协议重构 #  MySQL 5.7大幅重构了Protocol模块代码, 采用了OO的设计方式：WL#7126: Refactoring of protocol class：\nNew Protocol ``class` `hierarchy ============================ The ``new` `hierarchy consists of 4 classes: Protocol | Protocol_classic | |---Protocol_text |---Protocol_binary Protocol is an abstract ``class` `that defines the ``new` `API. Protocol_classic is ex-Protocol ``class``, implements core of both classic protocols - text and binary. Protocol_text and Protocol_binary are implementations of appropriate classic protocols.   Protocol作为一个注释丰满且只有纯虚函数的抽象类, 非常容易理顺protocol模块能够提供的API。细节实现主要在Protocol_classic中（所以上文的调用栈可以看到, 实际逻辑是走到Protocol_classic中的）, 而逻辑上还划分出的两个类:\n Protocol_binary是Prepared Statements使用的协议 Protocol_text场景  上面提到了MySQL 5.7.5引入的Deprecate EOF，实际上MySQL 5.7上对OK/EOF报文做了大量修改，使得client可以通过报文拿到更多的会话状态信息。方便中间层会话保持，主要涉及几个worklog：\nWL#4797: Extending protocol’s OK packet\nWL#6885: Flag to indicate session state\nWL#6128: Session Tracker: Add GTIDs context to the OK packet\nWL#6972: Collect GTIDs to include in the protocol’s OK packet\nWL#7766: Deprecate the EOF packet\nWL#6631: Detect transaction boundaries\n同时新增变量控制报文行为:\n  session_track_schema = [ON | OFF] ON时, 如果session中变更了当前database, OK报文中回返回新的database\n  session_track_state_change = [ON | OFF] ON时, 当发生会话环境改变时, 会给CLIENT返回一个FLAG(1)，会话环境变化包括：\n当前database\n系统变量\nUser-defined 变量\n临时表的变更\nprepare xxx\n 但是只通知变更发生，具体值是多少，还需要配合session_track_schema、session_track_system_variables使用，所以限制还是很多…\n  session_track_system_variables = [“list of string, seperated bt ‘,’”] 这个参数用来追踪的变量, 目前只有time_zone, autocommit, character_set_client, character_set_results, character_set_connection可选。当这些变量的值变动时，client可以收到variable_name: new_value的键值对\n  session_track_gtids = [OFF | OWN_GTID | ALL_GTIDS] OWN_GTID：在会话中产生新GTIDs（当然只读操作不会后推GTID位点）时，以字符串形式返回新增的GTIDs ALL_GTIDS：在每个包中返回当前的executed_gtid值，但是这样报文的payload很高，不推荐\n  session_track_transaction_info = [ON | OFF] 打开后, 通过标志位表示当前会话状态，有8bit可以表示状态信息（其中使用字符’_‘表示FALSE）：\n  T: 显示开启事务; I: 隐式开启事务（autocommit = 0）\n  r: 有非事务表读\n  R: 有事务表读\n  w: 非事务表写\n  W: 事务表写\n  s: 不安全函数（比如 select uuid()）\n  S: server返回结果集\n  L: 显示锁表(LOCK TABLES) 一个事务内，返回的状态值是累加的\n示例 表t1是InnoDB，表t2是MyISAM\nSTART TRANSACTION; // T_______ INSERT INTO t1 VALUES (1); // T___W___ INSERT INTO t2 VALUES (1); // T__wW___ SELECT f1 FROM t1; // T_RwW_S_ ... COMMIT/ROLLBACK;     OK和EOF报文在MySQL 5.6上是走不同的逻辑构造报文，但实际上都是返回一些执行状态。MySQL 5.7中的Deprecated EOF报文，实际上是复用了OK报文中新增的状态，但是实际上这两个报文还是不同的：\nOK Packet： header = 0 and length of packet \u0026gt; 7\nEOF Packet：header = 0xfe and length of packet \u0026lt; 9\n只是复用了在net_send_ok里的扩充逻辑。\n有了以上这些信息，我们可以做很多中间层的开发工作，比如读写分离就用状态追踪对外提供透明的读写分离。\n"},{"id":15,"href":"/docs/MySQL/Server/read_only/","title":"Read Only","section":"Server","content":"MySQL通过5个步骤来设置read_only：\n 获取global read lock，阻塞所有的写入请求 flush opened table cache，阻塞所有的显式读写锁 获取commit lock，阻塞commit写入binlog 设置read_only=1 释放global read lock和commit lock  设置read_only阻塞用户写入，但只能阻塞普通用户的更新，MySQL为了最大可能的保护数据一致性，增强了read_only功能，通过设置super read only，阻塞除了slave线程以外以为的所有用户的写入，包括super用户。\n"},{"id":16,"href":"/docs/MySQL/Server/returning/","title":"Returning","section":"Server","content":"（转载自阿里内核月报）\n背景 #  MySQL对于statement执行结果报文通常分为两类：Resultset和OK/ERR，针对 DML语句则返回OK/ERR报文，其中包括几个影响记录，扫描记录等属性。但在很多业务场景下，通常 INSERT/UPDATE/DELETE 这样的DML语句后，都会跟随SELECT查询当前记录内容，以进行接下来的业务处理， 为了减少一次 Client \u0026lt;-\u0026gt; DB Server 交互，类似 PostgreSQL / Oracle 都提供了 returning clause 支持 DML 返回 Resultset。\nAliSQL 为了减少对 MySQL 语法兼容性的侵入，并支持 returning 功能， 采用了 native procedure 的方式，使用DBMS_TRANS package，统一使用 returning procedure 来支持 DML 语句返回 Resultset。\n语法 #  DBMS_TRANS.returning(Field_list=\u0026gt;, Statement=\u0026gt;); 其中:\n Field list : 代表期望的返回字段，以 “,” 进行分割，支持 * 号表达； Statement ：表示要执行的DML 语句， 支持 INSERT / UPDATE / DELETE；  INSERT Returning #  针对 insert 语句， returning proc 返回插入到表中的记录内容；\nmysql\u0026gt; CREATE TABLE `t` ( `id` int(11) NOT NULL AUTO_INCREMENT, `col1` int(11) NOT NULL DEFAULT '1', `col2` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB; mysql\u0026gt; call dbms_trans.returning(\u0026quot;*\u0026quot;, \u0026quot;insert into t(id) values(NULL),(NULL)\u0026quot;); +----+------+---------------------+ | id | col1 | col2 | +----+------+---------------------+ | 1 | 1 | 2019-09-03 10:39:05 | | 2 | 1 | 2019-09-03 10:39:05 | +----+------+---------------------+ 2 rows in set (0.01 sec) 如果没有填入任何 Fields, returning 将退化成 OK/ERR 报文：\nmysql\u0026gt; call dbms_trans.returning(\u0026quot;\u0026quot;, \u0026quot;insert into t(id) values(NULL),(NULL)\u0026quot;); Query OK, 2 rows affected (0.01 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u0026gt; select * from t; +----+------+---------------------+ | id | col1 | col2 | +----+------+---------------------+ | 1 | 1 | 2019-09-03 10:40:55 | | 2 | 1 | 2019-09-03 10:40:55 | | 3 | 1 | 2019-09-03 10:41:06 | | 4 | 1 | 2019-09-03 10:41:06 | +----+------+---------------------+ 4 rows in set (0.00 sec) 注意：INSERT returning 只支持 insert values 形式的语法，类似create as， insert select 不支持：\nmysql\u0026gt; call dbms_trans.returning(\u0026quot;\u0026quot;, \u0026quot;insert into t select * from t\u0026quot;); ERROR 7527 (HY000): Statement didn't support RETURNING clause UPDATE Returning #  针对 update 语句， returning 返回更新后的记录：\nmysql\u0026gt; call dbms_trans.returning(\u0026quot;id, col1, col2\u0026quot;, \u0026quot;update t set col1 = 2 where id \u0026gt;2\u0026quot;); +----+------+---------------------+ | id | col1 | col2 | +----+------+---------------------+ | 3 | 2 | 2019-09-03 10:41:06 | | 4 | 2 | 2019-09-03 10:41:06 | +----+------+---------------------+ 2 rows in set (0.01 sec) 注意: UPDATE returning 不支持多表 update 语句。\nDELETE Returning #  针对 delete 语句， returning 返回删除的记录前映像：\nmysql\u0026gt; call dbms_trans.returning(\u0026quot;id, col1, col2\u0026quot;, \u0026quot;delete from t where id \u0026lt; 3\u0026quot;); +----+------+---------------------+ | id | col1 | col2 | +----+------+---------------------+ | 1 | 1 | 2019-09-03 10:40:55 | | 2 | 1 | 2019-09-03 10:40:55 | +----+------+---------------------+ 2 rows in set (0.00 sec) 注意 #  1. 事务上下文 DBMS_TRANS.returning() 不是事务性语句，根据 DML statement 来继承 事务上下文， 结束事务需要显式的 COMMIT 或者 ROLLBACK。\n2. 字段不支持计算 Field list 中，只支持表中原生的字段，或者 * 号， 不支持进行计算或者聚合等操作。\n"},{"id":17,"href":"/docs/MySQL/Server/signal_handler/","title":"Signal Handler","section":"Server","content":"引出问题 #  在之前MySQL发版测试后，QA发现一个问题，用mysql.server stop脚本无法使mysqld进程退出，只会无限等待。\n通过查看mysql.server脚本，发现使用stop选项时执行了如下操作：\n 检查mysql/var/mysql.pid文件是否存在，若不存在说明mysqld进程未启动；若存在，读出文件中记录的mysqld进程号pid； 发送kill -0 pid，检查是否真的存在进程号为pid的进程。若进程不存在则无需stop，直接删除mysql/var/mysql.pid文件； 若pid进程存在，发送kill pid（即kill -15 pid），随后循环等待直到mysql/var/mysql.pid文件被删除（意味着mysqld已经退出）。  由此逻辑可知，mysql.server stop脚本无限等待大概率是因为第3步发送kill pid后，mysqld并未走退出流程，mysql/var/mysql.pid迟迟不被删所致。通过观察mysql/log/mysql.err文件中的日志，发现执行mysql.server stop后只有这么一条记录：\nGot signal 15 from thread 0Got signal 15 from thread 0 说明mysqld进程确实有收到SIGTERM信号，但是并未发起shutdown流程，因此需要了解Linux信号处理机制及MySQL中信号处理逻辑才好判断是哪里出了问题。\nLinux信号处理机制 #  在Linux系统中，我们可以通过kill -信号名 进程ID的方式向指定进程发送各种信号。比如常用的kill pid其实等价于kill -15 pid，会向pid进程发送SIGTERM信号；在一个shell前台程序运行中按Ctrl+C，等价于kill -2 pid，会向pid进程发送SIGINT信号；而kill -9 pid则会向pid进程发送SIGKILL信号。\n当进程收到信号时如何处理？通常有几种处理方式：\n 默认：如果不自定义处理函数，则每个信号都有默认的处理方式。很多信号的默认处理方式都是进程中断+直接退出，极其暴力，这是我们不希望的，因此经常针对某些信号设置自定义处理函数； 捕获：利用signal函数或sigaction函数可以注册用户自定义的信号处理函数（进程级生效）。比如在程序中使用signal(SIGINT, handle_shutdown)，可以注册进程收到SIGINT信号时调用用户自定义函数handle_shutdown，然后用户就可以在此函数中实现一些优雅退出的代码，比如保存工作进度、打印日志等等，不至于退出得过于暴力。需要注意的是，并不是所有信号都允许被捕获并进入自定义处理函数，比如SIGKILL（kill -9）就不允许； 忽略：将某信号的回调函数设置为SIG_IGN(ignore)。进程收到此信号后直接丢弃，不作任何处理。一般涉及网络通信的服务端程序至少都会忽略坑爹信号SIGPIPE（当试图向对方发送数据时发现对方已断连就会触发，默认处理方式是进程退出，极其坑爹……）。需要注意的是，并不是所有信号都允许被忽略，比如SIGKILL（kill -9）就不允许； 屏蔽：通过pthread_sigmask函数可以针对某一线程设置一个信号屏蔽集合，比如选择屏蔽SIGTERM，那么该线程在收到此信号是只是将信号丢入一个pending队列，并不做处理。在此要强调，屏蔽和忽略是有区别的，忽略是直接将信号丢弃，而屏蔽是可以通过pthread_sigmask函数解除的，当解除以后pending队列里的信号就会再次弹出。每个线程可以设置不同的屏蔽规则，但是线程创建时默认继承父线程的屏蔽规则。  在多线程程序中，向进程发送的信号除硬件故障导致的信号外，是可能被随机转发到任何一个线程的。不仅如此，连续发送多个信号，可能在多个子线程中被并发触发，这使得信号处理逻辑变得异常复杂。试想一下，假如我们在进程中对SIGTERM信号注册了优雅退出的处理函数，当用户对进程执行了kill命令，你的线程1捕获到SIGTERM正在优雅退出时，用户等得不耐烦又发了一遍kill命令，此时线程2就有可能捕获到SIGTERM，和线程1并发进行退出，这可能会导致各种非预期行为发生。\n因此，**大部分多线程程序常用的信号处理模式是使用单独的信号处理线程，将可能在多个线程中并发出现的信号，转化为只在信号处理线程中排队依次出现，同步处理。**具体实现方式为：对于程序中绝大多数不关注信号的线程，都通过设置信号屏蔽规则在整个线程生命周期中将某些信号屏蔽；建立一个单独的信号处理线程，用一个无限循环调用sigwait函数等待所有关注的信号，阻塞式等待在别的线程“碰壁”后被加入pending队列的信号，每次取出一个信号，进行对应处理，再调用sigwait取下一个信号。这样就将多线程异步信号转为了单线程同步处理，使编写多线程程序的信号处理逻辑变得非常简单。当采用单独的信号处理线程后，不管哪个线程收到SIGTERM，都会在信号处理线程的sigwait函数中返回，这样同时就只可能由信号处理线程执行优雅退出，此时就算向进程发送别的信号，只要信号处理线程不将前一个信号处理完后再次调用sigwait，都是感知不到的。在下文中我们看到，MySQL正是使用了这种单独信号处理线程的模式。\nMySQL信号处理逻辑 #  在sql/mysqld.cc的主入口函数mysqld_main中靠前的位置，我们可以找到设置MySQL信号处理规则的函数my_init_signals：\n// sql/mysqld.cc mysqld_main() if (init_common_variables()) unireg_abort(MYSQLD_ABORT_EXIT); // Will do exit  my_init_signals(); 在my_init_signals函数中首先设置了进程级的信号处理规则：\n 将SIGSEGV、SIGABRT、SIGBUS、SIGILL、SIGFPE等严重错误信号的自定义处理函数设置为handle_fatal_signal（此函数主要负责在mysql.err日志中记录进程挂掉之前的状态信息、函数调用栈，以及生成core文件等）； 将SIGPIPE、SIGALRM信号忽略（几乎没有哪个网络程序的服务端不忽略这个信号……否则客户端不close连接的情况下断线，服务端再尝试给客户端发送数据的时候就会触发SIGPIPE，直接中断进程）； 将SIGTERM、SIGHUP信号的自定义处理函数设置为print_signal_warning（此函数只负责在mysql.err里打印一条Got signal xx from thread yy，也就是之前调用mysql.server stop时日志里打印Got signal 15 from thread 0的原因；在MySQL 5.7中已经改为SIG_DFL默认动作）；  my_init_signals函数中还设置了线程级的信号屏蔽\n 将SIGTERM、SIGHUP、SIGQUIT信号用pthread_sigmask函数屏蔽。  由于线程创建时默认继承父线程的屏蔽规则，在my_init_signals函数调用结束之后，mysqld_main产生的所有子进程都继承了以上的信号处理逻辑。\n之后，mysqld_main调用start_signal_handler函数，启动了独立的信号处理线程signal_thread，该线程执行函数signal_hand：\n// sql/mysqld.cc mysqld_main() #ifndef _WIN32  // Start signal handler thread.  start_signal_handler(); #endif  // sql/mysqld.cc start_signal_handler() mysql_mutex_lock(\u0026amp;LOCK_start_signal_handler); if ((error= mysql_thread_create(key_thread_signal_hand, \u0026amp;signal_thread_id, \u0026amp;thr_attr, signal_hand, 0))) { sql_print_error(\u0026#34;Can\u0026#39;t create interrupt-thread (error %d, errno: %d)\u0026#34;, error, errno); flush_error_log_messages(); exit(MYSQLD_ABORT_EXIT); } mysql_cond_wait(\u0026amp;COND_start_signal_handler, \u0026amp;LOCK_start_signal_handler); mysql_mutex_unlock(\u0026amp;LOCK_start_signal_handler); 在signal_hand函数中，信号处理线程signal_thread使用无限循环调用sigwait等待SIGTERM、SIGQUIT和SIGHUP三种信号，其中当收到SIGTERM、SIGQUIT时都会调用pthread_kill(main_thread_id)启动shutdown流程。\n因此可以知道，当收到SIGTERM信号时：\n 凡是在my_init_signals函数后启动的线程，除signal_thread线程外，都会屏蔽SIGTERM信号，不做处理； signal_thread线程收到SIGTERM信号，发起shutdown流程。  那么问题来了，如果有线程在my_init_signals函数前启动会如何？答案是该线程不屏蔽SIGTERM信号，如果进程将SIGTERM信号转发给该线程，则调用print_signal_warning打印一行日志。这样问题定位的思路就清晰了，kill mysqld不退出，一定是发给进程的SIGTERM被某个启动时机甚早，早于my_init_signals函数的线程拦截到了。\n问题定位 #  有了先前的知识，问题排查就简单多了，只要揪出这个早早启动的线程就好，通过如下步骤很容易找到：\n  通过pstack检查这个版本MySQL和普通MySQL的线程栈信息差异，发现这个版本引入了一个独有线程sampling_thread：\nThread 70 (Thread 0x7f162b266700 (LWP 19637)):#0 0x00007f162b31d9cd in nanosleep () from /opt/compiler/gcc-4.8.2/lib/libc.so.6 #1 0x00007f162b3476f4 in usleep () from /opt/compiler/gcc-4.8.2/lib/libc.so.6 #2 0x00000000010c71ba in bvar::detail::SamplerCollector::run() () at public/bvar/bvar/detail/sampler.cpp:134 #3 0x00000000010c8b89 in bvar::detail::SamplerCollector::sampling_thread(void*) () at public/bvar/bvar/detail/sampler.cpp:84 #4 0x00007f162cef51c3 in start_thread () from /opt/compiler/gcc-4.8.2/lib/libpthread.so.0 #5 0x00007f162b34e12d in clone () from /opt/compiler/gcc-4.8.2/lib/libc.so.6   在源代码中搜索，发现sampling_thread线程为另一基础库libbvar所建立，该线程是一个静态全局变量的一部分，在程序启动后即建立，因此创建时机自然比mysqld_main调用my_init_signals时间更早，并未屏蔽掉SIGTERM信号，导致信号被此线程捕获，通过print_signal_warning打印日志处理；而MySQL自身的signal_thread线程无法再获得SIGTERM信号，自然不会发起shutdown流程；\n  再通过pstack检查该版本MySQL的线程栈，找到signal_thread线程的轻量级进程号19908（Linux系统中线程使用进程实现，因此也有自己的进程号）：\n  使用kill 19908，将SIGTERM直接发给signal_thread线程，发现mysqld开始进入shutdown流程，说明如果signal_thread线程捕获了SIGTERM信号是能够正常发起退出流程的，只是因为主进程收到信号后大概率发给sampling_thread线程，才导致mysqld不退出。\n  问题解决 #  解决方案有两种，第一种是修改libbvar中相关代码，令sampling_thread线程屏蔽掉需要暴露给signal_thread线程的信号即可；第二种是在程序中控制sampling_thread线程的初始化时间晚于mysqld_main中调用my_init_signals函数的时间，以便继承全局的信号屏蔽策略。由于sampling_thread线程是静态全局变量创建，几乎不可能将其创建时间延后至my_init_signals函数之后，因此只能采用第一种方法。经验证，修改后的mysqld进程在收到SIGTERM信号后可以正确退出了。\n附录：mysql.server stop和mysqladmin shutdown的区别 #  有同学发现mysql.server stop无法另mysqld退出时，可以使用mysqladmin shutdown的方式是可以使mysqld进入退出流程的。这两种shutdown方式究竟有什么区别？\n通过浏览mysqladmin源代码，可以看出执行shutdown命令时实际是调用libmysql/libmysql.c中的mysql_shutdown函数，向MySQL服务端发送了命令类型为COM_SHUTDOWN的请求。服务端接收到请求后调用了kill_server函数，直接用pthread_kill(signal_thread, SIGTERM)指名道姓的给signal_thread线程发送了SIGTERM信号，这样信号就不可能被别的线程提前拦截处理了。\n// client/mysqladmin.cc execute_commands() case ADMIN_SHUTDOWN: { ... /* Issue COM_SHUTDOWN if server version is older then 5.7*/ int resShutdown= 1; if(mysql_get_server_version(mysql) \u0026lt; 50709) resShutdown= mysql_shutdown(mysql, SHUTDOWN_DEFAULT); else resShutdown= mysql_query(mysql, \u0026#34;shutdown\u0026#34;); // libmysql/libmysql.c mysql_shutdown() int STDCALL mysql_shutdown(MYSQL *mysql, enum mysql_enum_shutdown_level shutdown_level) { DBUG_ENTER(\u0026#34;mysql_shutdown\u0026#34;); ... DBUG_RETURN(mysql_real_query(mysql, C_STRING_WITH_LEN(\u0026#34;shutdown\u0026#34;))); } PS.\n也可以使用下列命令停MySQL\nkill -USR1 `pid of mysqld` "},{"id":18,"href":"/docs/MySQL/Server/startup/","title":"Startup","section":"Server","content":"MySQL启动过程\nmain() // 入口 sql/main.cc mysqld_main() // sql/mysqld.cc // 记录入参 my_progname = argv[0]; orig_argc = argc; orig_argv = argv; // 处理配置文件my.cnf及启动参数 load_defaults(MYSQL_CONFIG_NAME, load_default_groups, \u0026amp;argc, \u0026amp;argv, \u0026amp;argv_alloc); // 继续处理启动参数，为初始化系统表做准备 sys_var::m_parse_flag == PARSE_EARLY handle_early_options(); // 为status统计计数做准备 init_sql_statement_names(); // 初始化system variables哈希表,链表sys_var_chain sys_var,遍历链表后,加入到system_variable_hash哈希表 // sys_var_chain链表已经通过sys_vars.cc的sys_var()构造函数static初始化 sys_var_init(); // 计算打开文件数并初始化table cache adjust_related_options(\u0026amp;requested_open_files); // init error log global variables init_error_log(); // init audit global variables mysql_audit_initialize(); // 初始化query log和slow log query_logger.init(); // 初始化system variables init_common_variables(); default_storage_engine= const_cast\u0026lt;char *\u0026gt;(\u0026quot;InnoDB\u0026quot;); // 设置默认storage engine add_status_vars(status_vars); // 初始化status变量(show status), status_vars为全局变量 set_server_version(); get_options(\u0026amp;remaining_argc, \u0026amp;remaining_argv); // sys_var::m_parse_flag == PARSE_NORMAL // 设置thread_cache_size init_client_errs(); // 读出给client返回出错信息的文件 lex_init(); // 初始化词法分析 item_create_init(); // 初始化函数列表 func_array为全局变量 // 设置默认字符集和校验字符集 global_system_variables.collation_connection= default_charset_info; global_system_variables.character_set_results= default_charset_info; global_system_variables.character_set_client= default_charset_info; // 设置默认storage engine lex_init(); // 初始化信号量 my_init_signals(); // 启动核心模块 init_server_components(); mdl_init();\t// mdl元数据锁 table_def_init(); // 表定义缓存 init_server_query_cache();\t// Query Cache init binlog relaylog\t// Binlog Relaylog gtid_server_init();\t// GTID plugin_register_builtin_and_init_core_se(); // Load builtin plugins, initialize MyISAM, CSV and InnoDB // 初始化并创建GTID init_server_auto_options(); // 初始化SSL init_ssl(); // 初始化网络 network_init(); // 创建PID文件 create_pid_file(); // 初始化status variables init_status_vars(); // binlog相关检查初始化 check_binlog_cache_size(NULL); check_binlog_stmt_cache_size(NULL); binlog_unsafe_map_init(); // 初始化Slave init_slave(); // 创建线程处理信号量 start_signal_handler(); // 如果是安装初始化，创建handle_bootstrap线程进行初始化datadir,系统表 if (opt_bootstrap) bootstrap(mysql_stdin); // 创建manager线程 start_handle_manager(); // 执行DDL crash recovery execute_ddl_log_recovery(); // 监听socket事件 mysqld_socket_acceptor-\u0026gt;connection_event_loop(); if (signal_thread_id.thread != 0) ret= my_thread_join(\u0026amp;signal_thread_id, NULL); clean_up(1); mysqld_exit(MYSQLD_SUCCESS_EXIT); 其中，load_defaults()会寻找my.cnf，并根据load_default_groups，使用search_default_file_with_ext()解析每一行配置，并进行标准化（配置项名称前加上\u0026ndash;）\n// sql/mysqld.cc 2325 const char *load_default_groups[]= { ... 2329 \u0026quot;mysqld\u0026quot;,\u0026quot;server\u0026quot;, MYSQL_BASE_VERSION, 0, 0}; // mysys_ssl/my_default.cc my_load_defaults my_search_option_files my_search_option_files search_default_file_with_ext 配置项会被handle_default_option()缓存在内存中\n// mysys_ssl/my_default.cc handle_default_option() struct handle_option_ctx { MEM_ROOT *alloc; My_args *m_args; TYPELIB *group; }; "},{"id":19,"href":"/docs/MySQL/Server/thd/","title":"Thd","section":"Server","content":"THD对象 #  THD封装了线程相关的数据，可以视作一个处理单元。\nThe size of the THD is ~10K and its definition is found in sql_class.h.\nThe THD is a large data structure which is used to keep track of various aspects of execution state. Memory rooted in the THD will grow significantly during query execution, but exactly how much it grows will depend upon the query. For memory planning purposes we recommend to plan for ~10MB per connection on average.\nFor each client connection we create a separate thread with THD serving as a thread/connection descriptor class THD { NET net; // client connection descriptor  Vio* active_vio; Protocol *m_protocol; // Current protocol  Protocol_text protocol_text; // Normal protocol  Protocol_binary protocol_binary; // Binary protocol  ... } Global_THD_manager #  Global_THD_manager作为thd管理器，统一提供thd资源的管控，提供thd的全局查找、计数、执行操作。\n生命周期 #  create_instance() // 创建thd_mgr，mysqld启动时 destroy_instance() // 销毁thd_mgr，mysqld关闭时 get_instance() // 在mysqld运行时获取thd_mgr，以调用其api API #  add_thd() // 添加thd remove_thd() // 移除thd wait_till_no_thd // 移除所有thd thd操作 #  查找 #  提供Find_THD_Impl函数模板，并由以下方法调用：\n find_thd  函数模板\nFind_THD_variable 在PS查看thd信息时提供并发控制（thd-\u0026gt;LOCK_thd_data） Find_thd_user_var 在PS查看thd信息时提供并发控制（thd-\u0026gt;LOCK_thd_data） ... 计数 #  num_thread_running thread_created\n执行操作 #  提供Do_THD_Impl函数模板，并由以下方法调用：\n do_for_all_thd_copy ：提供LOCK_thd_remove对remove thd进行并发控制 do_for_all_thd ：不做并发控制  函数模板\nSet_kill_conn // kill thd List_process_list // 列出所有thd ... kill process_id #  kill命令格式\nKILL [CONNECTION | QUERY] processlist_id 其中：\n CONNECTION：中止processlist_id对应的查询中止，连接退出 QUERY ：中止processlist_id对应的查询中止，连接保持 Ctrl+C ：MySQL client Ctrl+C会新建立一个临时的connection，将kill query的命令发送给MySQL，停止之前的命令，再回收掉临时的connection  处理kill #  kill的执行是异步的，分为标记kill和中止执行两阶段。\n标记kill #  当MySQL Server收到kill命令时，会根据命令中指定的process_id，查找到对应的THD，设置kill标记\nstatic uint kill_one_thread(THD *thd, my_thread_id id, bool only_kill_query) { THD *tmp= NULL; uint error=ER_NO_SUCH_THREAD; Find_thd_with_id find_thd_with_id(id); DBUG_ENTER(\u0026#34;kill_one_thread\u0026#34;); DBUG_PRINT(\u0026#34;enter\u0026#34;, (\u0026#34;id=%u only_kill=%d\u0026#34;, id, only_kill_query)); tmp= Global_THD_manager::get_instance()-\u0026gt;find_thd(\u0026amp;find_thd_with_id); ... tmp-\u0026gt;awake(only_kill_query ? THD::KILL_QUERY : THD::KILL_CONNECTION); ... } 中止执行 #   空闲的process_id立即退出  void THD::awake(THD::killed_state state_to_set) { ... if (this-\u0026gt;m_server_idle \u0026amp;\u0026amp; state_to_set == KILL_QUERY) { /* nothing */ } else { killed= state_to_set; } ... }  在SQL的执行过程中，会在各种位置检测THD上的kill标记，中止执行，清理后退出\n  等待中响应\n如果查询此时等待在某个condition_variable上，那么短时间内可能很难唤醒，如果出现了死锁的情况，那么就更不可能唤醒了。因此，kill实现了针对等待的特殊响应，其主要思路是：在某个查询进入等待状态之前，在THD上记录下当前查询等待的condition_variable对象及其对应的mutex。\nvoid enter_cond(mysql_cond_t *cond, mysql_mutex_t* mutex, const PSI_stage_info *stage, PSI_stage_info *old_stage, const char *src_function, const char *src_file, int src_line) { DBUG_ENTER(\u0026#34;THD::enter_cond\u0026#34;); mysql_mutex_assert_owner(mutex); /* Sic: We don\u0026#39;t lock LOCK_current_cond here. If we did, we could end up in deadlock with THD::awake() which locks current_mutex while LOCK_current_cond is locked. */ current_mutex= mutex; current_cond= cond; enter_stage(stage, old_stage, src_function, src_file, src_line); DBUG_VOID_RETURN; } 在等待的条件上增加对thd-\u0026gt;killed状态的判断，即检测到killed时退出等待\nlonglong Item_func_sleep::val_int() { THD *thd= current_thd; Interruptible_wait timed_cond(thd); mysql_cond_t cond; timeout= args[0]-\u0026gt;val_real(); mysql_cond_init(key_item_func_sleep_cond, \u0026amp;cond); mysql_mutex_lock(\u0026amp;LOCK_item_func_sleep); thd-\u0026gt;ENTER_COND(\u0026amp;cond, \u0026amp;LOCK_item_func_sleep, \u0026amp;stage_user_sleep, NULL); error= 0; thd_wait_begin(thd, THD_WAIT_SLEEP); while (!thd-\u0026gt;killed) { error= timed_cond.wait(\u0026amp;cond, \u0026amp;LOCK_item_func_sleep); if (error == ETIMEDOUT || error == ETIME) break; error= 0; } thd_wait_end(thd); mysql_mutex_unlock(\u0026amp;LOCK_item_func_sleep); thd-\u0026gt;EXIT_COND(NULL); mysql_cond_destroy(\u0026amp;cond); return MY_TEST(!error); // Return 1 killed } kill发生时, 使用THD记录的condition_variable进行pthread_cond_signal，进行唤醒，等待的线程醒来检测kill标记，发现已被标记kill快速退出。\nvoid THD::awake(THD::killed_state state_to_set) { ... /* Broadcast a condition to kick the target if it is waiting on it. */ if (is_killable) { mysql_mutex_lock(\u0026amp;LOCK_current_cond); /* This broadcast could be up in the air if the victim thread exits the cond in the time between read and broadcast, but that is ok since all we want to do is to make the victim thread get out of waiting on current_cond. If we see a non-zero current_cond: it cannot be an old value (because then exit_cond() should have run and it can\u0026#39;t because we have mutex); so it is the true value but maybe current_mutex is not yet non-zero (we\u0026#39;re in the middle of enter_cond() and there is a \u0026#34;memory order inversion\u0026#34;). So we test the mutex too to not lock 0. Note that there is a small chance we fail to kill. If victim has locked current_mutex, but hasn\u0026#39;t yet entered enter_cond() (which means that current_cond and current_mutex are 0), then the victim will not get a signal and it may wait \u0026#34;forever\u0026#34; on the cond (until we issue a second KILL or the status it\u0026#39;s waiting for happens). It\u0026#39;s true that we have set its thd-\u0026gt;killed but it may not see it immediately and so may have time to reach the cond_wait(). However, where possible, we test for killed once again after enter_cond(). This should make the signaling as safe as possible. However, there is still a small chance of failure on platforms with instruction or memory write reordering. */ if (current_cond \u0026amp;\u0026amp; current_mutex) { mysql_mutex_lock(current_mutex); mysql_cond_broadcast(current_cond); mysql_mutex_unlock(current_mutex); } mysql_mutex_unlock(\u0026amp;LOCK_current_cond); } DBUG_VOID_RETURN; }   "},{"id":20,"href":"/docs/MySQL/Server/ThreadPool/","title":"Thread Pool","section":"Server","content":"背景 #  高并发场景 #  在高并发场景（访问共享资源）下，随着用户（并发）的增加，有以下两个挑战：\n  latency在后期成指数型增长  吞吐下降严重\n  造成这种现象的根本原因是通信、处理和同步成本大幅升高，最终达到处理极限，导致数据库的服务能力大幅下降，甚至无法响应请求。\n为了处理这种情况，有两种思路：\n  在系统达到预设性能指标时，采用管制措施保护数据库，使之可以持续提供稳定的服务。 这种方式称之为限流（aka “on-ramp metering”），由交通信号灯来控制多少车辆可以在峰值时通过。如下图所示：\n  另一种方式是线程池机制，减少内部的线程上下文切换开销、热点资源的竞争，提升CPU上有效代码的执行效率，同时也具备控制流量的能力。\n  我们最终期望实现的效果是：技术选型 #  MySQL的处理模型 #  现有的MySQL的请求处理架构采用的是单进程多线程方案，每个用户连接对应一个MySQL处理线程（后面称为工作线程），在这种情况下，随着用户连接数的增多，MySQL进程会创建大量的工作线程，系统性能在高并发场景下会有大幅的下降。\n而导致性能下降主要是这三个原因造成的：\n CPU cache miss 线程的上下文切换 热点共享资源的竞争（latch、锁\u0026hellip;）  采用线程池，则可以有针对性的对上述三点进行优化：将MySQL的工作线程和用户连接解绑，同一个工作线程应对多个用户连接上的请求：当前的用户请求处理完成后，工作线程再选择其他的用户请求进行处理。保证合理数量的工作线程提供可持续的处理能力，并且CPU cache的使用更高效，可以有效的降低线程的上下文切换代价，同时热点资源竞争的开销也随之降低，也降低了死锁发生的概率。\n线程池机制更加适用于OLTP场景（short CPU-bound queries），对于OLAP场景可能会产生护航（convey）问题。\n从某种程度上来看，线程池分离了连接资源（请求）和线程资源（处理）。\n不适用场景 #  线程池并不是万能的，对于以下几类场景并不适用：\n 间歇性的workload：这会导致不断的分配/回收线程资源。 OLTP大查询：大量并发、长时间的复杂查询，如果占满了所有处理线程，后续的请求会进行排队，造成latency的增加。 大量的消耗极小的简单查询：比如select 1，大量瞬间涌入的请求，如果进行排队可能也会造成latency的增加。 极高并发的prepared statement：prepared statement所使用的MySQL Binary Protocol会使交互往返多次，可能会造成请求的堆积。  效果 #  线程池的性能对比，这里参考官方给出的性能数据：\n60x Better Scalability: Read/Write (8192/128)\n18x Better Scalability: Read Only (8192/512)\n从这里可以看出，使用线程池后，可以提供稳定的高性能服务，符合我们的预期。\n线程池演进 #  从时间线上看，最早提出线程池方案的是MySQL企业版，然后MariaDB随之提供，Percona在port了MariaDB的实现后，增加了一些自己的功能。\nMySQL企业版：在MySQL 5.5企业版开始提供thread pool功能。\nMariaDB：在MariaDB 5.5开始提供thread pool功能。\nPercona：在MySQL 5.5.30和5.6.10-alpha开始提供thread pool功能。\nMySQL企业版、Percona、MariaDB提供的都是动态（自适应）线程池。\n选择静态线程池和动态线程池方案主要取决于：\n  是否有阻塞等耗时操作\n  是否会存在互相依赖\n  在大多数场景中，静态线程池都无法满足要求。\n 对比几家的实现方案，主要有几点不同：\n MariaDB和MySQL企业版在Windows平台实现不同：MySQL企业版使用的是WSAPoll（为了兼容性考虑），但这样也决定了不支持shared memory/named pipe两种连接方式，而MariaDB使用的是原生的Windows线程池。 MariaDB相对于MySQL企业版在不同平台使用了更加高效I/O复用模型 Percona在5.5 ~ 5.7版本增加了线程调度优先级，而这是MariaDB本身就有的，二者在线程调度优先级上的细节上不同。  整体功能性对比：\n    MySQL企业版 8.0 MariaDB 5.7 Percona-Server 5.7      MySQL企业版 8.0 MariaDB 5.7 Percona-Server 5.7   功能提供方式 plugin builtin buitin   并发调度算法 √ √ √   监听线程 √ √ √   高低优先级队列 √ √ √   限制最大的并发事务数 √     限制高优先级队列的事务数量 √ √ √   stall时长 √ √ √   线程数上限 √ √ √   低-\u0026gt;高调度 √ √ √   idle线程超时机制  √ √   网络等待优化   √   额外的服务端口（避免影响探活）  √ √   精确的等待时间统计  √     MySQL企业版 8.0 #  MySQL企业版通过插件的形式提供线程池功能、并可以通过系统表查看相应的线程池信息。\nMySQL 8.0.14前，线程池信息存储在INFORMATION_SCHEMA中。随着IS的废弃，相应信息移到PSI：\nSELECT * FROM performance_schema.tp_thread_state | | tp_thread_group_state | | tp_thread_group_stats;\nSELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE \u0026lsquo;%thread_pool%';\n 将工作线程划分为group，每个group对应一组用户连接。当建立连接时，thread poo manager通过RR（Round-Robin）的方式将用户连接和group对应起来。随后，每个group的监听线程用于响应用户的query请求，分为两种情况：\n 当前没有正在执行的其他语句，监听线程直接执行该语句 否则通过队列（高/低优先级）分发给工作线程  如果语句执行时间过长（认定为stall），则创建另一个线程作为监听线程。\n初始情况下，每个group创建一个监听线程，和一个后台线程用于监控线程组的状态。\n通过thread_pool_stall_limit来判断执行时间多长的query为stall，这样可以在work load上做出权衡：更快的执行不仅意味着系统访问更迅捷，也意味着死锁发生概率的降低；慢执行可以限制并发执行的数量。在stall时间未到时，同一group的query需要等待之前的query执行完成；当stall时间达到时，则会放行group中的下一个query。\n在某些情况下，比如磁盘I/O或者lock，可能导致block从而使整个group不可用，在这方面，设计了回调用于立即创建一个新的线程处理其他语句。\n队列划分为高优先级和低优先级：事务的第一个语句进入低优先级队列，事务的后续语句进入高优先级队列；如果是非事务引擎的语句，或者开启了autocommit的语句，则进入低优先级队列。\n提供的配置项：\n   配置项 说明     thread_pool_algorithm 并发调度算法   thread_pool_dedicated_listeners 是否为每个group开启一个监听线程用于响应用户的网络事件   thread_pool_high_priority_connection 是否开启高、低优先级队列   thread_pool_max_active_query_threads 每个group中可以有多少活跃的工作线程   thread_pool_max_transactions_limit 最大的事务数（活跃+非活跃），只能启动时设置   thread_pool_max_unused_threads 线程池中空闲的线程数量（区分consumer 1、reserve线程 N-1）   thread_pool_prio_kickup_timer 为了避免饥饿，请求在低优先级队列等待多长时间后移到高优先级队列   thread_pool_size group大小，一定程度上代表着可以并发执行的语句数量（默认16）   thread_pool_stall_limit stall时长    MariaDB 5.7 #  MariaDB在MySQL企业版推出线程池机制后随之提供了相应功能。但是有一些区别，这个在上面的方案比较中已经谈到。\n另外，MariaDB和Percona的线程调度优先级实现的细节上也有些不同：\n 优先级设置不同：MairaDB thread_pool_priority=auto,high, low；Percona thread_pool_high_prio_mode=transactions,statements,none Percona有thread_pool_high_prio_tickets Maria有thread_pool_prio_kickup_timer  提供的配置项：\n   配置项 说明     thread_pool_size 线程池大小   thread_pool_max_threads 线程池上限   thread_pool_min_threads 线程池下限   thread_pool_stall_limit stall时长   thread_pool_oversubscribe 高优先级的超订配额   thread_pool_idle_timeout 空闲线程超时时间   extra_port 旁路端口   extra_max_connections 旁路的最大连接数   thread_pool_dedicated_listener 是否开启监听线程用于响应用户的网络事件   thread_pool_exact_stats 精确等待时间统计   thread_pool_prio_kickup_timer 为了避免饥饿，请求在低优先级队列等待多长时间后移到高优先级队列   thread_pool_priority 高优先级调度模式    Percona-Server 5.7 #  提供了以下几点优化：\n 连接调度的优先级机制：通过thread_pool_high_prio_tickets来决定进入高优先级队列的connection优先级，更加高效的调度连接。 低优先级队列限流：当高优先级超订（thread_pool_oversubscribe）时，根据thread_pool_max_threads对低优先级限流，即不创建新的工作线程。 长时间的网络等待：对会出现长时间网络I/O等待（socket reads/writes）的场景（大结果集、BLOB、慢客户端），处理下一个query或者创建一个新的线程专门处理。  提供的配置项：\n   配置项 说明     配置项 说明   thread_pool_idle_timeout 空闲线程超时时间   thread_pool_high_prio_mode 高优先级调度模式   thread_pool_high_prio_tickets 进入高优先级队列的优先级   thread_pool_max_threads 线程池上限   thread_pool_oversubscribe 高优先级的超订配额   thread_pool_size 线程池大小   thread_pool_stall_limit stall时长   extra_port 旁路端口   extra_max_connections 旁路的最大连接数    High Level Design #  为了实现连接和处理的解耦，线程池技术的处理模型为：连接被抽象为THD，由工作线程（pthread）在处理时通过attach/detach进行挂载/卸载，pthread通过OS调度。同样，pthread和对应的内核任务（task_env）也是一个m:n的模型，这样，连接、线程、内核态任务都实行了m:n的调度。\n线程池的高效需要做到以下几点：\n 线程组数量和CPU核数对应，尽量保证每个CPU core上只有一个pthread运行 THD需要被均匀的分配到pthread上 pthread可以动态的根据workload弹性扩缩容 THD可以通过快速通道，优先被pthread处理  线程池本质上就是一个三层的pub/sub模型，第一层由conn_handler将监听接受到的连接（channel_info）分配给某个线程组（放入线程组的队列中），第二层由工作线程获取队列中的连接，或者监听epoll事件。\n从这里可以看到，工作线程一共要处理新、旧两种情况：新是指会话的第1次处理，旧是指会话的第n次处理。同时为了效率，还需要设立快速通道，可以优先处理某些连接上的请求（会话的第n次处理）。处理策略如下：\n 分为普通队列和高优先级队列，优先处理高优先级队列（快速通道） 从队列中取出新建链的连接或者epoll事件，即会话的第1次处理和后续请求（会话的第n次处理）  以下如果没有特别指出，则队列指的是普通队列（thread_group_t.queue），高优先级队列会指明为高优先级队列（thread_group_t.high_prio_queue）。  "},{"id":21,"href":"/docs/MySQL/Server/timeout/","title":"Timeout","section":"Server","content":"背景 #  在分布式环境下，异步网络是一个挑战，当遇到网络问题时，提供超时机制可以提升系统的可用性。并且，对于事务系统，锁机制也需要超时机制来保证资源可以在有限时间内释放，避免饥饿现象的产生。\n为此，MySQL在多种场景下提供了timeout机制。\n简单一句话，MySQL Protocol ping-pong模型各个节点都有超时检测  MySQL timeout配置 #  MySQL内有多种timeout，我们先看一下有多少：\nmysql\u0026gt; show global variables like '%timeout%'; +-----------------------------+----------+ | Variable_name | Value | +-----------------------------+----------+ | connect_timeout | 5 | | net_read_timeout | 30 | | net_write_timeout | 60 | | wait_timeout | 28800 | | interactive_timeout | 28800 | | lock_wait_timeout | 31536000 | | innodb_lock_wait_timeout | 3 | | innodb_rollback_on_timeout | OFF | +-----------------------------+----------+ 通过阅读官方文档，结合我们在下面对于timeout实现的论证，这里先放上结论：\n网络超时\n connect_timeout：在connection phase阶段超时 net_read_timeout：在comamnd phase阶段网络读超时 net_write_timeout：在comamnd phase阶段网络写超时 wait_timeout、interactive_timeout：在connection phase结束后，在command phase节点，多长时间没有收到命令包。这里的wait_timeout和interactive_timeout的区别只是连接的种类不同，wait_timeout是对noninteractive的连接空闲超时，interactive_timeout是对interactive的连接空闲超时（客户端连接时设置了CLIENT_INTERACTIVE）  总结一下：\n connection phase应用建链期间，没有收到client回复，connect_timeout command phase期间没有收到请求时，net_wait_timeout/net_interactive_timeout，当收到请求后，接受一次完整的请求间，如果出现网络读包超时，net_read_timeout，回包时写socket超时，net_write_timeout  锁超时\n lock_wait_timeout：获取mdl锁的超时时间 innodb_lock_wait_timeout：InnoDB中行锁等待的超时时间（对表锁无效） innodb_rollback_on_timeout：当InnoDB锁等待超时后，OFF只rollback事务中的最后一条语句，ON则rollback整个事务  timeout配置项的值范围\n   配置项 默认值 值范围 线上默认配置     connect_timeout 10 2 ~ 1 year 10   net_read_timeout 30 1 ~ 1 year 30   net_write_timeout 60 1 ~ 1 year 60   wait_timeout 8 hour 1 ~ 1 year 28800   interactive_timeout 8 hour 1 ~ 1 year 28800   lock_wait_timeout 1 year 1 ~ 1 year 31536000   innodb_lock_wait_timeout 50 1 ~ 1073741824 5   innodb_rollback_on_timeout OFF ON/OFF OFF    实现细节 #  下面结合代码看一下这些timeout的具体含义。\nconnect_timeout net_read_timeout net_write_timeout #  两处设置网络超时 1. 网络初始化 Protocol_classic::init_net mysql client my_net_init my_net_local_init my_net_set_read_timeout my_net_set_write_timeout net-\u0026gt;read_timeout = ... net-\u0026gt;write_timeout = ... 2. 网络读写 mysql client vio_socket_connect // 根据等待的连接事件（VIO_IO_EVENT_CONNECT） vio_read vio_write vio_socket_io_wait // 根据等待的读写事件（VIO_IO_EVENT_READ/VIO_IO_EVENT_WRITE）将select超时设置为vio-\u0026gt;read_timeout或vio-\u0026gt;write_timeout vio_io_wait // timeout:-1 select 即\n 在connection phase，将connect_timeout设置为net-\u0026gt;vio-\u0026gt;read_timeout/write_timeout 在command phase，将net_read_timeout设置为net-\u0026gt;vio-\u0026gt;read_timeout，将net_write_timeoutnet-\u0026gt;vio-\u0026gt;write_timeout  其实底层都是搞的select超时，然后通过以下调用链返回\nvio-\u0026gt;read vio_read_buff vio_read mysql_socket_recv vio_socket_io_wait vio_io_wait select vio-\u0026gt;write vio_write mysql_socket_send vio_socket_io_wait vio_io_wait select Protocol_classic::write my_net_write net_write_command net_flush net_write_buff net_write_packet net_write_raw_loop vio-\u0026gt;write 当select超时时，返回-1，然后设置net-\u0026gt;error = 2\n/* On failure, propagate the error code. */ if (count) { /* Socket should be closed. */ net-\u0026gt;error= 2; /* Interrupted by a timeout? */ if (vio_was_timeout(net-\u0026gt;vio)) net-\u0026gt;last_errno= ER_NET_WRITE_INTERRUPTED; else net-\u0026gt;last_errno= ER_NET_ERROR_ON_WRITE; #ifdef MYSQL_SERVER  my_error(net-\u0026gt;last_errno, MYF(0)); #endif  } 判断net-\u0026gt;error是否为非零，非零意味着有错误，然后关闭连接。\n connection phase：close_connection command phase ：end_connection  net_wait_timeout net_interactive_timeout #  在从网络读命令包之前设置net_wait_timeout，然后在读完之后设置为net_read_timeout\nbool do_command(THD *thd) { ... if (classic) { /* This thread will do a blocking read from the client which will be interrupted when the next command is received from the client, the connection is closed or \u0026#34;net_wait_timeout\u0026#34; number of seconds has passed. */ net= thd-\u0026gt;get_protocol_classic()-\u0026gt;get_net(); my_net_set_read_timeout(net, thd-\u0026gt;variables.net_wait_timeout); net_new_transaction(net); } rc= thd-\u0026gt;get_protocol()-\u0026gt;get_command(\u0026amp;com_data, \u0026amp;command); thd-\u0026gt;m_server_idle= false; if (classic) my_net_set_read_timeout(net, thd-\u0026gt;variables.net_read_timeout); return_value= dispatch_command(thd, \u0026amp;com_data, command); ... } lock_wait_timeout #  在获取mdl锁时设置等待时长\nthd-\u0026gt;mdl_context.acquire_locks(\u0026amp;mdl_requests, thd-\u0026gt;variables.lock_wait_timeout) 实践 #  需要调整wait_timeout/interactive_timeout：\n[Warning] Aborted connection 6 to db: 'unconnected' user: 'root' host: 'localhost' (Got timeout reading communication packets) 需要调整net_write_timeout：\n[Warning] Aborted connection 12 to db: 'test' user: 'root' host: 'localhost' (Got timeout writing communication packets) 需要注意的是，MySQL的关于网络的错误，除了超时以外都认为是error，没有做进一步的细分，比如可能会看到下面这种日志，有可能是客户端异常退出了，也有可能是网络链路异常。\n[Warning] Aborted connection 8 to db: 'unconnected' user: 'root' host: 'localhost' (Got an error reading communication packets) [Warning] Aborted connection 13 to db: 'test' user: 'root' host: 'localhost' (Got an error writing communication packets) "},{"id":22,"href":"/docs/MySQL/Server/user_connection_handler/","title":"User Connection Handler","section":"Server","content":"（转载自阿里内核月报）\n建立连接过程 #  MySQL建立连接过程如下\n// 初始化网络 network_init() set_ports(); // 设置port  Mysqld_socket_listener *mysqld_socket_listener= new (std::nothrow) Mysqld_socket_listener(bind_addr_str, mysqld_port, back_log, mysqld_port_timeout, unix_sock_name); Connection_acceptor\u0026lt;Mysqld_socket_listener\u0026gt; *mysqld_socket_acceptor= new (std::nothrow) Connection_acceptor\u0026lt;Mysqld_socket_listener\u0026gt;(mysqld_socket_listener); mysqld_socket_acceptor-\u0026gt;init_connection_acceptor(); ... // 监听socket事件 mysqld_socket_acceptor-\u0026gt;connection_event_loop() { Connection_handler_manager *mgr= Connection_handler_manager::get_instance(); while (!abort_loop) { Channel_info *channel_info= m_listener-\u0026gt;listen_for_connection_event(); if (channel_info != NULL) mgr-\u0026gt;process_new_connection(channel_info); } } Channel_info* Mysqld_socket_listener::listen_for_connection_event() { int retval= poll(\u0026amp;m_poll_info.m_fds[0], m_socket_map.size(), -1); // POLL  for (uint i= 0; i \u0026lt; m_socket_map.size(); ++i) { if (m_poll_info.m_fds[i].revents \u0026amp; POLLIN) { listen_sock= m_poll_info.m_pfs_fds[i]; is_unix_socket= m_socket_map[listen_sock]; break; } } MYSQL_SOCKET connect_sock= mysql_socket_accept(key_socket_client_connection, listen_sock, (struct sockaddr *)(\u0026amp;cAddr), \u0026amp;length); Channel_info* channel_info= new (std::nothrow) Channel_info_tcpip_socket(connect_sock); return channel_info; } void Connection_handler_manager::process_new_connection(Channel_info* channel_info) { check_and_incr_conn_count(); // 检查max_connections  m_connection_handler-\u0026gt;add_connection(channel_info); } // One_thread_connection_handler 一个线程处理所有连接 // Per_thread_connection_handler 一个线程处理一个连接 bool Per_thread_connection_handler::add_connection(Channel_info* channel_info) { // 检查thread cache是否有空闲  check_idle_thread_and_enqueue_connection(channel_info); // 没有空闲，创建用户线程  mysql_thread_create(key_thread_one_connection, \u0026amp;id, \u0026amp;connection_attrib, handle_connection, (void*) channel_info); } extern \u0026#34;C\u0026#34; void *handle_connection(void *arg) { my_thread_init(); // 线程初始化  for (;;) { THD *thd= init_new_thd(channel_info); // 初始化THD对象  thd_manager-\u0026gt;add_thd(thd); if (thd_prepare_connection(thd)) { // 请求第一次进入  lex_start(thd); // 初始化sqlparser  rc= login_connection(thd); check_connection(thd); acl_authenticate(thd, COM_CONNECT); // auth认证  thd-\u0026gt;send_statement_status(); prepare_new_connection_state(thd); // 准备接受QUERY  } else { while (thd_connection_alive(thd)) // 判活  { if (do_command(thd)) // 处理query sql/sql_parser.c  break; } end_connection(thd); } close_connection(thd, 0, false, false); thd-\u0026gt;release_resources(); // 进入thread cache，等待新连接复用  channel_info= Per_thread_connection_handler::block_until_new_connection(); } my_thread_end(); my_thread_exit(0); } 具体SQL处理流程：\nbool do_command(THD *thd) { // 新建连接，或者连接没有请求时，会block在这里等待网络读包  NET *net= thd-\u0026gt;get_protocol_classic()-\u0026gt;get_net(); my_net_set_read_timeout(net, thd-\u0026gt;variables.net_wait_timeout); net_new_transaction(net); rc= thd-\u0026gt;get_protocol()-\u0026gt;get_command(\u0026amp;com_data, \u0026amp;command); dispatch_command(thd, \u0026amp;com_data, command); } int Protocol_classic::get_command(COM_DATA *com_data, enum_server_command *cmd) { read_packet(); // 网络读包  my_net_read(\u0026amp;m_thd-\u0026gt;net); raw_packet= m_thd-\u0026gt;net.read_pos; *cmd= (enum enum_server_command) raw_packet[0]; // 获取命令号  parse_packet(com_data, *cmd); } bool dispatch_command(THD *thd, const COM_DATA *com_data, enum enum_server_command command) { switch (command) { case COM_QUERY: alloc_query(thd, com_data-\u0026gt;com_query.query, com_data-\u0026gt;com_query.length); // 从网络读Query并存入thd-\u0026gt;query  mysql_parse(thd, \u0026amp;parser_state); // 解析  } } // sql/sql_parse.cc void mysql_parse(THD *thd, Parser_state *parser_state) { mysql_reset_thd_for_next_command(thd); lex_start(thd); parse_sql(thd, parser_state, NULL); // 解析SQL语句  mysql_execute_command(thd, true); // 执行SQL语句  LEX *const lex= thd-\u0026gt;lex; TABLE_LIST *all_tables= lex-\u0026gt;query_tables; // 隐式提交 sql/transaction.cc  trans_commit_implicit(thd); switch (lex-\u0026gt;sql_command) { case SQLCOM_INSERT: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_DELETE: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_UPDATE: { res= lex-\u0026gt;m_sql_cmd-\u0026gt;execute(thd); break; } case SQLCOM_SELECT: { res= select_precheck(thd, lex, all_tables, first_table); // 检查privileges  res= execute_sqlcom_select(thd, all_tables); } // 显式提交  case SQLCOM_COMMIT: { trans_commit(thd); ha_commit_trans(thd, TRUE); Transaction_ctx *trn_ctx= thd-\u0026gt;get_transaction(); tc_log-\u0026gt;commit(thd, all)); // MYSQL_BIN_LOG::commit sql/binlog.cc  ordered_commit(thd, all, skip_commit); } ... } } thread cache #  连接复用受到thread_cache_size连接池配置大小的影响，0为关闭连接池；默认值为\n8 + (max_connections / 100) 运行时查看连接情况：\n Threads_cached：缓存的 thread数量，新连接建立时，优先使用cache中的thread Threads_connected：已连接的thread数量 Threads_created：建立的thread数量 Threads_running：running状态的 thread 数量  Threads_created = Threads_cached + Threads_connected\nThreads_running \u0026lt;= Threads_connected\nMySQL 建立新连接非常消耗资源，频繁使用短连接，又没有其他组件实现连接池时，可以适当提高 thread_cache_size，降低新建连接的开销\nmysql\u0026gt; show status like 'Thread%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 1 | | Threads_connected | 1 | | Threads_created | 2 | | Threads_running | 1 | +-------------------+-------+ 4 rows in set (0.00 sec) 源码分析 #     变量 类型 说明     channel_info class 连接信息   waiting_channel_info_list std::list\u0026lt;Channel_info*\u0026gt; 空闲连接链表   wake_pthread uint 空闲连接链表的长度   LOCK_thread_cache mysql_mutex_t 连接池锁   COND_thread_cache mysql_cond_t cond_wait 释放连接 block_until_new_connection()signal 新连接建立 check_idle_thread_and_enqueue_connection()broadcast 杀掉thread cache中的连接 kill_blocked_pthreads()   COND_flush_thread_cache mysql_cond_t cond_wait 杀掉thread cache中的连接 kill_blocked_pthreads()signal 释放连接 block_until_new_connection()   blocked_pthread_count ulong 被block的线程数   slow_launch_threads ulong 连接建立慢的线程数（\u0026gt; slow_launch_time）   max_blocked_pthreads ulong 被block的最大线程数，也就是thread_cache_size    block_until_new_connection #  handle_connection 线程结束之前，会执行 block_until_new_connection，尝试进入thread cache等待其他连接复用\n如果 blocked_pthread_count \u0026lt; max_blocked_pthreads，blocked_pthread_count++，然后等待被 COND_thread_cache 唤醒，唤醒之后 blocked_pthread_count– , 返回 waiting_channel_info_list 中的一个 channel_info ，进行 handle_connections 的下一个循环\ncheck_idle_thread_and_enqueue_connection #  检查是否 blocked_pthread_count \u0026gt; wake_pthread （有足够的block状态线程用来唤醒） 如有 插入 channel_info 进入 waiting_channel_info_list，并发出 COND_thread_cache 信号量\nauth连接限制 #  除了参数 max_user_connections 限制每个用户的最大连接数，还可以对每个用户制定更细致的限制。以下四个限制保存在mysql.user表中：\n MAX_QUERIES_PER_HOUR 每小时最大请求数（语句数量） MAX_UPDATES_PER_HOUR 每小时最大更新数（更新语句的数量） MAX_CONNECTIONS_PER_HOUR 每小时最大连接数 MAX_USER_CONNECTIONS 这个用户的最大连接数  GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...] resource_option: { | MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count } ALTER USER 'jeffrey'@'localhost' WITH MAX_QUERIES_PER_HOUR 90; 源码分析 #  typedef struct user_resources { uint questions; /* MAX_QUERIES_PER_HOUR */ uint updates; /* MAX_UPDATES_PER_HOUR */ uint conn_per_hour; /* MAX_CONNECTIONS_PER_HOUR */ uint user_conn; /* MAX_USER_CONNECTIONS */ /* Values of this enum and specified_limits member are used by the parser to store which user limits were specified in GRANT statement. */ enum {QUERIES_PER_HOUR= 1, UPDATES_PER_HOUR= 2, CONNECTIONS_PER_HOUR= 4, USER_CONNECTIONS= 8}; uint specified_limits; } USER_RESOURCES; ACL_USER #  ACL_USER 是保存用户认证相关信息的类 USER_RESOURCES 是它的成员变量\nclass ACL_USER :public ACL_ACCESS { public: USER_RESOURCES user_resource; ... } ACl_USER 对象保存在数组 acl_users 中，每次mysqld启动时，从mysql.user表中读取数据，初始化 acl_users，初始化过程在函数 acl_load 中\n调用栈如下：\nmain() mysqld_main() acl_init(opt_noacl); acl_reload(thd); acl_load(thd, tables); USER_CONN #  保存用户资源使用的结构体，建立连接时，调用 get_or_create_user_conn 为 THD 绑定 USER_CONN 对象：\n// 请求第一次处理时 acl_authenticate() if ((acl_user-\u0026gt;user_resource.questions || acl_user-\u0026gt;user_resource.updates || acl_user-\u0026gt;user_resource.conn_per_hour || acl_user-\u0026gt;user_resource.user_conn || global_system_variables.max_user_connections) \u0026amp;\u0026amp; get_or_create_user_conn(thd, (opt_old_style_user_limits ? sctx-\u0026gt;user().str : sctx-\u0026gt;priv_user().str), (opt_old_style_user_limits ? sctx-\u0026gt;host_or_ip().str : sctx-\u0026gt;priv_host().str), \u0026amp;acl_user-\u0026gt;user_resource)) -------\u0026gt; thd-\u0026gt;set_user_connect(uc); 每个用户第一个连接创建时，建立一个新对象，存入 hash_user_connections。\n第二个连接开始，从 hash_user_connections 取出 USER_CONN 对象和 THD 绑定。\n同一个用户的连接，THD 都和同一个 USER_CONN 对象绑定。\ntypedef struct user_conn { /* hash_user_connections hash key: user+host key */ char *user; char *host; /* Total length of the key. */ size_t len; ulonglong reset_utime; uint connections; uint conn_per_hour, updates, questions; USER_RESOURCES user_resources; } USER_CONN; 资源限制在源码中的位置\n   资源名称 函数     MAX_USER_CONNECTIONS check_for_max_user_connections()   MAX_CONNECTIONS_PER_HOUR check_for_max_user_connections()   MAX_QUERIES_PER_HOUR check_mqh()   MAX_UPDATES_PER_HOUR check_mqh()    调用链\nhandle_connection thd_prepare_connection(thd) login_connection check_connection acl_authenticate check_for_max_user_connections do_command dispatch_command mysql_parse check_mqh "},{"id":23,"href":"/docs/MySQL/Server/vio/","title":"Vio","section":"Server","content":"VIO模块 #  为不同的protocol提供network I/O wrapper，类似于winsock。\nVirtual I/O Library.\nThe VIO routines are wrappers for the various network I/O calls that happen with different protocols. The idea is that in the main modules one won\u0026rsquo;t have to write separate bits of code for each protocol. Thus vio\u0026rsquo;s purpose is somewhat like the purpose of Microsoft\u0026rsquo;s winsock library. https://dev.mysql.com/doc/internals/en/vio-directory.html\n SOCKET封装了socket fd，里面只有fd信息。\n目前支持的protocol：\n TCP/IP Unix domain socket Named Pipes（Windows only） Shared Memory（Windows only） Secure Sockets（SSL）  Unix domain socket\n用于实现同一主机上的进程间通信，即使用socket文件来进行通信。socket原本是为网络通讯设计的，但后来在socket的框架上发展出一种IPC机制，就是UNIX domain socket。虽然网络socket也可用于同一台主机的进程间通讯(通过loopback地址127.0.0.1)，但是UNIX domain socket用于IPC 更有效率：不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程。这是因为，IPC机制本质上是可靠的通讯，而网络协议是为不可靠的通讯设计的。\n使用方式：\nsocket(AF_UNIX, \u0026hellip;)\n 文件组织 #  include/violite.h VIO lite（封装了VIO的struct和对外的function） vio_priv.h VIO模块内部头文件 vio.c Declarations + open/close functions viosocket.c Send/retrieve functions viossl.c SSL variations viosslfactories.c Certification / Verification viopipe.c named pipe implemenation（windows only） vioshm.c shared memory implementation（windows only） 文件的层次关系和用途如下图所示：\n设计 #  3.1 I/O事件 #  网络I/O事件分为读、写、连接：\n VIO_IO_EVENT_CONNECT IN VIO_IO_EVENT_READ IN VIO_IO_EVENT_WRITE OUT VIO对象 #  对于建链后的连接，两端都分别分配一个VIO对象，用于描述和处理网络I/O。\nclient和server支持的连接方式和属性：\nclient Unix domain socket VIO_TYPE_SOCKET VIO_LOCALHOST | VIO_BUFFERED_READ TCP/IP VIO_TYPE_TCPIP VIO_BUFFERED_READ Shared Memory VIO_TYPE_SHARED_MEMORY VIO_LOCALHOST Named Pipes VIO_TYPE_NAMEDPIPE VIO_LOCALHOST server Channel_info_local_socket VIO_TYPE_SOCKET VIO_LOCALHOST Channel_info_tcpip_socket VIO_TYPE_TCPIP VIO对象的数据结构\nstruct st_vio { MYSQL_SOCKET mysql_socket; // socket fd (TCP/IP and Unix domain socket)  my_bool localhost; // VIO_LOCALHOST  struct sockaddr_storage local; // local internet address  struct sockaddr_storage remote; // remote internet address  size_t addrLen; // remote address len  enum enum_vio_type type; // protocol type  my_bool inactive; // 连接不再活跃（已关闭）  char desc[VIO_DESCRIPTION_SIZE]; // debug  char *read_buffer; // buffer for vio_read_buff 16k  char *read_pos; // start of unfetched data in the read buffer  char *read_end; // end of unfetched data  int read_timeout; // 读超时  int write_timeout; // 写超时  // 通过vtable interface来对不同协议下的实现进行解耦（多态）  /* viodelete is responsible for cleaning up the VIO object by freeing internal buffers, closing descriptors, handles. */ void (*viodelete)(Vio*); int (*vioerrno)(Vio*); size_t (*read)(Vio*, uchar *, size_t); size_t (*write)(Vio*, const uchar *, size_t); int (*timeout)(Vio*, uint, my_bool); int (*viokeepalive)(Vio*, my_bool); int (*fastsend)(Vio*); my_bool (*peer_addr)(Vio*, char *, uint16*, size_t); void (*in_addr)(Vio*, struct sockaddr_storage*); my_bool (*should_retry)(Vio*); my_bool (*was_timeout)(Vio*); /* vioshutdown is resposnible to shutdown/close the channel, so that no further communications can take place, however any related buffers, descriptors, handles can remain valid after a shutdown. */ int (*vioshutdown)(Vio*); my_bool (*is_connected)(Vio*); my_bool (*has_data) (Vio*); int (*io_wait)(Vio*, enum enum_vio_io_event, int); my_bool (*connect)(Vio*, struct sockaddr *, socklen_t, int); }; VIO对象的创建、初始化和销毁：\n常用函数 #  VIO模块中的常用函数如下：\nint vio_io_wait // 等待网络I/O时间通知（poll/select） vio_read // io读 vio_read_buff // io缓冲读 vio_write // io写 vio_fastsend // 尽可能设为TCP_NODELAY vio_keepalive // 尽可能设置TCP保活 SO_KEEPALIVE vio_timeout // 设置超时 vio_should_retry // 是否需要重试read/write（SOCKET_EINTR） vio_was_timeout // 是否超时（SOCKET_ETIMEDOUT） vio_is_connected // 是否处于连接中，通过vio_io_ait尝试read/write，或者通过socket_peek_read看是否有可读的数据 vio_pending // 获得缓冲区还有多少数据可以读 ioctl(FIONREAD) debug使用 vio_description // 打印socket/TCP信息 vio-\u0026gt;desc debug使用 vio_type // 获得连接的protocol类型 vio_errno // 获取socket error vio_fd // 获得socket fd vio_socket_connect // client发起connect vio_set_blocking // 设置socket fd为阻塞/非阻塞 vio_getnameinfo // 获得socket可读信息 vio_peer_addr // 获得远程socket的可读地址 vio_get_normalized_ip_string // 获得socket可读信息 vio_is_no_name_error // 是否错误为EAI_NONAME - Neither nodename nor servname provided, or not known. 即Name resolution error. Your hostname is invalid. It's not resolving through DNS to any network location. PSI观测 #  MySQL通过PSI机制观测一些关键路径和节点，在处理网络I/O时，其中的观测点是等待网络读写事件。\n入口是\nint vio_io_wait // 等待网络I/O时间通知（poll/select） 通过PSI提供状态的监测。\n宏 1. 定义变量 #define MYSQL_SOCKET_WAIT_VARIABLES(LOCKER, STATE) \\ struct PSI_socket_locker* LOCKER; \\ PSI_socket_locker_state STATE; 2. 进入socket等待 #define MYSQL_START_SOCKET_WAIT(LOCKER, STATE, SOCKET, OP, COUNT) \\ LOCKER= inline_mysql_start_socket_wait(STATE, SOCKET, OP, COUNT,\\ __FILE__, __LINE__) 3. 结束socket等待 #define MYSQL_END_SOCKET_WAIT(LOCKER, COUNT) \\ inline_mysql_end_socket_wait(LOCKER, COUNT) 使用\nMYSQL_SOCKET_WAIT_VARIABLES(locker, state) // 定义socket locker和locker state MYSQL_START_SOCKET_WAIT(locker, \u0026amp;state, vio-\u0026gt;mysql_socket, PSI_SOCKET_SELECT, 0); poll/select... MYSQL_END_SOCKET_WAIT(locker, 0); 其中函数指针的管理如下：\nstart_socket_wait_v1_t start_socket_wait; 在pfs.cc中的PFS_v1注册 pfs_start_socket_wait_v1 end_socket_wait_v1_t end_socket_wait; 在pfs.cc中的PFS_v1注册 pfs_end_socket_wait_v1 参考链接 #  https://dev.mysql.com/doc/internals/en/vio-directory.html\n"},{"id":24,"href":"/menu/","title":"Index","section":"Rick's Blog","content":" MySQL  Server  MySQL启动过程 MySQL的连接和请求处理  THD Protocol  Returning（转）   NET VIO MySQL 8.0 对网络模块的优化（转） MySQL 8.0 通过Resource Group来控制线程计算资源（转） MariaDB MaxScale Proxy Protocol（转） 用户建链（转）   Timeout机制 Thread Pool MDL IO_CACHE 内存管理 信号处理机制 日志 read_only lower_case_table_names Invisible Index   InnoDB  概览 源码结构 record page storage buffer pool latch B+ tree change buffer lock redo log transaction   Replication    "},{"id":25,"href":"/docs/MySQL/InnoDB/8_b+_tree/","title":"B+ tree","section":"Inno Db","content":"在InnoDB中，数据的存储组织为IoT，采用B+ tree的数据结构提供高效访问数据的方式（存取路径）。\n在B+ tree实现中，两块内容最为重要，一块是concurrency control，一块是SMO（Struct Modification Operations）。\n演进 #  B+ tree是由二叉查找树、平衡二叉树、B-tree演进而来的。\n二叉查找树 BST（Binary Search Tree）\n平衡二叉树 AVL（Balanced Binary Tree）\n在计算机科学中，AVL树是最早被发明的自平衡二叉查找树。在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为高度平衡树。查找、插入和删除在平均和最坏情况下的时间复杂度都是O(logn)。增加和删除元素的操作则可能需要借由一次或多次树旋转，以实现树的重新平衡。AVL 树得名于它的发明者 G. M. Adelson-Velsky 和 Evgenii Landis，他们在1962年的论文《An algorithm for the organization of information》中公开了这一数据结构。\n 二叉查找树 BST（Binary Search Tree） #  我们在介绍B+ tree之前，先了解一下binary search tree。在BST中，左子树的键值总是小于根的键值，右子树的键值总是大于根的键值。因此可以通过中序遍历得到键值的排序输出。\n中序遍历（LDR - Inorder Traversal）是二叉树遍历的一种，也叫做中根遍历、中序周游。在二叉树中，中序遍历首先遍历左子树，然后访问根结点，最后遍历右子树。\n比如有如下二叉查找树：\n中序遍历后输出：2、3、5、6、7、8。\n二叉查找树的平均查找速度比顺序查找来得更快：顺序查找的平均查找次数为(1+2+3+4+5+6)/6=3.3次，二叉查找树的平均查找次数为(3+3+3+2+2+1)/6=2.3次。\n但是，二叉查找树可以任意地构造，如果构造成下面的二叉查找树：\n则平均查找次数为(1+2+3+4+5+5)/6=3.16次，和顺序查找差不多。而二叉查找树的查找效率取决于树的高度，因此若想最大性能地构造一个二叉查找树，需要这棵二叉查找树是平衡的（即树的高度最小），因此引出了平衡二叉树，或称为AVL tree。\n平衡二叉树 AVL tree（Balanced Binary Tree） #  平衡二叉树的定义如下：首先符合二叉查找树的定义，其次必须满足任何节点的两个儿子子树的高度最大差为1。显然，上图不满足平衡二叉树的定义，而下图是一棵平衡二叉树。平衡二叉树对于查找的性能是比较高的，但不是最高的，只是接近最高性能。最好的性能需要建立一棵最优二叉树，但是最优二叉树的建议和维护需要大量的操作，因此，用户一般只需建立一棵平衡二叉树即可。\n平衡二叉树对于查询速度的确很快，但是维护一棵平衡二叉树的代价是需要付出代价的。通常来说，需要1次或多次左旋和右旋来得到插入或更新后树的平衡性。\n比如插入9，需要左旋以保证平衡：\n有的情况可能需要多次：\n上面2个图都是插入的例子，更新和删除操作同理，也是通过左旋或者右旋来完成的。因此对于一棵平衡树的维护是有一定开销的，不过平衡二叉树多用于内存结构对象中，因此维护的开销相对较小。\n平衡因子 = 左子树深度/高度 - 右子树深度/高度\n以下的动图更直观一些：\n平衡多路查找树 B-tree #  B-tree是为磁盘等外存储设备而设计的一种平衡查找树。\nB-tree需要满足：\n the nodes in a B-tree of order can have a maximum of children each internal node (non-leaf and non-root) can have at least (/2) children (rounded up) the root should have at least two children – unless it’s a leaf a non-leaf node with children should have keys all leaves must appear on the same level  使用B-tree可以高效的找到数据所在的磁盘块，这是因为B-tree的每个节点包含了大量的key信息和分支，相对于AVL tree减少了节点个数，查询效率更高（通过磁盘IO将数据加载到内存）。\nB+ tree #  B+ tree由B-tree和索引顺序访问方法（ISAM - Indexed Sequential Access Method，这也是MyISAM引擎最初参考的数据结构）演化而来。B+ tree相对于B-tree主要有两个区别：\n 所有叶子节点（记录）都由双向链表顺序串联起来。 非叶子节点只持有key，承担router到叶子节点的作用。  解读：\n 叶子节点的有序可以保证data access graph是从上到下的，这比B-tree的data分布在各层节点上，对于数据的加锁顺序更简单和友好 这可以保证非叶子节点可以最大化的存储key，而减少了树的高度。  比如下面这颗B+ tree，其高度为2，每页可存放4条记录，扇出（fan out）为5，如下图所示：\n所有记录都在叶子节点，并且是顺序存放的，如果用户从最左边的叶节点开始顺序遍历，可以得到所有键值的顺序排序：5、10、15、20、25、30、50、55、60、65、75、80、85、90。\nB+ tree和B-tree的不同之处有以下3点：\n B-tree的数据只会出现一次，存储在叶子节点或者非叶子节点。B+ tree的数据存储在叶子节点，非叶子节点作为router（可能出现多次）。 因为#1，B-tree的非叶子节点存储的数据量变小，扇出率低，所以B-tree的层高会更多，导致维护代价变大，并且搜索修改的性能变低。而B+ tree的非叶子节点只存储键值，所以是一个矮胖子，而B-tree是一个瘦高个子。 B-tree的查询效率和其数据在树中的位置相关。最大时间复杂度是root到leaf page（和B+树相同），最小时间复杂度为1（root）。B+ tree的查询复杂度是固定的。  在InnoDB中，每个页的大小是16KB，假设平均每行记录的大小为100个字节，则每个页能存放的记录数量为160。那么B+ tree高度和总共可以存放的记录之间的关系为：总记录数 = 记录数^树高^ log~d~(N)\n    树高(d) = 2 树高(d) = 3 树高(d) = 4     N 1602 409 6000 6 5536 0000    需要注意的是B+ tree只能找到记录所在的页，但是并不能定位到记录在页中的具体位置（偏移量），还需要通过page directory的二分查找才能找到记录。为了提高这部分效率，InnoDB会将页加载到内存中，加快查找效率。\n在B+ tree中，为了充分利用磁盘的顺序特性，InnoDB还会根据不同插入情况考虑不同的分裂点记录以及分裂的方向。\nindex tree #  在数据库中，每个表可能会创建多个索引。其中，存储行数据的B+ tree称为聚簇索引（clustered index），其他索引称为辅助索引（secondary index）。\nclustered index #  聚簇索引是用表的主键作为key来构造B+ tree的。如果没有显式创建主键（或唯一键），InnoDB会自动创建一个隐藏的ROWID作为主键。\n聚簇索引中的记录是根据主键顺序存放的，这里的顺序指的是逻辑上顺序，并不是物理存储上的顺序。因为物理存储要保证顺序的开销很大，另外数据库本身也做不到，这其中还牵涉到文件系统在磁盘上的布局。\n聚簇索引的非叶子节点存放的是\u0026lt;primary_key, page_no\u0026gt;，其中page_no指向下一层节点的页地址，称为index page；叶子节点存放的是实际的数据，称为data page。\n下面来看一个具体的例子：\ncreate table t1 ( a int not null auto_increment, b blob, c int not null, primary key (a), index idx_c (c) ) engine=InnoDB; insert into t1 values (1, repeat('a', 7000), -1); insert into t1 values (2, repeat('a', 7000), -2); insert into t1 values (3, repeat('a', 7000), -3); insert into t1 values (4, repeat('a', 7000), -4); 创建好表和记录后，我们先看一下文件的大小：144K，即9个页。\n通过hexdump查看二进制，我们将这9个页详细拆解一下。\n这9个页的关键信息如下图所示：\n对于page链表来说，空节点用FF表示，从上图可以看到：非叶子节点，prev+next都是FF，对于叶子节点，最左和最右叶子的prev和next都是FF  我们将数据页组织一下：\n注意：在聚簇索引root page中的*，第一个记录上加*表示该记录是页中的最小记录，即record header中min rec位置1（标记为蓝色）。这是InnoDB的B+ tree index的另一个不同之处，这种方式可以降低最小记录发生变更时须对页所进行的操作。\npage 3（聚簇索引 root page）\nBTR_SEG_TOP 0000 001b 0000 0002 0032\nBTR_SEG_LEAF 0000 001b 0000 0002 00f2\n​ \u0026lt; PK , page_no \u0026gt;\n00 10 00 11 00 0e \u0026lt;80 00 00 01,00 00 00 05\u0026gt;\n00 00 00 19 00 0e \u0026lt;80 00 00 02,00 00 00 06\u0026gt;\n00 00 00 21 ff d6 \u0026lt;80 00 00 04,00 00 00 07\u0026gt;\n 原始的ibd文件数据：\npage 0 FSP_HDR page 00000000 d3 e1 4c 85 00 00 00 00 00 00 00 00 00 00 00 00 |..L.............| 00000010 00 00 00 00 f4 78 5b 23 00 08 00 00 00 00 00 00 |.....x[#........| 00000020 00 00 00 00 00 1b 00 00 00 1b 00 00 00 00 00 00 |................| 00000030 00 09 00 00 00 40 00 00 00 21 00 00 00 08 00 00 |.....@...!......| 00000040 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| 00000050 00 01 00 00 00 00 00 9e 00 00 00 00 00 9e 00 00 |................| 00000060 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| 00000070 00 00 00 00 00 05 00 00 00 00 ff ff ff ff 00 00 |................| 00000080 ff ff ff ff 00 00 00 00 00 01 00 00 00 02 00 26 |...............\u0026amp;| 00000090 00 00 00 02 00 26 00 00 00 00 00 00 00 00 ff ff |.....\u0026amp;..........| 000000a0 ff ff 00 00 ff ff ff ff 00 00 00 00 00 02 aa aa |................| 000000b0 ff ff ff ff ff ff ff ff ff ff ff ff ff ff 00 00 |................| 000000c0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00003ff0 00 00 00 00 00 00 00 00 d3 e1 4c 85 f4 78 5b 23 |..........L..x[#| page 1 IBUF_BITMAP page 00004000 f0 c4 15 62 00 00 00 01 00 00 00 00 00 00 00 00 |...b............| 00004010 00 00 00 00 f4 77 6a 64 00 05 00 00 00 00 00 00 |.....wjd........| 00004020 00 00 00 00 00 1b 00 00 00 00 00 00 00 00 00 00 |................| 00004030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00007ff0 00 00 00 00 00 00 00 00 f0 c4 15 62 f4 77 6a 64 |...........b.wjd| page 2 INODE page 00008000 ea ab f8 5c 00 00 00 02 00 00 00 00 00 00 00 00 |...\\............| 00008010 00 00 00 00 f4 78 5b 23 00 03 00 00 00 00 00 00 |.....x[#........| 00008020 00 00 00 00 00 1b ff ff ff ff 00 00 ff ff ff ff |................| 00008030 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 |................| 00008040 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| * 00008060 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 05 d6 |................| 00008070 69 d2 00 00 00 03 ff ff ff ff ff ff ff ff ff ff |i...............| 00008080 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff |................| * 000080f0 ff ff 00 00 00 00 00 00 00 02 00 00 00 00 00 00 |................| 00008100 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| * 00008120 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 05 d6 |................| 00008130 69 d2 00 00 00 05 00 00 00 06 00 00 00 07 ff ff |i...............| 00008140 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff |................| * 000081b0 ff ff 00 00 00 00 00 00 00 03 00 00 00 00 00 00 |................| 000081c0 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| * 000081e0 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 05 d6 |................| 000081f0 69 d2 00 00 00 04 ff ff ff ff ff ff ff ff ff ff |i...............| 00008200 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff |................| * 00008270 ff ff 00 00 00 00 00 00 00 04 00 00 00 00 00 00 |................| 00008280 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 00 00 |................| * 000082a0 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 05 d6 |................| 000082b0 69 d2 ff ff ff ff ff ff ff ff ff ff ff ff ff ff |i...............| 000082c0 ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff |................| * 00008330 ff ff 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00008340 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 0000bff0 00 00 00 00 00 00 00 00 ea ab f8 5c f4 78 5b 23 |...........\\.x[#| page 3 INDEX page 0000c000 17 8d 80 b2|00 00 00 03 |ff ff ff ff|ff ff ff ff |................| FIL_PAGE_OFFSET FIL_PAGE_PREV FIL_PAGE_NEXT 0000c010 00 00 00 00 f4 78 5b 23 45 bf 00 00 00 00|00 00 |.....x[#E.......| 0000c020 00 00 00 00 00 1b|00 02 00 a2 80 05 00 00 00 00 |................| SPACE_ID 0000c030 00 9a 00 02 00 02|00 03 |00 00 00 00 00 00 00 00 |................| PAGE_N_RECS PAGE_MAX_TRX_ID 0000c040 |00 01|00 00 00 00 00 00 00 34|00 00 00 1b 00 00 |.........4......| PAGE_LEVEL INDEX_ID PAGE_BTR_SEG_LEAF 0000c050 00 02 00 f2|00 00 00 1b 00 00 00 02 00 32|01 00 |.............2..| PAGE_BTR_SEG_TOP record_header (5) 0000c060 02 00 1b 69 6e 66 69 6d 75 6d 00 04 00 0b 00 00 |...infimum......| 0000c070 73 75 70 72 65 6d 75 6d 00 10 00 11 00 0e 80 00 |supremum........| 0000c080 00 01 00 00 00 05 00 00 00 19 00 0e 80 00 00 02 |................| 0000c090 00 00 00 06 00 00 00 21 ff d6 80 00 00 04 00 00 |.......!........| 0000c0a0 00 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 0000c0b0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 0000fff0 00 00 00 00 00 70 00 63 17 8d 80 b2 f4 78 5b 23 |.....p.c.....x[#| page 4 INDEX page 00010000 9c d3 03 9b|00 00 00 04 |ff ff ff ff|ff ff ff ff |................| FIL_PAGE_OFFSET FIL_PAGE_PREV FIL_PAGE_NEXT 00010010 00 00 00 00 f4 78 5b 47 45 bf 00 00 00 00|00 00 |.....x[GE.......| 00010020 00 00 00 00 00 1b|00 02 00 ac 80 06 00 00 00 00 |................| SPACE_ID 00010030 00 a4 00 01 00 03|00 04| 00 00 00 00 00 00 0d 5c |...............\\| PAGE_N_RECS PAGE_MAX_TRX_ID 00010040 |00 00|00 00 00 00 00 00 00 35|00 00 00 1b 00 00 |.........5......| PAGE_LEVEL INDEX_ID PAGE_BTR_SEG_LEAF 00010050 00 02 02 72|00 00 00 1b 00 00 00 02 01 b2|01 00 |...r............| PAGE_BTR_SEG_TOP record_header (5) 00010060 02 00 41 69 6e 66 69 6d 75 6d 00 05 00 0b 00 00 |..Ainfimum......| 00010070 73 75 70 72 65 6d 75 6d| 00 00 10 ff f3 7f ff ff |supremum........| 00010080 ff 80 00 00 01 00 00 18 ff f3 7f ff ff fe 80 00 |................| 00010090 00 02 00 00 20 ff f3 7f ff ff fd 80 00 00 03 00 |.... ...........| 000100a0 00 28 ff f3 7f ff ff fc 80 00 00 04 00 00 00 00 |.(..............| 000100b0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00013ff0 00 00 00 00 00 70 00 63 9c d3 03 9b f4 78 5b 47 |.....p.c.....x[G| page 5 INDEX page 00014000 10 90 9a 69|00 00 00 05 |ff ff ff ff|00 00 00 06 |...i............| FIL_PAGE_OFFSET FIL_PAGE_PREV FIL_PAGE_NEXT 00014010 00 00 00 00 f4 78 5b 23 45 bf 00 00 00 00|00 00 |.....x[#E.......| 00014020 00 00 00 00 00 1b|00 02 37 62 80 04 1b f5 1b 75 |........7b.....u| SPACE_ID 00014030 00 00 00 05 00 00|00 01| 00 00 00 00 00 00 00 00 |................| PAGE_N_RECS PAGE_MAX_TRX_ID 00014040 |00 00|00 00 00 00 00 00 00 34|00 00 00 00 00 00 |.........4......| PAGE_LEVEL INDEX_ID PAGE_BTR_SEG_LEAF 00014050 00 00 00 00|00 00 00 00 00 00 00 00 00 00|01 00 |................| PAGE_BTR_SEG_TOP record_header (5) 00014060 02 00 1d 69 6e 66 69 6d 75 6d 00 02 00 0b 00 00 |...infimum......| 00014070 73 75 70 72 65 6d 75 6d| 58 9b 00 00 00 10 ff f0 |supremumX.......| 00014080 80 00 00 01 00 00 00 00 0d 55 b6 00 00 01 57 01 |.........U....W.| 00014090 10 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |.aaaaaaaaaaaaaaa| 000140a0 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |aaaaaaaaaaaaaaaa| * 00015be0 61 61 61 61 61 61 61 61 61 7f ff ff ff 58 9b 00 |aaaaaaaaa....X..| 00015bf0 00 00 18 00 00 80 00 00 02 00 00 00 00 0d 56 b7 |..............V.| 00015c00 00 00 01 58 01 10 61 61 61 61 61 61 61 61 61 61 |...X..aaaaaaaaaa| 00015c10 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |aaaaaaaaaaaaaaaa| * 00017750 61 61 61 61 61 61 61 61 61 61 61 61 61 61 7f ff |aaaaaaaaaaaaaa..| 00017760 ff fe 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00017770 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00017ff0 00 00 00 00 00 70 00 63 10 90 9a 69 f4 78 5b 23 |.....p.c...i.x[#| page 6 INDEX page 00018000 ed 83 1a c5|00 00 00 06 |00 00 00 05|00 00 00 07| |................| FIL_PAGE_OFFSET FIL_PAGE_PREV FIL_PAGE_NEXT 00018010 00 00 00 00 f4 78 5b 23 45 bf 00 00 00 00|00 00 |.....x[#E.......| 00018020 00 00 00 00 00 1b|00 02 37 62 80 04 00 00 00 00 |........7b......| SPACE_ID 00018030 1b f5 00 05 00 00|00 02| 00 00 00 00 00 00 00 00| |................| PAGE_N_RECS PAGE_MAX_TRX_ID 00018040 |00 00|00 00 00 00 00 00 00 34|00 00 00 00 00 00 |.........4......| PAGE_LEVEL INDEX_ID PAGE_BTR_SEG_LEAF 00018050 00 00 00 00|00 00 00 00 00 00 00 00 00 00|01 00 |................| PAGE_BTR_SEG_TOP record_header (5) 00018060 02 00 1d 69 6e 66 69 6d 75 6d 00 03 00 0b 00 00 |...infimum......| 00018070 73 75 70 72 65 6d 75 6d| 58 9b 00 00 00 10 1b 75 |supremumX......u| 00018080 80 00 00 02 00 00 00 00 0d 56 b7 00 00 01 58 01 |.........V....X.| 00018090 10 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |.aaaaaaaaaaaaaaa| 000180a0 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |aaaaaaaaaaaaaaaa| * 00019be0 61 61 61 61 61 61 61 61 61 7f ff ff fe 58 9b 00 |aaaaaaaaa....X..| 00019bf0 00 00 18 e4 7b 80 00 00 03 00 00 00 00 0d 5a ba |....{.........Z.| 00019c00 00 00 01 59 01 10 61 61 61 61 61 61 61 61 61 61 |...Y..aaaaaaaaaa| 00019c10 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |aaaaaaaaaaaaaaaa| * 0001b750 61 61 61 61 61 61 61 61 61 61 61 61 61 61 7f ff |aaaaaaaaaaaaaa..| 0001b760 ff fd 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 0001b770 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 0001bff0 00 00 00 00 00 70 00 63 ed 83 1a c5 f4 78 5b 23 |.....p.c.....x[#| page 7 INDEX page 0001c000 af a6 e2 6c|00 00 00 07 |00 00 00 06|ff ff ff ff| |...l............| FIL_PAGE_OFFSET FIL_PAGE_PREV FIL_PAGE_NEXT 0001c010 00 00 00 00 f4 78 5b 23 45 bf 00 00 00 00|00 00 |.....x[#E.......| 0001c020 00 00 00 00 00 1b|00 02 1b ed|80 03 00 00 00 00 |................| SPACE_ID 0001c030 00 80 00 05 00 00|00 01| 00 00 00 00 00 00 00 00| |................| PAGE_N_RECS PAGE_MAX_TRX_ID 0001c040 |00 00|00 00 00 00 00 00 00 34|00 00 00 00 00 00 |.........4......| PAGE_LEVEL INDEX_ID PAGE_BTR_SEG_LEAF 0001c050 00 00 00 00|00 00 00 00 00 00 00 00 00 00|01 00 |................| PAGE_BTR_SEG_TOP record_header (5) 0001c060 02 00 1d 69 6e 66 69 6d 75 6d 00 02 00 0b 00 00 |...infimum......| infimum record\\0 record_header(5) 0001c070 73 75 70 72 65 6d 75 6d| 58 9b 00 00 00 10 ff f0 |supremumX.......| supremum 0001c080 80 00 00 04 00 00 00 00 0d 5c bb 00 00 01 5a 01 |.........\\....Z.| 0001c090 10 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |.aaaaaaaaaaaaaaa| 0001c0a0 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 |aaaaaaaaaaaaaaaa| * 0001dbe0 61 61 61 61 61 61 61 61 61 7f ff ff fc 00 00 00 |aaaaaaaaa.......| 0001dbf0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 0001fff0 00 00 00 00 00 70 00 63 af a6 e2 6c f4 78 5b 23 |.....p.c...l.x[#| page 8 fresh page 00020000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00024000 详细拆解后：\nsegment inode page (page 2) BTR_SEG_TOP 0000 001b 0000 0002 0032 segment inode entry 00 00 00 00 00 00 00 01 FSEG_ID (01) 00 00 00 00 FSEG_NOT_FULL_N_USED 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FREE 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 FSEG_NOT_FULL 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FULL 05 d6 69 d2 FSEG_MAGIC_N 00 00 00 03 FSEG_FRAG_ARR 0..31 (03) (未用使用FF填充) ff ... ff BTR_SEG_LEAF 0000 001b 0000 0002 00f2 segment inode entry 00 00 00 00 00 00 00 02 FSEG_ID (02) 00 00 00 00 FSEG_NOT_FULL_N_USED 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FREE 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 FSEG_NOT_FULL 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FULL 05 d6 69 d2 FSEG_MAGIC_N 00 00 00 05 00 00 00 06 00 00 00 07 FSEG_FRAG_ARR 0..31 (05 06 07) ff ... ff BTR_SEG_TOP 0000 001b 0000 0002 01b2 segment inode entry 00 00 00 00 00 00 00 03 FSEG_ID (03) 00 00 00 00 FSEG_NOT_FULL_N_USED 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FREE 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 FSEG_NOT_FULL 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FULL 05 d6 69 d2 FSEG_MAGIC_N 00 00 00 04 FSEG_FRAG_ARR 0..31 (04) ff ... ff BTR_SEG_LEAF 0000 001b 0000 0002 0272 segment inode entry 00 00 00 00 00 00 00 04 FSEG_ID (04) 00 00 00 00 FSEG_NOT_FULL_N_USED 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FREE 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 FSEG_NOT_FULL 00 00 00 00 ff ff ff ff 00 00 ff ff ff ff 00 00 FSEG_FULL 05 d6 69 d2 FSEG_MAGIC_N ff ... ff FSEG_FRAG_ARR 0..31 (FF) page 3(聚簇索引 root page) BTR_SEG_TOP 0000 001b 0000 0002 0032 BTR_SEG_LEAF 0000 001b 0000 0002 00f2 \u0026lt; PK , page_no \u0026gt; 00 10 00 11 00 0e \u0026lt;80 00 00 01,00 00 00 05\u0026gt; 00 00 00 19 00 0e \u0026lt;80 00 00 02,00 00 00 06\u0026gt; 00 00 00 21 ff d6 \u0026lt;80 00 00 04,00 00 00 07\u0026gt; page 4(辅助索引 root page) BTR_SEG_TOP 0000 001b 0000 0002 01b2 BTR_SEG_LEAF 0000 001b 0000 0002 0272 \u0026lt; key , PK \u0026gt; 00 00 10 ff f3 \u0026lt;7f ff ff ff,80 00 00 01\u0026gt; 00 00 18 ff f3 \u0026lt;7f ff ff fe,80 00 00 02\u0026gt; 00 00 20 ff f3 \u0026lt;7f ff ff fd,80 00 00 03\u0026gt; 00 00 28 ff f3 \u0026lt;7f ff ff fc,80 00 00 04\u0026gt; record header (5) row id (4) trx id (6) rollback pointer (7) page 5 58 9b 00 00 00 10 ff f0 80 00 00 01 00 00 00 00 0d 55 b6 00 00 01 57 01 10 | 61 ... 61 | 7f ff ff ff 58 9b 00 00 00 18 00 00 80 00 00 02 00 00 00 00 0d 56 b7 00 00 01 58 01 10 | 61 ... 61 | 7f ff ff fe page 6 58 9b 00 00 00 10 1b 75 80 00 00 02 00 00 00 00 0d 56 b7 00 00 01 58 01 10 | 61 ... 61 | 7f ff ff fe 58 9b 00 00 00 18 e4 7b 80 00 00 03 00 00 00 00 0d 5a ba 00 00 01 59 01 10 | 61 ... 61 | 7f ff ff fd page 7 58 9b 00 00 00 10 ff f0 80 00 00 04 00 00 00 00 0d 5c bb 00 00 01 5a 01 10 | 61 ... 61 | 7f ff ff fc secondary index #  辅助索引的非叶子节点存储的是记录格式为\u0026lt;index_key，primary_key，page_no\u0026gt;，这里注意辅助索引的非叶子节点存放了主键值信息。另外，辅助索引节点的记录不保存系统列 trx_id和rollback point。\n这里给读者留2个问题：\n 为什么非叶子节点的记录中要包含primary key？ 辅助索引记录不包吃trx id和rollback point，那么MVCC正常吗？   辅助索引又称为二级索引或者非聚簇索引，其叶子节点保存的是\u0026lt;index_key，primary_key/primary记录的地址\u0026gt;。从这里可以看出，value可以有两种形式：\n 记录的主键值 记录的物理地址  MyISAM采用第一种方式存储，即采用堆表（heap table）的方式组织数据，即完整的记录存放在堆表中，主键和其他索引都存放的是记录的物理地址，这些索引的区别只是是否唯一、非空。所以MyISAM的索引和记录的关系，和InnoDB的对比如下图所示：\nInnoDB和MyISAM的辅助索引对比见以下表格：\n    InnoDB MyISAM     读 通过辅助索引仅能得到记录的主键值，查询完整记录还需要通过聚簇索引的B+ tree回表查询（bookmark lookup） 只需要一次B+ tree即可访问完整记录   写 仅主键变更需要更新辅助索引 记录发生改变，需要更新所有索引   B+树高度 聚簇索引通常比辅助索引的高度要高 低   范围查询 聚簇索引高效（顺序IO），辅助索引相对低效 低效（离散IO）    node pointer #  在clustered index和secondary index中，leaf page存储的是实际的数据，non-leaf page（n_level != 0）——存储的是node pointer（也称为router，指示牌\u0026hellip;），除了key之外，有一个指针用于执行下层的页地址。\nInnoDB中的B+ tree是一个ordinary B+ tree，而不是B-link tree。  node pointer也是由key-value构成，其中key需要可以唯一标识index record，value为指向下层的page no，（意味着下层的所有页\u0026lt;key，且non-leaf中下层的最左节点的key也是该key，leaf node是实际数据），对于key，不同的索引类型key有所不同：\n clusterd index：PK + \u0026lt; 系统列\u0026gt; non-unique secondary index：index_column, PK unique seconary index：index_column  另外，在每层的node pointer，最左节点都会设置一个特殊的bit来标记minimum record，即left most node of this level，数据表示为\u0026lt;min, page no\u0026gt;。这是为了更改好的组织树形结构的指针，对于非叶子节点，其page no指向下一层最左边的记录，即指向存储比本页面所有key都要小的记录页面。\nfill factor #  在InnoDB中，聚簇索引与辅助索引中叶子节点实际存放的记录数量还和填充因子（fill factor）有关。当填充因子小于1/2时需要进行页的合并，因此常态下填充因子肯定总是大于1/2的。此外，填充因子还受到插入的影响，如果是顺序插入，那么页的填充因子比较高，可以达到90%甚至更高。如果插入是无序的，那么按照Jim Gray所提到的，填充因子一般为69%。\n对于聚集索引，如果主键是自增ID，那么插入数据是顺序IO；如果副主索引是时间维度的key，那么也是顺序的，都可以有较高的填充因子。而如果主键为散列ID，比如UUID，那么插入就会无序，插入性能会有明显的退化，所以在线上场景中，尽量避免使用UUID类似的值作为主键。。\nconcurrency control #  B+ tree的concurrency control包括两个方面：\n 事务并发访问数据内容的并发控制，也被称为concurrency control protocol，比如MV2PL、MVTO、MVOCC 线程并发访问内存数据结构的并发控制，也被称为multi-threaded index concurrency control  在DBMS中，这两个目标用两种不同的机制来实现（lock和latch）。\n这二者的区别在此不介绍。\n以下内容buf_block-\u0026gt;lock代表frame latch，简称为block-\u0026gt;lock，也称为page latch。\nInnoDB中的B+ tree concurrency control #  在本章我们只讨论第二种情况，即如何用latch对B+ tree提供并发控制机制。在之前的介绍中我们知道，在B+ tree有非叶子节点和叶子节点，这两者都可以通过page表示，而page通过PCB保护。同时，InnoDB还要保证每一个index（B+树）的一致性，因此需要维护两种rw lock：\n index级别：保护B+ tree结构的一致性（非叶子节点），通过dict_index_t→lock保护整个B+ tree（不包含叶子节点），并且每个branch node通过block→lock保护。 block级别：保护叶子节点的一致性，通过block→lock保护叶子节点，并且上面的branch node也通过block→lock保护，所以block→lock统一用于保护PCB.frame（即数据页）的内部结构（页）的一致性  另外，rw lock保护的对象级别越高，冲突的可能性就越大，并发瓶颈也就越容易出现。\nB+ tree的latch并发控制策略为：\n 搜索：在B+ tree的遍历中，首先通过index→lock的方式遍历非叶子节点，当达到叶子节点时，加上block→lock后，释放index→lock。同时，在MySQL 5.7之前，为了节省CPU时间，只对index（非叶子节点）加一个s-latch，并不对中间遍历的每一个branch node加任何latch。 如果需要重组B+ tree（悲观插入记录），则需要对index加x-latch，并对页进行分裂  在叶子节点上确定split point 分配一个新的page 将新生成的node pointer更新到上一层的no-leaf page 释放index x-latch 将split point的记录移动到新的page    因此，InnoDB对B+树的并发控制的优化历史过程如下图所示：\n传统的B+ tree concurrency control #  单个节点并发控制\n如果要修改B+ tree的内容或者结构，必须先把该B+ tree节点读取到内存中，修改后再写回磁盘（Read-Modify-Write）。因此，在多线程并发的场景下，该节点在内存中被一个线程读取时，不能被另一个线程修改，这种场景就是典型的多线程编程中共享数据的临界区问题。数据库中使用latch来控制单个B+ tree节点的访问，从而保持 B+ tree物理结构的一致性，通常的做法是在每个节点（索引页）的控制块（PCB）中嵌入一个latch。\nlatch coupling\n当一个线程沿着某个路径把指针从一个节点移动到另一个节点时，比如从父节点移动到子节点，或者在叶子节点前后移动，在这期间不允许其它线程去改变这个指针。这个时候需要持有后续节点的latch后，才能释放当前节点，这种方法通常称为 ”latch crabbing”，也被称为蟹行。\n如果后续节点不在内存中时，还需要一次磁盘 IO 来获取该节点，因为latch的持有时间必须很短，在等待IO时，不应该长时间占据当前节点的latch，而应该释放。在后续节点的IO操作结束之后，再重新进行一次从root到leaf的遍历来获得之前当前节点的latch，以避免在等待IO的时间窗口内其他线程对B+树结构的修改导致的不一致问题。这个重新遍历的代价并不大，因为可以通过检查上一次遍历时保存的路径是否还有效，来重用之前的路径，当需要的节点已经从磁盘读取到内存池中时，它的祖先节点可能还没有被其它线程修改过。\n反向遍历 level list\nlatch coupling中除了上面提到的从父节点到子节点遍历的情况，还有一种是同层相邻节点遍历的场景，比如在范围查询时需要沿着leaf链表正向或反向遍历，而asc+desc会由于遍历的方向不同导致死锁。为了避免相向遍历产生的死锁，一种方法是让latch立即重试，即当一个 latch 被其它线程占有而获取不到时，立即返回失败而不是继续等待latch被其它线程释放，从而让冲突的对方能继续执行下去，而自己进行一次从root到leaf的重试。这里要考虑的一个问题是，如何避免两个冲突的线程同时重试的情况，因为同时重试后有可能还在相同的地方发生冲突，可以规定一个遍历方向的优先级，这样可以保证冲突时只有一个线程会重试，另一个线程会继续执行。\n递归向上更新\n另外，在对B+ tree的节点更改时可能会导致页的合并和分裂。最简单的解决办法是对整个B+ tree加一个互斥锁，但是这样太影响多线程并发，最好的方法应该是只对B+ tree的某些节点加锁（范围），可以有如下几种策略：\n 在从上到下查找目标节点时，就把整个路径节点加X-latch，这样在从下到上的节点变更时就不再需要加锁。很明显的这种方法每次都会锁住root节点，跟锁住整个B+树没有本质区别，严重影响 B-tree 的并发性。 在从上到下遍历时给查找路径加S-latch，在必要时再将S-latch升级成S-latch，升级过程中需要检查死锁的风险。由于这种方法是可能失败的，因此需要有额外的备选方案，增加了逻辑的复杂度。 引入一种共享互斥锁（SX-latch），从上到下遍历时给路径上的节点都加 SX-latch，SX-latch可以与S-latch相容，从而不会阻塞其它线程的读请求。但是SX-latch无法与自身相容，因此对于并发更新来说B+树的root节点依然是一个瓶颈，只允许一个线程进行修改B+树结构的操作。  这三种方法都需要一直持有遍历路径上节点的latch，直到一个节点不再会触发向上更新才释放路径上的所有 latch。实际情况是大多数节点都不是满的，因此大多数插入操作都不会触发节点分裂并向上变更，锁住整个路径的节点是没有必要的。如果在插入操作的从上到下遍历时主动进行节点分裂，就能避免了根节点的瓶颈问题，也没有升级 latch 时失败的问题。缺点是需要在实际分裂之前预先分配空间，造成一定的空间浪费，并且在可变长记录更新时无法准确地判断是否需要预先分裂。\n为了解决不必要的对节点加互斥锁，可以在第一次从上到下遍历时加共享锁，直到一个节点需要分裂时，重新回到 root 做一次遍历，这次给要分裂的节点加互斥锁，并进行实际的分裂操作。第二次遍历时可以通过检查第一次遍历保存的路径来进行重用，而不必从根节点重新遍历。\n以上参考了以下文章\nMySQL · 引擎特性 · InnoDB index lock前世今生\nMySQL · 内核特性 · InnoDB btree latch 优化历程\nDatabase · 理论基础 · B-tree 物理结构的并发控制\nB+ tree SMO #  页的分裂 #  页的分裂会对性能和成本都会有所影响，所以InnoDB对B+ tree的分裂进行了优化，使其更符合磁盘的特性：并不总是将中间记录作为分裂点，而是需要根据插入的情况进行判断，从而更为有效地利用磁盘空间。\n比如，如果记录是严格按照递增顺序进行插入的，页P1中有记录1、2、3、4、5、6、7、8、9、10。如果按照中间节点进行分裂的策略，那么分裂完成后页P1中有记录1，2，3，4，页P2中有记录5，6，7，8，9，10。如果顺序插入记录，这意味着之后不会再有记录向页P1中进行插入，会导致页P1的利用率很低，如果后续还有不断的顺序插入+分裂，每个页的填充率基本都在50%左右，这也会影响查询效率。\n因此，当页进行分裂操作时，首先需要通过页头（PAGE_HEADER）上的插入信息判断是否为连续插入模式，插入信息如下：\n PAGE_LAST_INSERT：最后插入记录的位置（offset） PAGE_DIRECTION：最后插入记录的方向 PAGE_N_DIRECTION：一个方向上连续插入记录的数量  页的分裂由btr_page_split_and_insert实现，分裂方式有两种：\n 50% - 50%算法：将旧页50%的数据量移动到新页 0% - 100%算法：不移动旧页任何的数据，只将引起分裂的记录插入到新页  如下图所示：\n在页的分裂插入，首先通过查找定位cursor（即insert point），然后确定分裂点（split rec），再进行页的分裂，最后插入新的记录（insert rec）。\n向右分裂和向左分裂都是有序插入（即连续插入），如果插入是无序的，则采用50% - 50%算法，页中的中间记录（middle record）作为分裂点记录（page_get_middle_rec）。\n在选择split rec时，不判断该记录是否为标记删除，即不对记录的delete flag进行判断。  分裂操作步骤如下：\n 确定split rec 从索引的数据段（LEAF段/TOP段）分配一个page，并对该页加x-latch 确定需要移动到新页中的第一条记录（first_rec）以及源页中的最后一条记录（move_limit） 进行B+树的分裂，在父节点添加node pointer记录，如果父节点空间不足，继续触发分裂操作 将记录移动到新的page中 确定insert rec待插入的page 将insert rec插入到page中 如果上述操作失败，对page reorganize，然后重新进行插入操作；如果还失败，回到步骤1再次尝试  这里需要注意，一次分裂操作可能会再次引起分裂。这是因为，待插入的记录非常大，如果第一次分裂完成后，左页占用了6000字节空间，右页占用10000字节，但需要往右页插入的记录一共占用8000字节，则右页还需要再进行一次分裂操作。这时分裂点记录的判断由btr_page_get_sure_split_rec实现。\n操作完成后，叶子节点的x-latch由mtr.commit释放，而index的x-latch在步骤4结束之后就进行释放，以减少对于index的竞争。\n连续向右插入后的分裂 #  通过启发式算法（eager heuristics）策略决定分裂点，当前insert point之后是否有两条记录，如果有，则采用50%-50%算法分裂；反之采用0%-100%算法分裂，如下图所示：\n下面是具体示例：\n连续向左插入后的分裂 #  向左分裂插入一共会有3种情况，如下图所示：\n这三种情况，第一种split rec为insert point，后两种split rec为insert point的下一个记录待分裂的记录，即\n 如果insert point为infimum或者infimum后的第一条记录，则split rec为insert point下一个 否则同值  \tif (infimum != insert_point \u0026amp;\u0026amp; page_rec_get_next(infimum) != insert_point) { *split_rec = insert_point; } else { *split_rec = page_rec_get_next(insert_point); } 向左分裂的的这三种情况如下图所示：\n在构建B+ tree插入的例子之前，我们先做如下假设：\n 页面扇出为3，即每个页面（non-leaf page和leaf page）最多可以插入3条记录，插入更多的记录，会产生页的分裂 插入的数据序列为10，20，5，8，23，22，50，21，53，40，9  第一步，我们首先插入10，20，5这3条记录，因为在插入时该B+树只有一个空的root page（位于non-leaf segment），所以在插入后，页面如下图所示：\n第二步，插入8，这时root page已满，需要进行页的分裂。我们之前已经知道，non-leaf page和leaf page存储在不同的segment中，所以我们需要从leaf segment中申请一个新的page，然后将root page的数据搬过来。从这里我们可以看到，在B+ tree的分裂中，root page始终是不会变的，无论变成多大的树，root page的page no始终如一。具体过程如下：\n 新创建一个leaf page（page 101） 将root page中的全部记录复制到leaf page中 清除原root page的全部记录，并构建minimum record的node pointer，其中的page no指向子节点页面page 101  第三步，虽然完成了root page的分裂，但page 101仍然没有空间可以存储接着待插入的8，所以还需要继续分裂，只不过这次是leaf page的分裂：\n 新创建一个leaf page（page 102） 将page 101的一部分记录（记录20）移到page 102中 将page 101和102组成双向链表（结成兄弟关系） 将page 102中的20构建父节点的node pointer，即\u0026lt;20, page 102\u0026gt;，并更新到父节点（page 100）（结成父子关系）  第四步，插入8，这时通过查找定位可以知道，insert point位于page 101的\u0026lt;10, data\u0026gt;。同理，插入23，22。\n第五步，插入50，这时首先定位到page 102，发现页已满，需要首先进行页的分裂，过程和第三步一样：\n 新创建一个leaf page（page 103） 将page 102的一部分记录（记录23）移到page 103中 将page 102和103组成双向链表（结成兄弟关系） 将page 103中的23构建父节点的node pointer，即\u0026lt;23, page 103\u0026gt;，并更新到父节点（page 100）（结成父子关系）  第六步，从root page开始重新搜索，定位到page 103，插入记录50，以同样的方式在page 102插入记录21，在page 103插入记录53\n第七步，插入记录40，定位到page 103，发现page 103已满，同样需要分裂，这里不再详述页的分裂步骤了，和前面相同。\n第八步，插入记录9，定位到page 101，发现page 101已满，同样进行页的分裂，此时的B+树处在变化中，不是完整的：\n但是\u0026hellip;\u0026hellip; root page已满，需要进行root page的分裂了。而我们同时也知道root page必须始终是root page，所以我们需要在non-leaf segment中申请一个新的page 106，并将page 100的全部数据移动到page 106中（相当于leaf page 101 105 102 103 104的父节点都从page 100变为page 106），并且page 100的minimum record也会指向page 106。\n这时我们尝试将之前pending的\u0026lt;10, page 105\u0026gt;插入到page 106中，但是page 106已满，我们需要页的分裂，此时分裂是non-leaf page的分裂，其分裂方式和leaf page的分裂一样：\n 新创建一个non-leaf page（page 107） 将page 106的一部分记录（记录53）移到page 107中 将page 106和107组成双向链表（结成兄弟关系） 将page 107中的记录53构建父节点（root page）的node pointer，即\u0026lt;53, page 107\u0026gt;，并更新到父节点（page 100）（结成父子关系）  这时，我们有了足够的空间用于容纳\u0026lt;10, page 105\u0026gt;，我们将其插入到page 106，让page 105\u0026quot;找到了爸爸\u0026quot;。\n第九步，最终我们要在page 1010中插入记录9，所有的步骤完成。\n我们将leaf page中的记录按照从左到右的顺序读入：5 8 9 10 20 21 22 23 40 50 53，即索引序。\n页的合并 #  当对页中的记录进行删除或者更新后，页的填充率可能会低于50%（BTR_CUR_PAGE_COMPRESS_LIMIT），这时会尝试进行页的合并，即页中的记录合并到页的左兄弟/右兄弟页中。如果待被合并的页为当前层的最后一个页，则需要减少B+树的高度（btr_lift_page_up）。与传统的B+ tree SMO不同的是，InnoDB并不要求合并操作一定发生，如果左右兄弟页没有空间进行合并，则放弃合并操作。\n合并页（btr_compress）总是判断先判断左兄弟页，如果左兄弟页存在，则将记录合并到左兄弟页中。如果左兄弟页没有足够空间，则尝试进行右兄弟页合并。\n如果合并的是叶子节点，则开始合并前（函数调用前）要保证已经对其左右兄弟节点加x-latch。\n页的回收 #  从上面我们可以得知，随着数据的插入，B+ tree会不断的分裂，从而导致B+ tree成为一个矮胖子。如果数据不断的删除，传统的B+ tree都会有一个fill factor来控制page中数据的删除比例，如果达到阈值，则进行页的合并。\n在InnoDB中的记录删除，分为以下3种情况：\n 在删除记录时，如果该page只剩下一条记录，就直接将该page回收（也只有在这种情况下，InnoDB才会回收page）。从这里我们可以看到InnoDB和传统的B+ tree的删除算法的不同，也可以说，InnoDB除了此情况外，根本不存在页的合并这么一个说法。 当然，这个page不能是root page 如果要删除的记录是non-leaf page，并且为该page的最左边记录（infimum的下一条记录），则说明此时的记录删除还会影响上一层的父节点node pointer。所以此时还要更新其父节点node pointer（可能出现继续向上递归）。 此时，可以放心的将本page要删除的记录删除掉，记录删除的操作就完成了。  对于#1，从B+ tree中摘除一个page的流程如下：\n 首先判断该page的同层兄弟节点，如果没有任何兄弟，则认定该层只有一个节点，并且为空节点。这样可以递归的将整个B+ tree释放掉，最后只剩下一个空的root page 找到该page上一层的父节点node pointer，删除该node pointer记录的流程参照上面的#2 将该page从前后兄弟的双向链表中摘除 归还到本表空间的碎片管理链表中  对于#2，更新父节点node pointer的流程如下：\n 根据当前的待删除记录找到父节点中的node pointer，通过递归删除的方法将其删除 根据待删除记录的下一条记录，构造一个新的node pointer，其中指向子节点的page no不变 将#2构造的node pointer插入到当前节点的父节点中，即更新父节点中的node pointer  页的查找 #  通过B+ tree查找是InnoDB中最为常见的操作，无论是查询，还是增删改，都需要首先定位到所需操作的记录。在InnoDB中，通过btr_cur_search_to_nth_level来查找指定的记录。\nbtr_cur_search_to_nth_level的函数原型为：\n/********************************************************************//** Searches an index tree and positions a tree cursor on a given level. NOTE: n_fields_cmp in tuple must be set so that it cannot be compared to node pointer page number fields on the upper levels of the tree! Note that if mode is PAGE_CUR_LE, which is used in inserts, then cursor-\u0026gt;up_match and cursor-\u0026gt;low_match both will have sensible values. If mode is PAGE_CUR_GE, then up_match will a have a sensible value. */ void btr_cur_search_to_nth_level( /*========================*/ dict_index_t*\tindex,\t/*!\u0026lt; in: index */ ulint\tlevel,\t/*!\u0026lt; in: the tree level of search */ const dtuple_t*\ttuple,\t/*!\u0026lt; in: data tuple; NOTE: n_fields_cmp in tuple must be set so that it cannot get compared to the node ptr page number field! */ page_cur_mode_t\tmode,\t/*!\u0026lt; in: PAGE_CUR_L, ...; NOTE that if the search is made using a unique prefix of a record, mode should be PAGE_CUR_LE, not PAGE_CUR_GE, as the latter may end up on the previous page of the record! Inserts should always be made using PAGE_CUR_LE to search the position! */ ulint\tlatch_mode, /*!\u0026lt; in: BTR_SEARCH_LEAF, ..., ORed with at most one of BTR_INSERT, BTR_DELETE_MARK, BTR_DELETE, or BTR_ESTIMATE; cursor-\u0026gt;left_block is used to store a pointer to the left neighbor page, in the cases BTR_SEARCH_PREV and BTR_MODIFY_PREV; NOTE that if has_search_latch is != 0, we maybe do not have a latch set on the cursor page, we assume the caller uses his search latch to protect the record! */ btr_cur_t*\tcursor, /*!\u0026lt; in/out: tree cursor; the cursor page is s- or x-latched, but see also above! */ ulint\thas_search_latch, /*!\u0026lt; in: latch mode the caller currently has on search system: RW_S_LATCH, or 0 */ const char*\tfile,\t/*!\u0026lt; in: file name */ ulint\tline,\t/*!\u0026lt; in: line where called */ mtr_t*\tmtr);\t/*!\u0026lt; in: mtr */ btr_cur_search_to_nth_level中的mode表示查询模式，但在具体的查询场景上有不同的使用限制：\n 非叶子节点上进行查询时，其mode只能为LE/L，GE→L G→ LE 对于insert，其在叶子节点上总是通过LE进行定位待插入记录的前一条记录 对主键和唯一索引进行查询时，只能为GE，而不能为LE，这是因为InnoDB支持MVCC，因此即使有唯一性约束，但在实际的页中仍然可能包含多个键值相同的记录，只是其中仅有一个记录的delete flag=0，并且该记录总是在最后一个  比如如下的叶子节点的主键记录，对于用户来说，看到的主键记录只有1, 2, 3, 4, 5：(1, \u0026lsquo;A\u0026rsquo;), *(2, \u0026lsquo;B\u0026rsquo;), (2, \u0026lsquo;C\u0026rsquo;), *(3, \u0026lsquo;D\u0026rsquo;), (3, \u0026lsquo;E\u0026rsquo;), (4, \u0026lsquo;F\u0026rsquo;), (5, \u0026lsquo;G\u0026rsquo;)如果根据LE查询为3的记录，则会得到记录的前一个页，很显然这会引起并发错误。  这里留给读者一个问题：为什么删除的记录在左边，delete=flag=0的记录一定在右边呢？  非叶子节点的mode #  对于非叶子节点，其mode只能为LE/L，即GE→L，G→ LE。\n/* We use these modified search modes on non-leaf levels of the B-tree. These let us end up in the right B-tree leaf. In that leaf we use the original search mode. */ switch (mode) { case PAGE_CUR_GE: page_mode = PAGE_CUR_L; break; case PAGE_CUR_G: page_mode = PAGE_CUR_LE; break; default: page_mode = mode; break; } 这是由B+ tree的特点所决定的，即依左搜索（上面图中的灰色框），然后通过取下一个记录的方式实现mode翻转，恢复正常（L→GE G→LE ）。这样会遇到一个问题，即通过mode L、LE定位到的low为supremum，需要再搜索下一页的第一条用户记录才能得到最终的记录（high）。\n/* PHASE 4: Look for matching records in a loop */ if (page_rec_is_supremum(rec)) { /* A page supremum record cannot be in the result set: skip it now that we have placed a possible lock on it */ prev_rec = NULL; goto next_rec; } 这里留给读者一个问题：B+ tree的特点为什么决定了要依左搜索？  latch_mode #  latch_mode表示在搜索过程中需要对page和index加的latch种类：\n   latch_mode 说明     BTR_SEARCH_LEAF 查找leaf page，index加s-latch，leaf page加s-latch后释放index s-latch   BTR_MODIFY_LEAF 修改leaf page，index加s-latch，leaf page加x-latch后释放index s-latch   BTR_NO_LATCHES index加s-latch，leaf page不加任何latch   BTR_MODIFY_TREE 修改B+树，index加x-latch，leaf page加x-latch   BTR_CONT_MODIFY_TREE 修改B+树（继续），函数开始前对index已加x-latch，对leaf page已加x-latch   BTR_SEARCH_PREV 查找leaf page→prev，对leaf page加s-latch   BTR_MODIFY_PREV 修改leaf page→prev，对leaf page加x-latch   BTR_SEARCH_TREE 搜索B+树   BTR_CONT_SEARCH_TREE 搜索B+树（继续）    如果btr_cur_search_to_nth_level最终定位到的页不是叶子节点，即level大于0，那么对页加x-latch，同时不释放索引内存对象上的latch保护；否则根据latch_mode对叶子节点加上latch保护，并根据latch_mode选择是否释放索引对象上的latch保护。\n从上面的latch_mode中我们可以发现，InnoDB总是先对index加上s-latch保护，随后进行页的操作，如果insert/update/djelete操作不会引起非叶子节点发生变化，即不会发生分裂、合并、树的高度变化，则在定位到leaf page后立即释放index的s-latch，这种方式称为乐观方式；否则，立即释放index及leaf page上的latch，并通过悲观方式对index和leaf page加x-latch保护。\n这里的latch_mode指定是朴素的latch方式，也就是优化前的index concurrency control  另外，latch_mode中还包含以下的flag：\n BTR_INSERT BTR_ESTIMATE BTR_IGNORE_SEC_UNIQUE BTR_DELETE_MARK BTR_DELETE BTR_ALREADY_S_LATCHED BTR_LATCH_FOR_INSERT BTR_LATCH_FOR_DELETE BTR_RTREE_UNDO_INS BTR_MODIFY_EXTERNAL BTR_RTREE_DELETE_MARK  BTR_ESTIMATE表示需要对返回的结果集数量进行预估，仅在RANGE查询开始前需要进行预估操作。\nbtr cursor #  btr cursor用于表示B+ tree的查找，其数据结构如下：\nstruct btr_cur_t { dict_index_t*\tindex;\t此次查询所使用的的索引 page_cur_t\tpage_cur;\tpage cursor purge_node_t*\tpurge_node;\t/*!\u0026lt; purge node, for BTR_DELETE */ buf_block_t*\tleft_block;\t左兄弟页，用于BTR_SEARCH_PREV/BTR_MODIFY_PREV que_thr_t*\tthr;\t/*!\u0026lt; this field is only used when btr_cur_search_to_nth_level is called for an index entry insertion: the calling query thread is passed here to be used in the insert buffer */ // 以下通过btr_cur_search_to_nth_level赋值 enum btr_cur_method\tflag;\t使用何种方式查询得到的记录结果 BTR_CUR_HASH = 1\tAHI BTR_CUR_HASH_FAIL\tAHI-\u0026gt;B+ tree BTR_CUR_BINARY,\tB+ tree BTR_CUR_INSERT_TO_IBUF\t/*!\u0026lt; performed the intended insert to the insert buffer */ BTR_CUR_DEL_MARK_IBUF\t/*!\u0026lt; performed the intended delete mark in the insert/delete buffer */ BTR_CUR_DELETE_IBUF\t/*!\u0026lt; performed the intended delete in the insert/delete buffer */ BTR_CUR_DELETE_REF\t/*!\u0026lt; row_purge_poss_sec() failed */ ulint\ttree_height;\t查询所在的层 ulint\tup_match;\tpage_cur_search_with_match设置 ulint\tup_bytes;\tpage_cur_search_with_match设置 ulint\tlow_match;\tpage_cur_search_with_match设置 ulint\tlow_bytes;\tpage_cur_search_with_match设置 ulint\tn_fields;\tAHI介绍 ulint\tn_bytes;\tAHI介绍 ulint\tfold;\tfold value，用于BTR_CUR_HASH btr_path_t*\tpath_arr;\t查询得到记录所走过的路径（保存每个路径上的查询信息） }; 其中，btr_path_t用于描述B+ tree自上而下的查询路径，每一层一个slot表示（即一个btr_path）\nstruct btr_path_t { ulint\tnth_rec;\t查询到的记录是页中的第几个（包括伪记录） ulint\tn_recs;\t页中的记录总数（不包括伪记录） ulint\tpage_no;\tpage no ulint\tpage_level;\tpage level，通过page level的变化来感知是否发生页的重组 }; 比如在一个page中的记录信息如下：\n\trecords: (inf, a, b, c, d, sup) index of the record: 0, 1, 2, 3, 4, 5 如果查找的是记录c，则nth_rec=3，n_recs=4。\n我们通过nth_rec、n_recs可以预估一个range查询返回的记录数量（btr_estimate_n_rows_in_range）：\n 统计range查询时两次定位之间的记录数量，即n_rows 统计下层共有多少记录：n_rows = n_rows * 每个page的平均记录数 当查询到leaf page后，预估range的记录数量：如果树的层数\u0026lt;=2，则预估值 = n_rows；否则预估值 = 2 * n_rows  在B+ tree中一个索引页至少有2个记录，所以预估值最小为n_rows * 2。\n我们接下来看一个例子：\n比如range查询为[40, 120)，首先通过btr_cur_search_to_nth_level(PAGE_CUR_GE 40)定位到记录40，此时查询路径为：\npath1[0].nth_recs = 1, path1[0].n_rec = 4 // root page path1[1].nth_recs = 1, path1[1].n_rec = 4 // leaf page 如果通过btr_cur_search_to_nth_level(PAGE_CUR_LE 120)定位到记录120，则查询路径为：\npath2[0].nth_recs = 3, path2[0].n_rec = 4 // root page path2[1].nth_recs = 1, path2[1].n_rec = 3 // leaf page 预估的流程为：\n 统计root page页的两次记录之间的距离：n_rows = path2[0].nth_recs - path2[0].nth_recs = 3 - 1 = 2 计算leaf page的平均数量：(path1[1].n_rec + path2[1].n_rec) / 2 = (4 + 3) / 2 = 7 / 2 = 3 统计下层（leaf page）共有多少记录：n_rows = n_rows * (leaf page的平均数量) = 2 * 3 = 6 因为已到leaf page，且树的层数\u0026lt;=2，预估值 = n_rows = 6  可以看到range查询实际需要遍历的记录数为7，这里预估值为6，相差不多。\n更进一步，InnoDB还考虑了两次查询之间树的变化情况（高度、页）。\n持久游标 #  btr0cur模块用于对B+树索引进行search、insert、update、delete操作，并产生对应的undo log和redo log，并处理可能出现的page split和merge。但是，在大多数情况下，上层并不直接调用btr0cur中的这些相关函数，比如btr_cur_search_to_nth_level，而是通过持久游标（persistent cursor）的对象来处理select、update、delete，并且，查询到的记录也会保存在持久游标中。\n在进行select、update、delete操作时，首先需要定位到第一条记录，然后开始扫描下一个记录（fetch next record），直到扫描到不符合条件的记录为止。持久游标用于保存（btr_pcur_store_position）每次查询到的记录（保存在old_rec中），待查询下一条记录时，首先恢复上一次查询的记录（如果页没有发生改变，直接使用old_rec来定位下一条记录，否则，要根据old_rec中的索引键值重新定位记录再进行查询（btr_pcur_restore_position）），然后再获取下一条记录。这样设计的原因是当用户扫描记录时，页中的记录可能会发生变化。这时若按照之前的记录进行扫描，可能得到错误的情况。比如页中的记录发生了变化，或者页发生了split/merge。\nB+ tree中变更记录 #  插入记录 #  在InnoDB中，往B+ tree中插入记录采用乐观插入和悲观插入两个阶段，首先采用乐观插入（btr_cur_optimistic_insert），如果发现此次插入操作会导致页的分裂，则对页进行一次整理，再尝试一次，如果还是不行，再采用悲观插入（btr_cur_pessimistic_insert）。\n乐观插入 #  首先来看一下乐观插入的参数：\nbtr_cur_optimistic_insert( ulint flags, 0 BTR_NO_UNDO_LOG_FLAG 不需要记undo log，比如回滚时不需要再次产生undo log BTR_NO_LOCKING_FLAG 表示插入后不需要对查询的记录加锁。比如对于change buffer，不会对其进行并发的读取，因此不需要进行锁保护 BTR_KEEP_SYS_FLAG 不含有该bit时，更新记录的隐藏列rollback pointer。比如如果插入的是辅助索引或者非叶子节点，记录不含有隐藏列，需要设置该位 BTR_KEEP_POS_FLAG btr_cur_pessimistic_update() must keep cursor position when moving columns to big_rec BTR_CREATE_FLAG the caller is creating the index or wants to bypass the index-\u0026gt;info.online creation log BTR_KEEP_IBUF_BITMAP the caller of btr_cur_optimistic_update() or btr_cur_update_in_place() will take care of updating IBUF_BITMAP_FREE btr_cur_t* cursor, cursor定位到insert point（即待插入记录的前一条记录），模式为PAGE_CUR_LE | BTR_INSERT ulint** offsets,插入后的物理记录的original offset mem_heap_t** heap, dtuple_t* entry, 待插入的记录（逻辑记录） rec_t** rec, 插入后的记录（物理记录） big_rec_t** big_rec,如果插入的是大记录，返回插入后的大记录（extern指向的big rec） ulint n_ext, 大记录中存储的列数 que_thr_t* thr, query thread mtr_t* mtr) 乐观插入的执行流程如下：\n 将待插入的记录（逻辑记录）转换为物理记录，计算页是否有足够的空间来容纳。这里需要注意的是，InnoDB要求插入操作保留1/32页的大小作为预留空间（fill factor）。其目的是降低update操作时页的分裂概率。 如果插入的tuple需要转换为大记录格式，则首先将over-flow page的数据保存在big_rec中，待btr_cur_optimistic_insert返回DB_SUCCESS后，再向over-flow page插入big_rec。从这里可以看出，大记录的插入是分为两个阶段进行的。 进行碎片整理后，再次尝试插入 如果还没有足够的空间，则返回DB_FAIL 检查锁的信息，并生成对应的undo日志（btr_cur_ins_lock_and_undo）。若锁检测到下一个记录已经被其他事务持有锁，则等待，返回DB_WAIT_LOCK。 向页中插入记录（page_cur_tuple_insert） 更新AHI 不包含BTR_NO_LOCKING_FLAG，更新锁信息（lock_update_insert） 最后若插入的对象是插入缓冲时（插入缓冲本身也是一棵B+树），更新对应插入缓冲位图页中的信息。 函数执行完成后，会释放持有页的x-latch，如果还有行溢出数据（big_rec），则还要对index和leaf page加x-latch，再将溢出的列数据存放到over-flow page中（btr_store_big_rec_extern_fields）。  悲观插入 #  悲观插入的参数和乐观插入完全相同。\n但有以下几点不同：\n 持有了index的x-latch，影响了B+ tree的并发 为了避免死锁，除了持有page自身的x-latch外，还需要持有前后兄弟节点的x-latch 确定悲观插入后，需要预留一些区的空间，以保证B+ tree的结构变化一定有足够的磁盘空间（3个区）可以保证这次操作完成。插入操作完成后会释放这部分预留空间。 在InnoDB中，对于非叶子节点的插入都是悲观插入（btr_insert_on_non_leaf_level）  悲观插入的执行流程如下：\n 检查锁的信息，并生成对应的undo日志（btr_cur_ins_lock_and_undo）。若锁检测到下一个记录已经被其他事务持有锁，则函数返回DB_WAIT_LOCK。 将待插入的记录（逻辑记录）转换为物理记录，然后可以计算页是否有足够的空间来容纳，如果没有，返回DB_FAIL。 如果插入的tuple需要转换为大记录格式，则首先将over-flow page的数据保存在big_rec中，待btr_cur_optimistic_insert返回DB_SUCCESS后，再向over-flow page插入big_rec。 经过之前的检查，这一步确保记录是一定能够插入页中的： root page插入（btr_root_raise_and_insert） 非root page插入（btr_page_split_and_insert） 更新AHI（btr_search_update_hash_on_insert） 不包含BTR_NO_LOCKING_FLAG，更新锁信息（lock_update_insert） 最后若插入的对象是插入缓冲时（插入缓冲本身也是一棵B+树），更新对应插入缓冲位图页中的信息。 函数执行完成后，会释放持有页的x-latch，如果还有行溢出数据（big_rec），则还要对index和leaf page加x-latch，再将溢出的列数据存放到over-flow page中（btr_store_big_rec_extern_fields）。  更新记录 #  和插入操作一样，更新分为非主键更新和主键更新两个场景。这是因为，node pointer在clustered index和secondary index所存储的key是不同的，其leaf层存储的data也是不一样的。\nsecondary index上没有存储多版本信息，因此secondary index的更新，需要delete mark+insert，并通过clustered index维护update vector和index columns，来存储变化的delta。\n非主键更新 #  非主键更新指的是对非PK进行update，比如非PK列，或者辅助索引列。\n非主键更新分为乐观和悲观两种。其中，乐观更新又分为原地更新（update in place）和普通的乐观更新。\n原地更新 #  原地更新（row_upd_changes_field_size_or_external）是指更新记录中的各个列大小在更新过程中没有发生改变（注意，并不是记录的整体大小没有发生变化）。另外，含有extern属性的列不能原地更新。\n更新操作中变化列的新值保存在upd_t（update vector）中：\n/* Update vector structure */ struct upd_t{ mem_heap_t* heap; ulint info_bits; // info_bits dtuple_t* old_vrow; /*!\u0026lt; pointer to old row, used for virtual column update now */ ulint n_fields; // 新列的数量 upd_field_t* fields; // 新列 }; /* Update vector field */ struct upd_field_t{ unsigned field_no:16;// 列号 dfield_t new_val; // 新值 dfield_t* old_v_val; /*!\u0026lt; old value for the virtual column */ }; 对于辅助索引来说，一般都不是原地更新，这是因为如果更新了辅助索引的列，一般都会引起索引的位置发生变化（\u0026lsquo;AAA\u0026rsquo; → \u0026lsquo;BBB\u0026rsquo;），只有一种情况比较特殊，即只有字符集发生改变，且排序规则没有改变，这时才会原地更新。\nchange buffer tree也不能是原地更新。\n原地更新（btr_cur_update_in_place）的具体流程如下：\n 如果是压缩页，检查是否有足够的空间（btr_cur_update_alloc_zip） 检查lock，生成undo log 更新系统列（trx ID + rollback pointer） 更新AHI 将update vector更新到物理记录中 处理extern列（之前设置了delete mark，现在需要把extern列设为record owned） 更新insert buffer  乐观更新 #  普通的乐观更新就是delete mark+insert，具体流程如下图所示：\n其中，乐观更新（btr_cur_optimistic_update）的过程中仅需要持有记录所在页的x-latch。\n从上面的流程我们可以看出，乐观更新首先判断是否可以原地更新，如果不可以，再进行delete mark + insert。而在delete mark+insert中，因为记录还在同一个页中，并且主键并没有改变，只是新旧记录的heap no发生了变化，所以只需要更新锁信息即可（不需要创建锁）：将原来的锁信息移动到伪记录infimum上，待记录更新完成后再将伪记录上的锁移动到新记录上。记录的锁通过移动完成，不需要先删除后新建的方式来完成，提高了锁管理的效率。\n悲观更新 #  在悲观更新中，首先检查是否可以乐观更新，这是因为更新记录所在的page可能已经split，乐观更新有可能成功。\n对于锁信息的处理方式和乐观更新一样，即首先将更新记录的锁信息移动到伪记录infimum上，待更新完成后将锁信息移动回原记录上。但是，因为悲观更新会引起页的分裂操作，因此在某些情况下，需要对伪记录supremum锁进行修正。发生这种修正的条件为：\n 更新的记录不是页中的第一个用户记录 更新操作会引起页向右进行分裂 分裂点记录就是更新记录  那么这时分裂后原记录所在页的伪记录supremum会继承分裂点记录后的锁信息，并设置为gap类型。但是由于更新时记录本身已经有锁，因此伪记录supremum需要继承的是更新的记录，也就是分裂点记录锁的信息，而在分裂时，其锁信息被移动到了伪记录infimum上。\n主键更新 #  在真正的生产环境中，应该尽量避免更新主键。\n主键更新的流程：\n 原主键记录 delete mark insert新主键记录 purge线程判断是否有其他事务引用已删除的主键记录，如果没有，则彻底删除  这里我们对第2步中的extern列进行详细说明：extern列会把大部分数据存放在over-flow page中。如果更新不涉及extern列，那么新插入的主键记录需要继承（inherit）over-flow page的数据。同样，在purge线程清理已删除的主键记录时，如果其不拥有（owned）over-flow page，则不删除over-flow page。类似的，如果回滚时被删除的记录列继承over-flow page，那么同样不能删除over-flow page。继承和拥有通过extern列上的BTR_EXTERN_OWNER_FLAG和BTR_EXTERN_INHERITED_FLAG进行标记：BTR_EXTERN_OWNER_FLAG为0表示记录拥有该over-flow page，purge线程可以进行删除。回滚时，如果BTR_EXTERN_INHERITED_FLAG为1，则不能删除over-flow page。\n删除记录 #  因为InnoDB支持MVCC，所以对于删除操作，分为2个步骤：\n delete mark purge  其中，步骤#1在用户线程（事务）中完成；步骤#2通过后台的purge线程完成，purge线程检查是否还有其他事务还在使用该记录，如果没有，则将记录彻底删除，记录所占用的空间放回到page的PAGE_FREE链表。\ndelete mark #  其中，根据删除对象分为clustered index和secondary index：\n   clustered index的delete markbtr_cur_del_mark_set_clust_rec secondary index的delete markbtr_cur_del_mark_set_sec_rec     1. 加锁（lock_clust_rec_modify_check_and_lock），可能存在的隐式锁转化为显式锁，加显示锁（LOCK_X | LOCK_REC_NOT_GAP） 2. 生成undo log3. delete mark4. 更新系统列5. 生成redo log 1. 加锁（btr_cur_del_mark_set_sec_rec），加显示锁（LOCK_X | LOCK_REC_NOT_GAP） 2. delete mark3. 生成redo log    从上面可以看出来，两种索引的delete mark区别主要有两点：锁转换和支持数据访问的MVCC。\n产生的redo log如下：\npurge deleted index record #  purge线程对delete mark记录进行gc，具体的策略是：先删除辅助索引，再删除聚簇索引中的deleted index record，调用链如下：\nrow_purge row_purge_record row_purge_del_mark row_purge_remove_sec_if_poss // 删除辅助索引中的deleted index record row_purge_remove_sec_if_poss_leaf // 只删除index record btr_cur_optimistic_delete // 乐观删除 row_purge_remove_sec_if_poss_tree // 需要修改index tree btr_cur_pessimistic_delete // 悲观删除 row_purge_remove_clust_if_poss // 删除聚簇索引中的deleted index record row_purge_remove_clust_if_poss_low BTR_MODIFY_LEAF： btr_cur_optimistic_delete // 乐观删除 BTR_MODIFY_TREE | BTR_LATCH_FOR_DELETE： btr_cur_pessimistic_delete // 悲观删除 从上面的调用我们可以得知：\n如果是叶子节点（BTR_MODIFY_LEAF），则采用乐观删除（btr_cur_optimistic_delete），仅对记录所在的页加x-latch，调用page_cur_delete_rec，并且有如下前提：\n 删除的记录不包含extern属性的列 删除操作不会引起页发生合并操作  否则（BTR_MODIFY_TREE | BTR_LATCH_FOR_DELETE）采用悲观删除（btr_cur_pessimistic_delete），不仅对记录所在的页加x-latch，还对其兄弟节点和index持有x-latch。\n悲观删除具体还需要区分该记录的位置：位于叶子节点还是非叶子节点，删除叶子节点中的记录，不需要更新上层叶子的键值，这样的设计可以减少对于上层节点更新的开销，如下图所示：\n从上面的图中可以看到，删除leaf node上的记录25，不需要更新上层的node pointer，因为这并没有破坏查询得到的结果。\n而删除非叶子节点记录，则需要根据该记录的不同情况进行不同的处理：\n 删除的非叶子节点记录含有REC_INFO_MIN_REC_FLAG标识，并且记录所在page 是B+ tree当前层的第一个page，删除记录，并对下一个记录设置REC_INFO_MIN_REC_FLAG标识 删除的非叶子节点记录含有REC_INFO_MIN_REC_FLAG标识，并且记录所在page不是B+ tree当前层的第一个page，删除记录，并更新上一层的node pointer 直接删除记录  具体示例如下图所示：\n留给读者一个小问题，InnoDB是如何保证B+ tree中叶子节点不会出现latch死锁？  自适应哈希索引（转） #  本节AHI转自阿里内核月报\n自适应哈希索引，简称AHI（Adaptive Hash Index），目的是为了可以使InnoDB在有足够的内存和特定的工作负载下，看起来更像一个内存数据库，并且不会牺牲任何事务特征和稳定性，即对热数据的访问提供加速，也可以称之为一种特定的page table。\n我们从前面可以看到，InnoDB是基于索引组织表来构建数据的，因此查询都是基于B+ tree来进行的，而B+ tree的查询效率和树的高度成正比，tree的高度代表了I/O次数。而如果采用哈希查询，则复杂度为O(1)。InnoDB通过观察B+ tree的搜索模式，在特定条件下使用index key的前缀（前缀长度任意）自动建立哈希索引。\n被称为”自适应“的原因，一是其根据观察搜索模式自动建立索引的机制，二是在访问时首先查询哈希索引，对用户是透明的。从某种意义上来说，AHI可以理解为建立在B+ tree索引上的索引。\n另外，在某些场景下，AHI并不适用，比如高并发的join操作，模糊或者范围查询。在这种场景下，维护AHI的成本比其带来的收益更大，所以需要根据具体的访问方式来决定是否开启AHI。\nAHI和B+ tree索引的区别如下：\n    B+ tree索引 AHI     查询时间复杂度 O(logd(N)) O(1)   是否持久化 是（并通过日志保证完整性） 否（仅在内存中）   索引对象 页 记录    InnoDB在进行B+ tree查询搜索时，为了减少了重新寻路的次数，采用多种策略来提高性能：\n 对于连续记录扫描，InnoDB在满足比较严格的条件时采用row cache的方式连续读取8条记录（并将记录格式转换成Server层的MySQL Format），存储在用户线程私有的row_prebuilt_t::fetch_cache中。这样一次寻路就可以获取多条记录，在server层处理完一条记录后，可以直接从cache中取数据而无需再次寻路，直到cache中数据取完，再进行下一轮。 另一种方式是，当一次进入InnoDB层获得数据后，在返回server层前，当前的btr cursor会暂存到持久游标（row_prebuilt_t::pcur）中，当再次返回InnoDB层捞数据时，如果对应的block没有发生任何修改，则可以继续沿用之前存储的cursor，无需重新定位。 通过观察记录的搜索模式，创建AHI，其后的查询可以直接定位记录。  创建AHI #  AHI就是一个哈希表（btr_search_sys::hash_tables），在InnoDB的buffer pool初始化过程中创建，大小为buffer pool的1/64。\n从这里可以看出：\n AHI占用buffer pool的空间 在MySQL 5.7.8之前，AHI使用btr_search_latch latch来控制并发访问，在高负载的情况下这可能会成为性能的瓶颈点。于是，在MySQL 5.7.8中，将AHI分为多个partition（默认为8），每个partition有自己的latch来控制并发访问（btr_search_latches），来降低latch的争用。 MySQL从5.7.5可以运行时调整buffer pool size，当buffer_pool动态调整的大小超过一定程度时（扩大2倍或者缩小2倍以上），重新分配AHI（btr_search_sys_resize()）  可以通过观察show engine innodb status中的SEMAPHORES使用情况来判断latch的争用情况，比如看到大量线程在btr0sea.cc上争用latch，这种情况下，可能需要考虑关闭AHI\nPercona有一篇文章讲AHI的btr_search_latch和index lock的性能问题：Index lock and adaptive search – next two biggest InnoDB problems，MySQL 5.7通过对AHI拆分为多个partition（commit id: ab17ab91）以及引入更细粒度的索引锁协议（WL#6326 fix index→lock contention）来解决这两个问题\n 函数调用链如下：\ninnobase_init() innobase_start_or_create_for_mysql() buf_pool_init(srv_buf_pool_size, srv_buf_pool_instances) // 2147483648(2G) 8 btr_search_sys_create(buf_pool_get_curr_size() / sizeof(void*) / 64) // 2147221504/8/64=4193792 ib_create((hash_size / btr_ahi_parts, ...) // 4193792/8=524224 hash_create(n); // 524224 -\u0026gt; 553193 数据结构如下：\n维护AHI的查询信息 #  在每个索引对象上（dict_index_t-\u0026gt;search_info），维护了记录的查询信息（btr_search_t），用来启发当前索引对AHI的使用。\n当索引对象添加到数据字典缓存时，创建该索引的查询信息。随后，搜索从B+ tree的root page开始，定位到leaf page时，更新查询信息（btr_search_info_update）。代码如下：\nbtr_cur_search_to_nth_level\nlevel == 0 #ifdef BTR_CUR_ADAPT /* We do a dirty read of btr_search_enabled here. We will properly check btr_search_enabled again in btr_search_build_page_hash_index() before building a page hash index, while holding search latch. */ if (btr_search_enabled \u0026amp;\u0026amp; !index-\u0026gt;disable_ahi) { btr_search_info_update(index, cursor); } #endif 当info-\u0026gt;hash_analysis值超过BTR_SEARCH_HASH_ANALYSIS（17）时，也就是说对该索引寻路到叶子节点17次后，才会去做AHI分析，这是为了避免频繁的索引查询分析产生的过多CPU开销。\nInnoDB通过索引条件构建一个可用于查询的tuple，而AHI需要根据tuple定位到叶子节点上记录的位置，既然AHI是建立在B+树索引上的索引，其的键值就是通过B+树索引的前N列的值计算得来的，所有的查询信息（search info）统计都是为了确定一个合适的\u0026quot;Ｎ\u0026quot; ，这个值是动态的，会跟随应用的负载（查询模式）自适应调整并触发block上的AHI rebuild。\n维护AHI的查询信息（btr_search_info_update_slow）分为三部分，我们依次进行介绍：\n 更新index的查询信息（index search info） 更新block的查询信息（block search info） build AHI  更新索引的查询信息 #  通过之前的介绍我们知道，查询首先将结果保存在btr cursor中，其中，通过二叉查找比较得到的匹配结果放在变量up_match、up_bytes、low_match、low_bytes中。AHI根据这4个变量来更新索引的查询信息（btr_search_info_update_hash），即更新search info的left_side、n_fields、n_bytes。其中left_size表示在相同索引前缀时采用最左/最右记录构建AHI，n_fields表示推荐的AHI列数，n_bytes为索引中的字符串前缀大小。这样，通过left_size+n_fields就可以确定选择哪些列来作为索引前缀构建AHI的哈希记录（fold, rec）。当用户的SQL的索引前缀列的个数大于等于构建AHI时的前缀索引，就可以用上AHI。\n两种情况需要build建议的前缀索引列（set a new recommendation）：\n 当前是第一次为该索引做AHI分析，btr_search_t::n_hash_potential = 0，需要构建建议的前缀索引列 新的记录匹配模式发生了变化(info-\u0026gt;left_side == (info-\u0026gt;n_fields \u0026lt;=cursor-\u0026gt;low_match))，需要重新设置前缀索引列  代码如下：\ncmp = ut_pair_cmp(cursor-\u0026gt;up_match, cursor-\u0026gt;up_bytes, cursor-\u0026gt;low_match, cursor-\u0026gt;low_bytes); if (cmp == 0) { info-\u0026gt;n_hash_potential = 0; /* For extra safety, we set some sensible values here */ info-\u0026gt;n_fields = 1; info-\u0026gt;n_bytes = 0; info-\u0026gt;left_side = TRUE; } else if (cmp \u0026gt; 0) { info-\u0026gt;n_hash_potential = 1; if (cursor-\u0026gt;up_match \u0026gt;= n_unique) { info-\u0026gt;n_fields = n_unique; info-\u0026gt;n_bytes = 0; } else if (cursor-\u0026gt;low_match \u0026lt; cursor-\u0026gt;up_match) { info-\u0026gt;n_fields = cursor-\u0026gt;low_match + 1; info-\u0026gt;n_bytes = 0; } else { info-\u0026gt;n_fields = cursor-\u0026gt;low_match; info-\u0026gt;n_bytes = cursor-\u0026gt;low_bytes + 1; } info-\u0026gt;left_side = TRUE; } else { info-\u0026gt;n_hash_potential = 1; if (cursor-\u0026gt;low_match \u0026gt;= n_unique) { info-\u0026gt;n_fields = n_unique; info-\u0026gt;n_bytes = 0; } else if (cursor-\u0026gt;low_match \u0026gt; cursor-\u0026gt;up_match) { info-\u0026gt;n_fields = cursor-\u0026gt;up_match + 1; info-\u0026gt;n_bytes = 0; } else { info-\u0026gt;n_fields = cursor-\u0026gt;up_match; info-\u0026gt;n_bytes = cursor-\u0026gt;up_bytes + 1; } info-\u0026gt;left_side = FALSE; } 从上述代码可以看到，在low_match和up_match之间，选择小一点match的索引列数的来进行设置，但不超过唯一确定索引记录值的列的个数：\n 当low_match小于up_match时，left_side设置为true，表示相同前缀索引的记录只缓存最左记录 当low_match大于up_match时，left_side设置为false，表示相同前缀索引的记录只缓存最右记录  另外，如果不是第一次进入，有两种情况需要递增n_hash_potential（如果使用AHI构建索引，潜在的可能成功的次数）：\n  本次查询的up_match和当前推荐的前缀索引n_fields都能唯一决定一条索引记录（比如unique index），则根据search_info推荐的前缀索引列构建AHI肯定能命中\n/* Test if the search would have succeeded using the recommended hash prefix */ if (info-\u0026gt;n_fields \u0026gt;= n_unique \u0026amp;\u0026amp; cursor-\u0026gt;up_match \u0026gt;= n_unique) { increment_potential: info-\u0026gt;n_hash_potential++; return; }   本次查询的tuple可以通过建议的前缀索引列构建的AHI定位到\ncmp = ut_pair_cmp(info-\u0026gt;n_fields, info-\u0026gt;n_bytes, cursor-\u0026gt;up_match, cursor-\u0026gt;up_bytes); if (info-\u0026gt;left_side ? cmp \u0026lt;= 0 : cmp \u0026gt; 0) { goto increment_potential; }   从上面我们可以得知，AHI对于同一个索引使用相同的查询模式进行查询的场景，收益最大。\n更新block的查询信息 #  我们更新完index的查询信息后，接着需要更新数据页block的查询信息（btr_search_update_block_hash_info）。\n需要更新的查询信息有：\n btr_search_info::last_hash_succ：是否最近一次通过AHI查找成功 buf_block_t::n_hash_helps：如果使用当前推荐的前缀索引列构建AHI，可能命中的次数 buf_block_t::left_size：同index search info buf_block_t::n_fields ：同index search info buf_block_t::n_fields ：同index search info  处理流程：\n 首先将btr_search_info::last_hash_succ设为false。这意味着每次分析一个新的block，都会导致AHI短暂不可用。 比较index search info和block search info的三元组，计算n_hash_helps 判断是否为整个page构建AHI  这里展开说一下#3，即当满足下面3个条件时，就会为整个page构建AHI：\n block可能命中的次数（n_hash_helps）大于page总记录数的1/16 index（连续）潜在的可能成功的次数（n_hash_potential）大于100 还没有为当前block构造过索引（!block-\u0026gt;index），或者当前block上已经构建了AHI索引并且其可能命中的次数（n_hash_helps）大于page总记录数的2倍，或者当前block上推荐的前缀索引列和当前已构建的AHI索引发生了变化。  build AHI #  如果在上一步中判断可以为当前的page构建AHI（build_index = TRUE），则根据当前推荐的索引前缀build AHI。\nbuild分为3个阶段：\n 检查阶段：检查是否已开启AHI，加btr_search_latch s-latch，如果block上已经构建了AHI（block-\u0026gt;index）但是和当前推荐的不同，则清空block的AHI对应项（btr_search_drop_page_hash_index），释放btr_search_latch s-latch 搜集阶段：根据推荐的索引列计算记录的fold值，将page中对应记录的内存地址放入（fold, rec*）数组中 left_side不同，相同的前缀索引列值不一样，比如page上记录为 (2, 1), (2, 2), (5, 3), (5, 4), (7, 5), (8, 6)，n_fields＝１ 如果left_side = true （最左），则hash存储的记录为(2, 1), (5, 3), (7, 5), (8, 6) 如果left_side = false（最右），则hash存储的记录为(2, 2), (5, 4), (7, 5), (8, 6) 插入阶段：加btr_search_latch x-latch，将搜集到的（fold, rec*）插入到AHI中  index search info #  我们整体看一下index search info：\n/** The search info struct in an index */ struct btr_search_t{ ulint ref_count; // AHI中的block数（即block-\u0026gt;index的个数） buf_block_t* root_guess; /*!\u0026lt; the root page frame when it was last time fetched, or NULL */ ulint withdraw_clock; /*!\u0026lt; the withdraw clock value of the buffer pool when root_guess was stored */ ulint hash_analysis; // 在B+树索引中进入leaf page的次数，超过BTR_SEARCH_HASH_ANALYSIS（17），则更新index search info ibool last_hash_succ; // 是否最近一次通过AHI查找成功（该值不一定准确） ulint n_hash_potential; // 如果使用AHI构建索引，潜在的可能成功的次数 ulint n_fields; // 推荐的AHI列数 ulint n_bytes; // 索引中的字符串前缀大小 ibool left_side; // 在相同索引前缀时采用最左/最右记录构建AHI ulint n_hash_succ; // AHI查询命中的次数 ulint n_hash_fail; // AHI查询miss的次数 ulint n_patt_succ; // 未使用 ulint n_searches; // B+树的查询次数 }; 使用AHI #  在B+ tree搜索中（btr_cur_search_to_nth_level）使用AHI，代码如下：\n#ifndef BTR_CUR_ADAPT  guess = NULL; #else  info = btr_search_get_info(index); if (!buf_pool_is_obsolete(info-\u0026gt;withdraw_clock)) { guess = info-\u0026gt;root_guess; } else { guess = NULL; } #ifdef BTR_CUR_HASH_ADAPT  # ifdef UNIV_SEARCH_PERF_STAT  info-\u0026gt;n_searches++; # endif  /* Use of AHI is disabled for intrinsic table as these tables re-use the index-id and AHI validation is based on index-id. */ if (rw_lock_get_writer(btr_get_search_latch(index)) == RW_LOCK_NOT_LOCKED \u0026amp;\u0026amp; latch_mode \u0026lt;= BTR_MODIFY_LEAF \u0026amp;\u0026amp; info-\u0026gt;last_hash_succ \u0026amp;\u0026amp; !index-\u0026gt;disable_ahi \u0026amp;\u0026amp; !estimate # ifdef PAGE_CUR_LE_OR_EXTENDS  \u0026amp;\u0026amp; mode != PAGE_CUR_LE_OR_EXTENDS # endif /* PAGE_CUR_LE_OR_EXTENDS */ \u0026amp;\u0026amp; !dict_index_is_spatial(index) /* If !has_search_latch, we do a dirty read of btr_search_enabled below, and btr_search_guess_on_hash() will have to check it again. */ \u0026amp;\u0026amp; UNIV_LIKELY(btr_search_enabled) \u0026amp;\u0026amp; !modify_external \u0026amp;\u0026amp; btr_search_guess_on_hash(index, info, tuple, mode, latch_mode, cursor, has_search_latch, mtr)) { btr_cur_n_sea++; DBUG_VOID_RETURN; } # endif /* BTR_CUR_HASH_ADAPT */#endif /* BTR_CUR_ADAPT */我们从中可以看到，需要满足如下条件才能够使用AHI：\n 没有加btr_search_latch写锁。如果加了写锁，可能操作时间比较耗时，走AHI检索记录得不偿失 本次查询或者DML不改变B+树的结构（latch_mode \u0026lt;= BTR_MODIFY_LEAF） 最近一次使用AHI（可能）成功（info→last_hash_succ） 已开启AHI 查询优化阶段的估值操作，例如计算range范围等，典型的堆栈包括：handler::multi_range_read_info_const　–\u0026gt; ha_innobase::records_in_range –\u0026gt; btr_estimate_n_rows_in_range –\u0026gt; btr_cur_search_to_nth_level 不是空间索引 调用者无需分配外部存储页(BTR_MODIFY_EXTERNAL，主要用于辅助写入大的blob数据，参考struct btr_blob_log_check_t)。  当满足上述所有条件时，才根据当前的查询tuple对象计算fold，并查询AHI（btr_search_guess_on_hash）；只有当前检索使用的tuple列的个数大于等于构建AHI的列的个数时，才能够使用AHI索引。\nbtr_search_guess_on_hash\n 首先用户提供的前缀索引查询条件必须大于等于构建AHI时的前缀索引列数，这里存在一种可能性：索引上的search_info的n_fields 和block上构建AHI时的cur_n_fields值已经不相同了，但是我们并不知道本次查询到底落在哪个block上，这里一致以search_info上的n_fields为准来计算fold，去查询AHI 在检索AHI时需要加\u0026amp;btr_search_latch的S锁 如果本次无法命中AHI，就会将btr_search_info::last_hash_succ设置为false，这意味着随后的查询都不会去使用AHI了，只能等待下一路查询信息分析后才可能再次启动（btr_search_failure） 对于从ahi中获得的记录指针，还需要根据当前的查询模式检查是否是正确的记录位置（btr_search_check_guess）  如果本次查询使用了AHI，但查询失败了（cursor-\u0026gt;flag == BTR_CUR_HASH_FAIL），并且当前block构建AHI索引的curr_n_fields等字段和btr_search_info上的相符合，则根据当前cursor定位到的记录插入AHI。参考函数：btr_search_update_hash_ref。\n从上述分析可见，AHI如其名，完全是自适应的，如果检索模式不固定，很容易就出现无法用上AHI或者AHI失效的情况。\nshortcut查询模式 #  在row_search_mvcc函数中，首先会去判断在满足一定条件时，使用shortcut模式，利用AHI索引来进行检索。\n只有满足严苛的条件时（例如需要唯一键查询、使用聚集索引、长度不超过八分之一的page size、隔离级别在RC及RC之上、活跃的Read view等等条件，具体的参阅代码），才能使用shortcut：\n 加btr_search_latch的S锁 通过row_sel_try_search_shortcut_for_mysql检索记录；如果找到满足条件的记录，本次查询可以不释放 btr_search_latch，这意味着InnoDB/server层交互期间可能持有AHI锁，但最多在10000次（BTR_SEA_TIMEOUT）交互后释放AHI latch。一旦发现有别的线程在等待AHI X 锁，也会主动释放其拥有的S锁。  然而， Percona的开发Alexey Kopytov认为这种长时间拥有的btr_search_latch的方式是没有必要的，这种设计方式出现在很久之前加锁、解锁非常昂贵的时代，然而现在的CPU已经很先进了，完全没有必要，在Percona的版本中，一次shortcut的查询操作后都直接释放掉btr_search_latch（参阅bug#1218347）。\nAHI运维和监控项 #  AHI参数 #  InnoDB提供了如下AHI参数\ninnodb_adaptive_hash_index：开启/关闭AHI innodb_adaptive_hash_index_parts：AHI分区数\nAHI监控项 #  打开AHI监控项\nset global innodb_monitor_enable = module_adaptive_hash; 查看AHI的运行状态\n通过show engine innodb status查看AHI各个partition的使用情况\n------------------------------------- INSERT BUFFER AND ADAPTIVE HASH INDEX ------------------------------------- ... Hash table size 553193, node heap has 108 buffer(s) Hash table size 553193, node heap has 122 buffer(s) Hash table size 553193, node heap has 121 buffer(s) Hash table size 553193, node heap has 159 buffer(s) Hash table size 553193, node heap has 154 buffer(s) Hash table size 553193, node heap has 118 buffer(s) Hash table size 553193, node heap has 154 buffer(s) Hash table size 553193, node heap has 89 buffer(s) 0.00 hash searches/s, 0.00 non-hash searches/s 通过INFORMATION_SCHEMA.INNODB_METRICS查看运行时信息\nmysql\u0026gt; select status, name, subsystem,count, max_count, min_count, avg_count, time_enabled, time_disabled from INFORMATION_SCHEMA.INNODB_METRICS where subsystem like '%adaptive_hash%'; +----------+------------------------------------------+---------------------+----------+-----------+-----------+-------------------+---------------------+---------------+ | status | name | subsystem | count | max_count | min_count | avg_count | time_enabled | time_disabled | +----------+------------------------------------------+---------------------+----------+-----------+-----------+-------------------+---------------------+---------------+ | enabled | adaptive_hash_searches | adaptive_hash_index | 8038491 | 8038491 | NULL | 1.22195712800762 | 2019-03-27 11:42:05 | NULL | | enabled | adaptive_hash_searches_btree | adaptive_hash_index | 58550686 | 58550686 | NULL | 8.900479966630051 | 2019-03-27 11:42:05 | NULL | | disabled | adaptive_hash_pages_added | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | | disabled | adaptive_hash_pages_removed | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | | disabled | adaptive_hash_rows_added | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | | disabled | adaptive_hash_rows_removed | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | | disabled | adaptive_hash_rows_deleted_no_hash_entry | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | | disabled | adaptive_hash_rows_updated | adaptive_hash_index | 0 | NULL | NULL | NULL | NULL | NULL | +----------+------------------------------------------+---------------------+----------+-----------+-----------+-------------------+---------------------+---------------+ 8 rows in set (0.00 sec) "},{"id":26,"href":"/docs/MySQL/InnoDB/6_buffer_pool/","title":"buffer pool","section":"Inno Db","content":"基于磁盘的数据库系统通常都是用缓冲池（buffer pool）技术来弥补CPU和磁盘之间的速度鸿沟。\n通过以下机制来达成目标：\nspatial control 空间控制\n where to write pages on disk. the goal is to keep pages that are used together often as physically close together as possible on disk.  temporal control 时间控制\n when to read pages into memory, and when to write them to disk. the goal is minimize the number of stalls from having to read data from disk.  在以下章节中，InnoDB数据结构中的buf_block_t，buf_page_t、frame按照Jim Gray的命名进行转换，称为block（以下称为PCB，包含meta+data）、meta和frame（data）。\n概述 #  buffer pool #  InnoDB是基于磁盘存储的存储引擎，磁盘中的记录以页为单位进行管理。\n简单来说，buffer pool就是存储引擎自管理的一块内存区域，缓存频繁访问的数据，从而减少读取写入磁盘对于数据库的影响。\n这里给读者留一个问题，为什么不依赖操作系统或者文件系统提供的page cache机制，或者MMAP来进行缓存？  当数据库读取页时，首先分配一个PCB，然后通过该PCB将页\u0026quot;FIX\u0026quot;住，然后发起磁盘IO从磁盘将实际的页读入buffer pool，将frame指向加载的页数据，访问完数据后再”UNFIX“该PCB。下次再读取该页时，先从buffer pool中查找，否则再走上面的流程从磁盘读取。\n当数据库修改页时，首先通过上面的机制保证页在buffer pool中后，进行\u0026quot;FIX“，修改页内的数据后，”UNFIX“，此时前台的数据修改完成。后台再以一定的频率（至少在checkpoint时）同步/异步刷回磁盘（原地写/追加写）。\n从这里可以看出，读取和写入都为了最大化的利用缓存，对于读取，尽量延长数据保持在内存中的时间，而对于写入，则delay写回磁盘的时机。\n对于InnoDB来说，buffer pool中缓存的页有6种类型：索引页、数据页、undo页、change buffer、AHI、lock info，而不仅仅是缓存数据页和索引页。如下图所示：\n其中的data page、index page、change buffer page都是需要异步持久化到外存的，undo page分为两种情况：已提交的事务的undo page已经在事务提交时保证其持久化，未提交的事务的undo page可能刷入磁盘也可能没有。\n而AHI和lock info是不持久化的，生命周期仅限于运行时。锁也是在事务的生命周期里产生的，在crash recovery时会根据事务信息重建锁状态，所以锁不需要持久化，另外，每个锁信息通常是在trx_lock_t.lock_heap中分配。而AHI是根据访问的数据行动态建立的hashtable，无需保留。\nPCB、meta和frame #  前面讲过，buffer pool的资源单位为页。为了管理页，页在缓冲池中按照PCB为单元进行管理，其中包括meta和data（frame）。\nPCB和其中的meta、frame（数据页）之间的关系如下图所示：\n从上面的内存布局可以看出，PCB中的第一个字段（必须）是meta，以保证page table指向的buf_block_t和buf_page_t两种类型的指针可以相互转换，block→frame指向真正存放数据的数据页。\nmeta中的元信息有：page size、state、oldest_modification、newest_modification、access_time\u0026hellip;，对于压缩页，解压后的实际数据指针也是用frame指向。\n这里给读者留一个问题：frame和PCB的生命周期是一致的吗？  如果对表进行了压缩，则对应的数据页称为压缩页，如果需要从压缩页中读取数据，则压缩页需要先解压，形成解压页，解压页为16KB。压缩页的大小是在建表的时候指定，目前支持16K，8K，4K，2K，1K。即使压缩页大小设为16K，在blob/varchar/text的类型中也有一定好处。假设指定的压缩页大小为4K，如果有个数据页无法被压缩到4K以下，则需要做B-tree分裂操作，这是一个比较耗时的操作。正常情况下，Buffer Pool中会把压缩和解压页都缓存起来，当Free List不够时，按照系统当前的实际负载来决定淘汰策略。如果系统瓶颈在IO上，则只驱逐解压页，压缩页依然在Buffer Pool中，否则解压页和压缩页都被驱逐。  page table #  buffer pool通过page table（也称为page hashtable，其Key是space_id, page_no，value是PCB地址）可以快速找到已经被读入内存的数据页，而不用线性遍历LRU List去查找。这里的page table不是AHI，AHI是为了减少B+树的扫描，而page hash是为了避免扫描链表（LRU List）。\nAHI在B+ tree一章详细介绍。\nbuffer chunk #  从MySQL 5.7开始，支持动态调整buffer pool的大小（resizing），由此引入了buffer chunk的概念，可以简单的将buffer chunk理解为一块花布，里面是page里的数据网格，每次调整buffer pool按照花布为粒度进行批量处理（即支持遍历访问）。\nchunk size的大小可以通过innodb_buffer_pool_chunk_size设定（默认128M），buffer chunk的数量=缓冲池instance大小/chunk size，同样，为了避免性能问题，buffer chunk不应该超过1000。\n一个buffer chunk的结构如下：\nstruct buf_chunk_t{ ulint size; 一共有多少页（PCB[]+frame[]） unsigned char* mem; 指向frame[]首地址 ut_new_pfx_t mem_pfx; 指向allocator的地址（包括cookie），用于deallocator buf_block_t* blocks; PCB[] }; buffer chunk的初始化只在InooDB启动或者resizing时进行（buf_chunk_init），向memory allocator申请chunk size大小的内存（mmap），并按照页的粒度串到free链表上，即把buffer pool打好格子，格子内划分为一组buf_block_t。通过遍历一个chunk可以访问大多数的数据页（比如查找压缩页 buf_chunk_contains_zip），有两种状态的数据页除外：没有被解压的压缩页（BUF_BLOCK_ZIP_PAGE）以及被修改过且解压页已经被驱逐的压缩页（BUF_BLOCK_ZIP_DIRTY）。\n用mmap分配的内存都是虚存，在top命令中占用VIRT这一列，而不是RES这一列，只有相应的内存被真正使用到了，才会被统计到RES中，提高内存使用率。这样是为什么常常看到MySQL一启动就被分配了很多的VIRT，而RES却是慢慢涨上来的原因。\nos_mem_alloc_large()\n​\tmmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | OS_MAP_ANON, -1, 0)\n NUMA的内存分配策略（mbind）也在这里控制。  页链表 #  buffer pool中的内存空间以页的倍数来申请，即以页为单位进行管理。\n在InnoDB中，页通常通过free、LRU和flush这三个逻辑链表组织并管理起来。free链表用于保存未使用的页，当free链表中的页用尽后，则根据LRU算法淘汰已经使用的页，最频繁使用的页在LRU链表的头部，最少使用的页在LRU链表的尾部。\n在InnoDB的LRU链表设计中（LRU-K），通过加入midpoint位点划分出老区（历史队列）和新区（缓冲区）。新读取到的页，虽然是最近访问的页，并不直接放到LRU链表的头部，而是放到midpoint的（默认3/8）位置，这种算法在InnoDB中称为midpoint insertion strategy。这样做而不采用朴素的LRU算法的原因，是因为某些低频但是数据量大的SQL操作或者预读可能会将缓冲池中频繁使用的页刷出（sequential flooding），从而影响缓冲池的效率。\n此外，有些页不属于数据页或索引页，从free链表申请，但使用完毕后，并不放入LRU链表，比如AHI，lock info。\n例如在下面的例子中，buffer pool - free - LRU = 65528 - 43916 - 21603 = 9，还缺少9个页，这些页其实分配给了AHI：\n---------------------- BUFFER POOL AND MEMORY ---------------------- Total large memory allocated 1126694912 Dictionary memory allocated 941200 Buffer pool size 65528 Free buffers 43916 Database pages 21603 Old database pages 7810 Modified db pages 0 Pending reads 0 Pending writes: LRU 0, flush list 0, single page 0 Pages made young 0, not young 0 0.00 youngs/s, 0.00 non-youngs/s Pages read 202, created 21401, written 14543 0.00 reads/s, 0.00 creates/s, 0.00 writes/s No buffer pool page gets since the last printout Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s LRU len: 21603, unzip_LRU len: 0 I/O sum[0]:cur[0], unzip sum[0]:cur[0] 这里显示AHI占用了9个页\n------------------------------------- INSERT BUFFER AND ADAPTIVE HASH INDEX ------------------------------------- Ibuf: size 1, free list len 0, seg size 2, 0 merges merged operations: insert 0, delete mark 0, delete 0 discarded operations: insert 0, delete mark 0, delete 0 Hash table size 276671, used cells 121, node heap has 1 buffer(s) Hash table size 276671, used cells 585, node heap has 2 buffer(s) Hash table size 276671, used cells 21, node heap has 1 buffer(s) Hash table size 276671, used cells 5, node heap has 1 buffer(s) Hash table size 276671, used cells 13, node heap has 1 buffer(s) Hash table size 276671, used cells 10, node heap has 1 buffer(s) Hash table size 276671, used cells 13, node heap has 1 buffer(s) Hash table size 276671, used cells 15, node heap has 1 buffer(s) 0.00 hash searches/s, 0.00 non-hash searches/s buffer pool中的页不仅用于读取，也可能被修改，被修改的页称为脏页（dirty page），脏页肯定在LRU链表中，同时也在flush链表中。LRU链表用于管理缓冲池中页的可用性，flush链表用于管理页的刷回（LRU也会触发刷回），两者互不影响。\n下图是free链表，LRU链表，flush链表之间的关系：\n![InnoDB_buffer_pool free_LRU_flush](/InnoDB_buffer_pool free_LRU_flush.png)\nLRU链表中还包含没有被解压的压缩页，这些压缩页刚从磁盘读取出来，还没来的及被解压。\n关于LRU算法的详情下面讨论。\n另外，如果启用页压缩，则还使用了以下的页链表：\n Unzip LRU List：这个链表中存储的数据页都是解压页，也就是说，这个数据页是从一个压缩页通过解压而来的。 Zip Free：压缩页有不同的大小，比如8K，4K，InnoDB使用了类似内存管理的伙伴系统来管理压缩页（binary buddy allocator算法）。Zip Free可以理解为由5个链表构成的一个二维数组，每个链表分别存储了对应大小的内存碎片，例如8K的链表里存储的都是8K的碎片，如果新读入一个8K的页面，首先从这个链表中查找，如果有则直接返回，如果没有则从16K的链表中分裂出两个8K的块，一个被使用，另外一个放入8K链表中。 Zip Clean List：这个链表只在Debug模式下有，主要是存储没有被解压的压缩页。这些压缩页刚刚从磁盘读取出来，还没来的及被解压，一旦被解压后，就从此链表中删除，然后加入到Unzip LRU List中。  阿里云RDS MySQL 5.6增加了Quick List：使用带Hint的SQL查询语句，可以把所有这个查询的用到的数据页加入到Quick List中，一旦这个语句结束，就把这个数据页淘汰，主要作用是避免LRU List被全表扫描污染。\nbuffer pool instance #  在MySQL 5.5中，为了提高buffer pool的并发性能，将pool划分为多个instance，一页只会存在于一个instance中（通过page_id（space_id+page_no）hash后按instance_num取模），这样，buf_pool_t表示了一个buffer pool instance。各个instance之间没有竞争关系，之前介绍的buffer pool中的各个逻辑链表、buffer chunks，以及mutex都在每个instance中。\n特别的，instace不能过多也不能过小，否则会有性能问题：当buffer pool小于1GB时，只会有1个instance，而最多有64个（使用6 bit表示instance no）。\nbuffer pool的管理 #  buffer pool的组织 #  从上面的介绍可以看出，buffer pool把页作为基本资源单位，并通过一系列PCB+frame作为chunk，多个chunk组合成一个instance，形成一个完整的buffer pool。buffer pool的层次结构如下图所示：\n其中，buffer pool中分配的内存地址需要16KB对齐，这是基于以下两方面的考虑：\n 性能：内存地址对齐可以减少内存的传输次数 支持DIRECT_IO：数据页的DIRECT_IO要求内存地址4KB对齐  页的状态 #  页的状态一共有8种。理解这8种状态和其相互转换的关系，对buffer pool的页管理机制会有更清晰直观的理解。\n页的状态流转图如下：\n下面依次解释一下这8种状态：\n  BUF_BLOCK_NOT_USED：处于free链表中的数据页，都是此状态。此状态为长态。\n  BUF_BLOCK_READY_FOR_USE：当从free链表中获取一个空闲的数据时，状态会从BUF_BLOCK_NOT_USED变为BUF_BLOCK_READY_FOR_USE（buf_LRU_get_free_block)。这个状态为暂态，处于这个状态的数据页不处于任何逻辑链表中。\n  BUF_BLOCK_FILE_PAGE：也称为buffered file page，正常被使用的数据页都是这种状态。LRU链表中的大部分数据页都是这种状态。此状态为长态。\n  BUF_BLOCK_MEMORY：也称为memory object，缓冲池中的非用户数据页（存储系统信息，例如InnoDB行锁，自适应哈希索引以及压缩页的数据等）被标记为BUF_BLOCK_MEMORY。处于这个状态的数据页不处于任何逻辑链表中。此状态为长态。\n  BUF_BLOCK_REMOVE_HASH：当加入free链表之前，需要先从page hash移除。这种状态就表示此页面已经从page hash移除，但是还没被加入到free链表时的中间状态。\n  BUF_BLOCK_POOL_WATCH：被哨兵看管的数据页。这种类型的page是提供给purge线程用的。InnoDB为了实现MVCC，需要把之前的数据记录在undo log中，如果没有读请求再需要它，就可以通过purge线程删除。换句话说，purge线程需要知道某些数据页是否被读取，实现的方式（buf_pool_watch_set）是首先查看page hash，看看这个数据页是否已经被读入，如果没有读入，则获取一个BUF_BLOCK_POOL_WATCH类型的哨兵数据页控制体，同时加入page_hash中（但其中并没有真正的数据：buf_block_t::frame为空），并把其类型置为BUF_BLOCK_ZIP_PAGE（表示已经被使用了，其他purge线程就不会用到这个控制体了）。如果查看page hash后发现有这个数据页，只需要判断控制体在内存中的地址是否属于Buffer Chunks即表示对应数据页已经被其他线程读入（buf_pool_watch_occurred）。另一方面，如果用户线程需要这个数据页，先查看page hash是否为BUF_BLOCK_POOL_WATCH类型的数据页，如果是则回收，从free链表（即在buffer chunks中）分配一个空闲的控制体，填入数据。这里的核心思想就是通过控制体在内存中的地址来确定数据页是否还在被使用。此状态为暂态。\n每个缓冲池instance配置配置purge thread（默认：4）个哨兵数据页控制体，通过buffer_pool-\u0026gt;watch数组管理。\n也意味着即使所有purge线程同时访问同一个buffer pool instance，也至少会有一个空闲的watch[n]\n   BUF_BLOCK_ZIP_PAGE：也称为clean compressed page（不全面，未包含purge语音）。当从磁盘读取压缩页时，先通过malloc分配一个临时的buf_page_t，然后从伙伴系统中分配出压缩页存储的空间，把磁盘中读取的压缩数据存入其中，然后把这个临时的buf_page_t标记为BUF_BLOCK_ZIP_PAGE状态（buf_page_init_for_read），当这个压缩页被解压时，将页状态修改为BUF_BLOCK_FILE_PAGE，并加入LRU List和Unzip LRU List（buf_page_get_gen）。如果一个压缩页对应的解压页被evict，但是需要保留这个压缩页并且该压缩页不是脏页，则这个压缩页被标记为BUF_BLOCK_ZIP_PAGE（buf_LRU_free_page）。所以正常情况下，处于BUF_BLOCK_ZIP_PAGE状态的不会很多。前面两种被标记为BUF_BLOCK_ZIP_PAGE的压缩页都在LRU list中。另外，上面提到的WATCH状态可以得知，如果被某个purge线程使用了，也会被标记为BUF_BLOCK_ZIP_PAGE。\n  BUF_BLOCK_ZIP_DIRTY：即在flush链表中的compressed page。如果一个压缩页对应的解压页被evict，但是需要保留这个压缩页并且该压缩页是脏页，则被标记为BUF_BLOCK_ZIP_DIRTY（buf_LRU_free_page）。如果该压缩页随后又被解压，则状态会变为BUF_BLOCK_FILE_PAGE。因此BUF_BLOCK_ZIP_DIRTY也是暂态。这种类型的数据页都在flush list中。\n  整体来说，大部分的数据页都处于BUF_BLOCK_NOT_USED状态（在free链表中）和BUF_BLOCK_FILE_PAGE状态（大部分处于LRU链表中，LRU链表中还包含除被purge线程标记的BUF_BLOCK_ZIP_PAGE状态的数据页），少部分处于BUF_BLOCK_MEMORY状态，极少数处于其他状态。前三种状态的数据页都不在buffer chunks上，对应的控制体都是临时分配的，这三种状态也称为invalid state（buf_block_state_valid）。\nbuffer pool concurrency control #  buffer pool中并发访问通过mutex来进行保护，包括：\n PCB 各种页链表的管理 buffer pool  buffer pool的并发控制机制和性能密切相关。最开始，InnoDB通过buffer_pool→mutex来控制并发，即buffer pool中的所有读取、查询、刷新、状态修改等操作都需要持有该mutex，极易形成hotspot。随后，InnoDB将其拆分为多个并发访问的保护对象：缓冲池mutex（buf_pool_t-\u0026gt;mutex）、压缩页（buf_pool_t-\u0026gt;zip_mutex）、flush链表mutex（buf_pool_t→flush_list_mutex）、PCB.mutex。\n内存的并发控制要点：\n 多个对象之间的封锁传递性 加锁从大到小crabbing 可以采用引用计数减少竞争 通过相容性提供更高并发 前后台路径的正交影响  free/LRU/flush list mutex #  所有的数据页都在free/LRU/flush list上，所以如果操作这些list，首先需要获得list mutex，然后在进行IO的操作前，把list mutex释放。\nMySQL 5.7提供了flush_list_mutex\nMySQL 8.0新增了free_list_mutex、LRU_list_mutex\npage table rwlock #  page table提供了内存页的快速查找，提供的是page table slot的rw lock（hash_table_t→sync_obj.rw_locks），5.6之前是整个page table.mutex。\n加锁路径为：page table slot lock→ block/frame lock\nblock mutex #  block mutex保护的是PCB中meta的buf_fix_count、io_fix、state等等元信息，其目的是减少buffer pool mutex、frame rwlock的竞争。\n在代码里的说明：\nbuf_block_t { ... BPageLock\tlock;\t/* read-write lock of the buffer frame */ BPageMutex\tmutex;\t/* mutex protecting this block: state (also protected by the buffer pool mutex), io_fix, buf_fix_count, and accessed; we introduce this new mutex in InnoDB-5.1 to relieve contention on the buffer pool mutex */ } 加锁流程如下：\n FIX page （S/X） buf_page_t.buf_fix_count++ / buf_page_t.io_fix++（block mutex只在操作++\u0026ndash;时使用） UNFIX page （\u0026ndash;）  这里看一下buf_fix_count和io_fix。\nio_fix 表示当前的page frame正在进行读写IO操作：\n BUF_IO_READ：为读取从磁盘加载中 BUF_IO_WRITE：为写入从磁盘加载中  这里不同的fix代表：读写时buffer pool中没有该页，需要从磁盘加载上来（BUF_IO_READ/BUF_IO_WRITE）\nPIN代表已在buffer pool中，正在被访问，相当于引用计数（使用buf_fix_count），不能从buffer pool中刷掉。每次访问都会++，在最后mtr.commit时–。判断该page是否可以访问不用通过frame rwlock，而是通过buf_fix_count\u0026gt;0，这代表已被FIX，这可以减少frame rwlock的竞争。\n这里有两方面的优化目的：\n 在用户的正向路径上减少frame rwlock的调用 在后台路径上减少frame rwlock的调用：刷脏，replace  比如在flush page时要检测其是否可以被flush，直接判断io_fix：\nbuf_flush_ready_for_flush if (bpage-\u0026gt;oldest_modification == 0 || buf_page_get_io_fix(bpage) != BUF_IO_NONE) { return(false); } 检查page是否可以被replace，判断io_fix，并且确保当前没有其他线程在引用，也要保证没有修改过：\nbuf_flush_ready_for_replace if (buf_page_in_file(bpage)) { return(bpage-\u0026gt;oldest_modification == 0 \u0026amp;\u0026amp; bpage-\u0026gt;buf_fix_count == 0 \u0026amp;\u0026amp; buf_page_get_io_fix(bpage) == BUF_IO_NONE); } frame rwlock #  frame通过PCB.lock（buf_block_t.lock）保护\n在实际获取一个page时（buf_page_get_gen），流程如下：\n FIX block block→buf_fix_count++ / block→io_fix++ 同步读取，block-\u0026gt;io_fix\u0026ndash; lock page frame rwlock UNFIX block（block-\u0026gt;buf_fix_count–，unlock page frame rwlock）、后台异步读取后（buf_page_io_complete）（block-\u0026gt;buf_fix_count–，unlock page frame rwlock）  从这里也可以看出，如果io_fix设置了，则用户线程需要等待IO完成，不会获取page frame rwlock，这样做尽可能的减少持有page frame rwlock的机会。因为page frame rwlock的调用在B+树的核心路径上。\n那我们再接着讨论rwlock的加锁类型。InnoDB在访问B+树的时候，会采用乐观访问的方式，先对page frame加S lock，如果可能修改，再加SX lock，确认要修改的时候加X lock。\n另外，只有叶子节点才使用page frame rwlock来保护page frame的一致性，对于非叶子节点通过索引（dict_index_t→lock）来进行保护。\nInnoDB还做了很多优化，比如之前MySQL 5.6会拿着整颗B+树的index lock，而是尽可能的只拿着会引起B+树结构变化的子树，比如引入SX lock，在真正要修改的时候才会获得X lock。\n引入SX lock是对读取的优化, 对写入并没有优化. 因为持有SX lock 的时候, S lock 操作是可以进行的, 但是X lock 操作不可以。即允许更多的读取，X lock的并发度和之前一样。  从上面我们可以看出，这些优化只是对用户的前台访问路径有效果（减少了page frame rwlock的获取），而在后台路径上并没有过多的优化。\n比如上面提到的，在后台路径的异步IO场景下，page frame rw_lock是在buf_page_io_complete 之后才会放开的。因此page frame rw_lock 的持有周期是整个异步IO的周期，直到IO操作完成。而page frame rw_lock又是用户访问路径（B+树）的核心路径（btr_cur_search_to_nth_level），所以可能会因为后台异步IO而对用户的访问造成大量堵塞，如果是非叶子节点，影响范围更大，更明显。而异步IO卡顿的出现可能是因为InnoDB simulated AIO 的队列长度是（io_read_threads+io_write_threads）* 256，可能会出现大量的page在IO等待队列中，page 因为在IO 等待队列中等待，如果存储设备的IO latency恶化，问题会更加明显。\n当然我们也通过simulated AIO 优化, copy page等等减少持有page frame 的时长.\nbuf_page_io_complete主要做两个工作：\n page io_fix设置成NONE，表示这个page的io操作已经完成 将page frame rw_lock释放（如果是读，释放X lock，如果是写，释放SX lock）  这里读操作要拿X lock主要是为了避免多个线程同时去读这个page，如果同时有其他线程要访问该page（在1，2之间），会尝试加S lock（buf_wait_for_read），如果S lock加成功，则说明该page已IO读取完成（2步完成）。\n最后总结一下buffer pool的并发控制：\n free/LRU/flush list的操作通过相关的list mutex保护 后面4个mutex（引入block mutex、buf_fix_count/io_fix避免不必要的获得page frame rw_lock的开销）  先加page hash slot rwlock 获得block mutex 释放page hash slot rwlock 修改buf_fix_count/io_fix 释放block mutex 持有page frame rw_lock 操作完成后修改buf_fix_count，释放page frame rw_lock    buffer pool warmup #  MySQL 5.6引入了buffer pool warmup功能，解决以下问题：\n MySQL冷启动以后数据的预热问题 运行时的OLAP污染buffer pool  为此，提供dump+load机制进行warmup。\n功能实现上也简单，通过buf_dump_thread后台线程，如果收到srv_buf_dump_event事件通知，则遍历缓冲池instance，将缓冲池中的页按照\u0026lt;space_id, page_no\u0026gt;的方式dump到文件中。在遍历过程中，需要对buffer_pool→mutex加锁，所以会引起性能抖动，需要注意。同时，在load时，因为会有大量的IO读发生，所以最好warmup后再提供服务。\n在buf_load实现中，先把外储的文件读入内存，然后使用归并排序对数据排序（因为page no代表了物理位置），以64个数据页为单位进行IO合并，然后发起一次真正的读取操作。排序的作用就是便于IO合并。\nwarmup功能的相关参数如下：\ninnodb_buffer_pool_dump_at_shutdown\ninnodb_buffer_pool_load_at_startup\ninnodb_buffer_pool_dump_now\ninnodb_buffer_pool_load_abort\ninnodb_buffer_pool_dump_pct\ninnodb_buffer_pool_filename\n buffer pool resizing #  为了支持计算资源（内存）的弹性控制，MySQL 5.7提供了在运行时动态调整缓冲池大小（resizing）的功能。\n我们从之前的介绍可以得知，每个缓冲池instance都是由相同数量的buffer chunks组成的，每个buffer chunk的大小通过innodb_buffer_pool_chunk_size指定（实际上会稍大5%，因为需要存放PCB）。在resizing的时候以chunk size为单位进行动态增大/缩小，调整完毕后缓冲池的大小=innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数。\nresizing通过buf_resize_thread完成，在resizing的过程中可以通过Innodb_buffer_pool_resize_status查看状态。\n下面来看一下resizing的处理过程：\n 根据新的buffer pool size计算每个缓冲池instance新的chunks数量 禁用AHI（并清空AHI） 收缩：  锁buffer_poo→mutex 从free链表中把待删除chunk上的page放入待删除链表（buffer_poo→withdraw） 如果上述page为脏页，则刷脏 解锁 重复a~d直至收缩收集完成   resizing  锁buffer_pool-\u0026gt;mutex、page hash 从待删除链表中以chunk为单位收缩buffer pool 清空待删除链表 扩大buffer pool：分配新的chunks buffer pool重新分配所有的chunks 如果扩大/收缩超过2倍，重置page hash，改变hash桶大小 解锁 如果扩大/收缩超过2倍，重启和buffer pool大小相关的内存结构，如锁系统（lock_sys_resize），AHI（btr_search_sys_resize），数据字典（dict_resize）   开启AHI  从上面可以看到，扩容比缩容相对容易。在收缩时，如果有事务一致未提交，并且占用了待收缩的页，导致收缩一直重试，则会通过日志输出。同时，为了避免频繁重试，重试的时长处采用指数形式增长。\n另外，resizing后，serach过程中的btr cursor保存的page可能重新进行了加载，AHI也都进行了清空，都需要重新读取。\nLRU #  LRU算法 #  InnoDB有两种LRU算法：\n 朴素的LRU算法：当LRU链表的长度小于BUF_LRU_OLD_MIN_LEN（512）时启用，读到的buf_page放入LRU链表的头部，不区分冷区和热区。（两方面的考虑：空间腾挪机会小，less is more） LRU-K（midpoint insertion strategy）：当链表长度大于BUF_LRU_OLD_MIN_LEN时启用，区分冷热区。  LRU链表的布局如下图所示：\n这张图来自官方，逻辑上说明了LRU的布局。\n在LRU中，保证使用最多最频繁使用的数据页尽可能keep在内存中，同时还要考虑dirty page evict的反压，这时LRU算法的核心。\n在InnoDB中，通过全局时钟策略（clock）来作为算法的核心，有两个时钟的概念：\n access clock：用于数据访问的标识 evict clock：用于数据evict的标识  access clock #  全局access clock时钟：每次往LRU热区中add，则++，即buffer_pool-\u0026gt;ulint_clock\n每个PCB记录全局时钟中其自己的位置，即PCB.LRU_position\n这样，数据访问的热点就可以评估出来：\n 全局access clock代表全局热度 每个页可以通过LRU_position-ulint_clock得知其在LRU中的位置，从而得知其是否被频繁访问，同时意味着evict的概率大小（也要考虑dirty page flush）  evict clock #  全局evict clock：每次LRU evict++，即buf_pool-\u0026gt;freed_page_clock\n每个PCB记录上一次移到LRU头的global evict clock，记入block-\u0026gt;freed_page_clock\n这样，evict的信息就可以展现出来：\n 全局evict clock代表evict的频率，即置换出去的页的频繁程度 PCB和evict clock的差值：频繁（access_clock）而置换概率低的（evict clock），越应该在LRU的上部  整体上如图所示：\n这样的设计有个问题，维护clock的开销和访问成正比，这会是一个hotspot。\n维护策略 #  OLD的维护是通过ratio/次数（BUF_LRU_OLD_TOLERANCE）的barrier来保证链表的调整不过于频繁，这个barrier通过ratio或者次数。\n新读取进来的页面默认被放在OLD（LRU_old位置）。\n再次访问时：\n OLD经过一段时间窗口（innodb_old_blocks_time 1s），才被移到NEW；否则还待在OLD 位于NEW区中前1/4移动到头部 位于NEW区中3/4不移动  flush后放入尾部，表示可以evict\n尾部evict\n如图所示：\nLRU相关的函数调用链如下：\n留给读者2个小问题：\n 官方图中的OLD区指针（LRU_old）是静态的吗？什么时候会进行调整？ 如何计算评估evict和keep in memory的策略？   页的读取 #  数据页的读取分为2种：\n 逻辑读：从buffer pool中读取数据页 物理读：从磁盘中读取数据页，然后再在内存中修改数据  从中可以看出，物理读在逻辑读的过程之中，所以我们从逻辑读开始。\n逻辑读取 #  逻辑读取是指从buffer pool中访问指定的页(space, offset)。首先在page table中查找（buf_page_hash_get），如果没有，意味逻辑读miss，则走物理读从磁盘上将页加载到buffer pool中，然后再访问。\n读取页面 #  页面的读取逻辑由buf_page_get_gen实现，我们在深入内部逻辑之前，先看一下buf_page_get_gen的2个参数：读取模式（mode）和page-latch类型（rw_latch），以便我们有一个全局的视角作为参考。\n读取模式参数mode（其中前三种使用较多）：\n BUF_GET：默认获取数据页的方式，如果数据页不在buffer pool，则从磁盘读取；如果在，则判断是否要把其加入到LRU list中，以及判断是否需要进行线性预读。读取加s-latch，修改加x-latch。 BUF_GET_IF_IN_POOL：只在buffer pool中查找，不在直接返回null，不走磁盘，后续内存操作同上 BUF_PEEK_IF_IN_POOL：只在buffer pool中查找，不走磁盘，不进行后续内存操作 BUF_GET_NO_LATCH：不加任何latch，其他同#1，B+树的非叶子节点页的读取走这种模式。 BUF_GET_IF_IN_POOL_OR_WATCH：purge thread使用，只在buffer pool中查找（在：加LRU list+线性预读，不在：设置watch） BUF_GET_POSSIBLY_FREED：与BUF_GET类似，只是允许相应的数据页在函数执行过程中被释放，主要用在估算B+tree两个slot之间的数据行数。  参数rw_latch为读到数据页后所持有的latch，有以下几种：\n RW_S_LATCH RW_SX_LATCH RW_X_LATCH RW_NO_LATCH  根据FIX Rules，每个页在放入buffer pool后都需要加上latch来保护（buf_block_t-\u0026gt;lock），因此需要rw_latch：RW_S_LATCH/RW_SX_LATCH/RW_X_LATCH，另外还有RW_NO_LATCH，这是因为在InnoDB中，对于B+树索引的所有非叶子节点的页访问都是通过索引本身的latch（dict_index_t→lock）来保护的，因此对于非叶子节点（页）的访问，页可以不需要持有任何latch，从而节省CPU的资源以提高性能。\n然而，不管访问模式如何，根据FIX Rules规则，都需要FIX，在InnoDB中不是根据是否持有该页的latch来进行判断的，而是根据buf_page_t-\u0026gt;buf_fix_count（引用计数）来决定的。因此，只要读取了页，就需要更新buf_fix_count（buf_block_buf_fix_func）。这也就意味着，如果页被FIX了，那么其对应的buf_fix_count一定是\u0026gt;0的。\n留给读者1个小问题：page-latch何时释放  另外，再次重申一下PCB.meta中的buf_fix_count和io_fix两个变量，这两个变量主要用来做并发控制，减少mutex锁定的范围。当从buffer pool读取数据页时，会其加读锁，然后递增buf_page_t::buf_fix_count，同时设置buf_page_t::io_fix为BUF_IO_READ，然后即可以释放读锁。后续如果其他线程在驱逐数据页(或者刷脏)的时候，需要先检查一下这两个变量，如果buf_page_t::buf_fix_count不为零且buf_page_t::io_fix不为BUF_IO_NONE，则不允许驱逐(buf_page_can_relocate)。这里的技巧主要是为了减少PCB上mutex的争抢，而对数据页的内容（frame），读取的时候依然要加读锁，修改时加写锁。\n读取页面的函数调用链如下：\nbuf_page_get_gen buf_page_hash_get_low // 从page hash中查找（缓冲池） miss: block= null buf_read_page // 物理读取 buf_read_page_low\tbuf_page_init_for_read // 从缓冲池中通过LRU算法分配对象buf_block_t，加block-\u0026gt;mutex，并加x-latch（block-\u0026gt;lock）,用以保护其指向的内存（block-\u0026gt;frame，即页的实际存储的内容） fil_io // 读取磁盘上的数据页 buf_page_io_complete // 同步/异步IO完成，释放x-latch buf_page_set_io_fix // 释放io_fix buf_read_ahead_random // 随机预读 failed retry 100... // 物理读取失败则重试100次 hit: fix_block= block buf_block_fix // FIX page update buf_page-\u0026gt;access_time // 更新page ts buf_read_ahead_linear // 线性读取 buf_page_make_young_if_needed// 更新LRU 以下是buf_page_get_gen的详细流程：\n  通过page_id（space_id+page_no）查找数据页位于哪个buffer pool instance中，instance_no = (space_id \u0026laquo; 20 + space_id + page_no \u0026raquo; 6) % instance_number，即通过space_id+page_no算出一个fold值然后按照instance取余。这里有个小细节，page_no的第六位被砍掉，这是为了保证一个extent的数据（2的6次方=64）能被缓存到同一个buffer pool instance中，便于后面的预读操作。\n  在page table中查找page，返回PCB（buf_page_hash_get_low）\n  如果没有在buffer pool中，并且mode为BUF_GET_IF_IN_POOL_OR_WATCH则设置watch数据页（buf_pool_watch_set）\n  如果没有在buffer pool中，并且mode为BUF_GET_IF_IN_POOL | BUF_PEEK_IF_IN_POOL | BUF_GET_IF_IN_POOL_OR_WATCH，则返回null，表示没有找到数据页。\n  如果没有在buffer pool中，并且mode为其他，发起磁盘同步读取（buf_read_page）(buf_read_page)。 在读取磁盘数据之前，如果是非压缩页，则先从free链表中获取空闲的数据页，如果没有，则需要通过同步刷脏来释放数据页，最后将获取到的空闲数据页加到LRU链表上（buf_page_init_for_read） 如果是压缩页，则临时分配一个buf_page_t用来做控制体，通过伙伴系统（buf_buddy_alloc）分配到压缩页存数据的空间，最后同样加入到LRU List中。\n  调用IO子系统的接口同步读取页面数据（fil_io + IORequest::READ）。 如果读取失败，重试100次（BUF_PAGE_READ_MAX_RETRIES，每次重新从#1开始），然后触发断言中止mysqld（Unable to read page + abort） 如果成功，则判断是否要进行随机预读（buf_read_ahead_random）\n  判断读取的数据页是不是压缩页，如果是的话，因为从磁盘中读取的压缩页的控制体是临时分配的，所以需要重新分配block（buf_LRU_get_free_block），把临时分配的buf_page_t给释放掉，用buf_relocate函数替换掉，接着进行解压，解压成功后，设置state为BUF_BLOCK_FILE_PAGE，最后加入Unzip LRU链表中。\n  接着，我们判断这个页是否是第一次访问（buf_page_is_accessed），如果是则设置buf_page_t::access_time。\n如果mode不为BUF_PEEK_IF_IN_POOL，判断是否把这个数据页移到young list中（buf_page_make_young_if_needed）。 如果mode不为BUF_GET_NO_LATCH，给数据页加上读写锁。 如果mode不为BUF_PEEK_IF_IN_POOL，并且这个数据页是第一次访问，则判断是否需要进行线性预读。\n  从这里可以看出，#5是性能的重点，如果buffer pool中没有数据页，会产生同步IO，并且，如果没有空闲的PCB，则还可能需要进行同步刷脏，这会造成明显的stall，所以我们在日常的运维中需要格外关注这种场景。\n为物理读取准备PCB #  从上面可以看到，如果逻辑读miss，则需要准备好PCB，以便后续通过文件IO填充frame（buf_page_init_for_read），进行物理读取，其流程如下：\nbuf_page_init_for_read(){ ... // 1. 核心函数，用于获取一个空闲的PCB，可能会触发LRU List和Flush List的淘汰  block = buf_LRU_get_free_block(buf_pool); // 2. 获得buffer pool的互斥保护  buf_pool_mutex_enter(buf_pool); // 3. 对page table slot rwlock加x-latch  hash_lock = buf_page_hash_lock_get(buf_pool, page_id); rw_lock_x_lock(hash_lock); // 4. 获得block mutex  buf_page_mutex_enter(block); // 5. 初始化Page，并加到page table中  buf_page_init(buf_pool, page_id, page_size, block); // 6. 设置page的io_fix类型为BUF_IO_READ，同时解除page table slot锁定（设置io_fix而不用对block fix是因为page hash的slot已提供互斥保护）  buf_page_set_io_fix(bpage, BUF_IO_READ); rw_lock_x_unlock(hash_lock); // 7. 加入到LRU List  buf_LRU_add_block(bpage, TRUE /* to old blocks */); // 8. 持有page frame rwlock x-latch  rw_lock_x_lock_gen(\u0026amp;block-\u0026gt;lock, BUF_IO_READ); // 9. 释放block mutex  buf_page_mutex_exit(block); // 10.释放buffer pool的互斥保护  buf_pool_mutex_exit(buf_pool); ... // 11. 返回PCB  return (bpage); } 这里注意两点：\n 对buffer pool的保护在最外面，是为了其中的free list、LRU list和flush list的保护 block-\u0026gt;mutex只是为了保护io_fix和buf_fix_count，数据页（内容）的保护通过frame lock来保证，这个直到io完成才释放  获取空闲PCB #  我们再接着从#1深入下去，来探究获得空闲PCB的具体逻辑（buf_LRU_get_free_block）。\n页首先从free链表中分配，如果free链表已用尽（为空），则从LRU链表中申请（buf_LRU_search_and_free_block）。\n而从上面的LRU算法可以知道，尾部的页可以被替换，但替换需要满足以下的前提条件：\n 不是脏页（bpage-\u0026gt;oldest_modification == 0） 页没有被其他线程使用（bpage-\u0026gt;buf_fix_count == 0 \u0026amp;\u0026amp; buf_page_get_io_fix(bpage) == BUF_IO_NONE）  其中，脏页需要先进行刷盘再替换；页如果正在被其他线程使用，则不能立即被替换出去。（buf_flush_read_for_replace）\n如果在LRU链表中找到可以替换的页，则对页进行free操作，即将block→state设置为BUF_BLOCK_MEMORY，从page table和AHI中删除。\n那么，获取空闲PCB的具体流程如下：\n  整体健康检查：从free list中取首先检查free list和LRU list的长度，如果发现他们在buffer chunks占用太少的空间，则表示太多的空间被lock info，自AHI等内部结构给占用了（一般是是大事务导致的。候会报错误日志）\n  从free list中取，大部分情况下可以取到并直接返回\n  取不到再通过LRU list和flush list中evict（buf_LRU_get_free_block），在这个过程中，通过迭代的方式获取\n通过定义n_iterations，用于标识是第几次进行迭代获取的空闲block。总共分为三种情况作处理，具体如下：\n  iteration 0：\n 从free list中获取（buf_LRU_get_free_only），返回 如果free list中没有，设置try_LRU_scan，开始扫描LRU list尾部的100个page，如果找到一个可以回收的page，则进行回收（buf_LRU_free_page），并将其加入到free list中，然后再从free list上取 如果还是没有找到空闲的page，则尝试从LRU list的尾部flush一个page（buf_flush_single_page_from_LRU），并将其加入到free list中，然后再从free list上取 将n_iterations++，并重复1-3的步骤    n_iterations = 1：\n和iterator 0几乎一样，只是在扫描LRU List时是扫描整个链表而不是只扫描尾部的一部分了，并唤醒刷脏线程（page_cleaner），其余流程完全一致。如果未找到则将n_iterations++\n  n_iterations \u0026gt; 1：\n和round 1一样，只是会在在flush之前每次sleep 10ms。如果还是找不到空闲的page，则继续n_iterations++。\n  当n_iterations \u0026gt; 20时，会打印日志\n    总结一下：\n 整体健康检查 从free取 从LRU扫100，从flush刷1 从LRU扫全部，从flush刷1 从LRU扫全部，从flush刷1，并等一等，休眠10ms 重复上一步\u0026hellip; 20次以上，打日志  物理读取 #  从磁盘中将页读入缓冲池称为页的物理读取（physical read）。\n函数调用链如下：\nbuf_read_page_low buf_page_init_for_read // 从buffer pool中通过LRU算法分配PCB，加frame x-latch）用以保护其指向的页的实际存储数据） fil_io // 读取磁盘上的数据页（文件IO） buf_page_io_complete // 释放frame lock x-latch 从上面可以看出，frame block有以下作用：\n 当发起读取操作时，能够告诉当前线程发起的I/O操作是否完成，只有当buf_page_io_complete完成后，才能确保block→frame对象中的页内容是完整的 如果有其它线程并发的发起对该页的请求，则需要等待本次I/O操作完成，以保证并发访问的页的数据一致性  同时，当有多个线程并发访问同一个页时，即存在多个线程调用buf_page_init_for_read，也不会有问题，其中会通过buf_pool→mutex来进行并发控制的保护。第一个进行物理读取的线程在调用buf_page_init_for_read时会从LRU链表中返回PCB对象。\n当PCB对象通过buf_page_init_for_read 初始化操作完成后，仅仅需要调用fil_io，即可将磁盘上页的内容读取到block→frame中。这里的读取操作分为同步和异步两种（sync变量区分）。对于同步读取，操作完成后直接调用buf_page_io_complete释放持有的frame lock x-latch。对于异步读取，则通过io_thread释放所持有的x-latch（fil_aio_wait）。\n另外，对于insert buffer的bitmap页、事务系统页，读取操作必须为同步IO操作。\n随机预读 #  随机预读（buf_read_ahead_random）只会在物理读取之后触发。通过innodb_random_read_ahead控制是否开启随机预读（默认：关闭）\n随机预读是指判断某个区域内的页（1个extent）是否其大多数已被访问（13个），并且这些被访问的页是否为热点数据（热区前1/4），如果满足条件，InnoDB则认为该区域内的页都可能被访问，于是提前进行读取操作，提高数据库的读取性能。预读根据区域内的（space, offset）进行以异步IO的方式进行顺序读取。总结起来，预读就是将之后可能被访问的页通过顺序的方式进行读取，从而提高数据库性能。\n随机预读的判断代码如下：\n/* Count how many blocks in the area have been recently accessed, that is, reside near the start of the LRU list. */ for (i = low; i \u0026lt; high; i++) { const buf_page_t*\tbpage = buf_page_hash_get( buf_pool, page_id_t(page_id.space(), i)); if (bpage != NULL \u0026amp;\u0026amp; buf_page_is_accessed(bpage) \u0026amp;\u0026amp; buf_page_peek_if_young(bpage)) { recent_blocks++; if (recent_blocks \u0026gt;= BUF_READ_AHEAD_RANDOM_THRESHOLD(buf_pool)) { buf_pool_mutex_exit(buf_pool); goto read_ahead; } } } 此外，如果当前InnoDB读取压力较大时（buf_pool→n_pend_reads），也不会启用随机预读，即如果缓冲池中一半的页都在等待读取操作完成。\n/** If there are buf_pool-\u0026gt;curr_size per the number below pending reads, then read-ahead is not done: this is to prevent flooding the buffer pool with i/o-fixed buffer blocks */ #define BUF_READ_AHEAD_PEND_LIMIT\t2  buf_read_ahead_random() { ... if (buf_pool-\u0026gt;n_pend_reads \u0026gt; buf_pool-\u0026gt;curr_size / BUF_READ_AHEAD_PEND_LIMIT) { buf_pool_mutex_exit(buf_pool); return(0); } } 最后一点，预读会把整个范围内的页都读取上来，甚至还包括原来读取过的页。\n线性读取 #  线性预读（read-ahead algorithms）（buf_read_ahead_linear）通过判断页的访问是否为顺序的来触发。\n线性预读的算法为：读取一个页，如果该页是某个区域的边界（border），并且该区域内的部分页（56个，innodb_read_ahead_threshold）都已经被顺序访问（正序/逆序：通过判断数据页access time为升序/逆序），则触发线性预读操作，以异步IO的方式顺序读取之前/之后的一个extent。\n从这里可以看出，线性读取的判断比较苛刻，这是因为其主要是为了解决全表扫描的性能问题。\nInnoDB中除了有预读功能，在刷脏页的时候，也能进行预写（buf_flush_try_neighbors）。当一个数据页需要被写入磁盘的时候，查找其前面或者后面邻居数据页是否也是脏页且可以被刷盘（没有被IOFix且在old list中），如果可以的话，一起刷入磁盘，减少磁盘寻道时间。预写功能可以通过innodb_flush_neighbors参数来控制。不过在现在的SSD磁盘下，这个功能可以关闭。\n线性预读也是通过区域的方式管理，区域大小由BUF_READ_AHEAD_LINEAR_AREA定义（默认32）。判断页是否在该区域的边界：是否在该32个页的第一个或者最后一个页。\nlow = (page_id.page_no() / buf_read_ahead_linear_area) * buf_read_ahead_linear_area; high = (page_id.page_no() / buf_read_ahead_linear_area + 1) * buf_read_ahead_linear_area; if ((page_id.page_no() != low) \u0026amp;\u0026amp; (page_id.page_no() != high - 1)) { /* This is not a border page of the area: return */ return(0); } 最后，也是最容易被忽视的，线性预读要求物理上也是连续的页：通过当前读到的页，判断FIL_HEADER中保存的下一个页的位置，判断是否为物理连续：\n/* Read the natural predecessor and successor page addresses from the page; NOTE that because the calling thread may have an x-latch on the page, we do not acquire an s-latch on the page, this is to prevent deadlocks. Even if we read values which are nonsense, the algorithm will work. */ pred_offset = fil_page_get_prev(frame); succ_offset = fil_page_get_next(frame); buf_pool_mutex_exit(buf_pool); if ((page_id.page_no() == low) \u0026amp;\u0026amp; (succ_offset == page_id.page_no() + 1)) { /* This is ok, we can continue */ new_offset = pred_offset; } else if ((page_id.page_no() == high - 1) \u0026amp;\u0026amp; (pred_offset == page_id.page_no() - 1)) { /* This is ok, we can continue */ new_offset = succ_offset; } else { /* Successor or predecessor not in the right order */ return(0); } 在上面有一点特别的：通过block-\u0026gt;frame判断前一个/后一个页的偏移量时没有通过frame保护（持有block-\u0026gt;lock），但这样也不会有问题，这是因为，即使并发导致读取的数据不一致，最坏的情况也就是将不应该线性读取的页进行了读取，或者应该预读的页没有进行线性读取，因为只是读而不是写，对数据本身不会破坏。另一个考虑是，有可能进行预读的线程已经持有了对该页的frame x-latch，那么将会导致死锁产生（参加上面的注释部分）。\n线性预读和随机预读是完全不同的预读算法，具体差异见下：\n    随机预读 线性预读     触发阈值 13 56   触发判断条件 页是否活跃 访问是否顺序   目标页是否被读取 否 是   触发时间 页被读取前 页被读取后    预读是一种启发式的优化方式，但并不是一定会带来性能的提升（并不所有场景都适用），也可能会给性能带来反作用。比如，如果预读到的页之后并未被访问，则LRU机制会把有用的页evict出去，导致buffer pool的命中率下降。\n以下部分补充自阿里内核月报\nInnoDB 预读 VS Oracle 多块读\n目前，IO 仍然是数据库的性能杀手，为了提高 IO 利用率和吞吐量，不同的数据库都设计了不同的方法，InnoDB提供了预读功能（read-ahead），Oracle提供了多块读功能（multiblock-read），在这里来进行一个对比和分析。\nInnoDB read-ahead\nInnoDB提供了两种预读的方式，一种是线性预读（Linear read ahead），当连续读取一个extent的threshold个page时，会触发上/下一个extent（BUF_READ_AHEAD_PAGES=64个页）的预读。另一种是随机预读（Random read-ahead），当连续读取设定数量的page后，会触发读取这个extent的剩余page。\nInnoDB的预读是在后台异步完成的。\nOracle multiblock-read 对堆表进行全表扫描时，需要大量IO，Orache可以在session级别设置db_file_multiblock_read_count，这样在读取堆表结构的数据块的时候，一次IO读取多个数据块，大大减少了IO的次数。在针对大表的汇总分析查找中，效果非常明显。这里要注意不要设置过大，否则会造成buffer cache flooding。\n场景分析\n下面我们看两个非常典型的场景:\n 高并发，小IO：在高并发的场景下，latency主要取决于IO请求的时间，而InnoDB的预读是异步的，只能起到预热（warmup）的效果，对latency收益很小；而Oracle如果触发了multiblock-read，latency反而会上升，因为一次同步IO请求的时间增加了。 低并发，高IO吞吐：通常情况下，对线上数据的汇总查询尽量安排在业务低峰期，同时希望能够完全使用主机的资源来完成查询。在使用全表扫描时，InnoDB会触发read-ahead，每次提前异步读取下一个extent的page，加快读取的速度。 Oracle会一次IO读取多个block，提高读取的吞吐量。  所以从这里可以看到，对于第二个场景，这个优化才会产生效果。\n为什么在聚集查询的时候，Oracle的效果会比InnoDB要好？\n如果是机械盘，则又回到了 IOPS 和 throughput 的讨论上去了。InnoDB的read-ahead，在触发的时候，针对下一个extent，对每一个page提交了异步IO请求，也就是增加了IO request次数，虽然Native AIO和disk会有针对性合并IO，但仍然非常有限，而Oracle每次提交合并多个连续数据块的IO请求，能够更好利用disk的吞吐能力。\n所以，InnoDB在针对aggregation类型的查询的时候，想要完全使用IO的吞吐能力，相比较Oracle的multiblock-read，会偏弱一点。\n优化\n针对目前的InnoDB机制，可以有以下几种优化方法:\n 在session级别，提供可设置预读的触发条件，并使用多个后台线程来完成异步IO请求。因为没有减少小IO请求，收益甚小 采用和Oracle类似的方法，独立一个buffer pool，专门进行多块读，针对previous/next extent，一次读取到buffer pool中 终极优化方法，使用并行查询，Oracle在全表扫描的时候，使用/* parallel */ hint方法启动多个进程完成查询，这里需要将InnoDB的聚簇索引进行逻辑分片，采用多线程对逻辑分区进行并行查询   InnoDB logical read-ahead\n上面提到的InnoDB linear read-ahead和Oracle的multiblock read，其之所以带来了更高的吞吐量，都是基于数据存储的连续性的假设，比如InnoDB使用自增字段作为PK，或者是Oracle使用默认的堆表。\n上图是一个典型的InnoDB聚簇索引表，这样的结构很容易构造，比如使用一个非连续的字段作为索引字段，随机对记录进行插入，因此，叶子节点链表上的page no就会产生非连续性，如果进行一次全表扫描，会因为page no的非连续性而无法触发InnoDB的预读，更没有办法合并IO请求。\n因此，对于存在时间比较长，变更又比较多的大表，除非我们对于这个表进行重建，否则叶子节点的离散性会随着时间的推移越来越严重。但对于在线应用来说，重建又会产生比较大的运维风险，这里有一种平衡的方法：logical read-ahead。\nlogical read-ahead\n逻辑预读的思路是根据branch节点来预读leaf节点。\n逻辑预读使用两个扫描路径:\n 一个cursor定位到leaf page，然后根据leaf page之间的双链表，moves_up进行扫描数据； 另一个cursor定位到branch节点，因为InnoDB B-Tree结构的每一层都由双向链表进行连接，然后这个cursor就沿着branch节点进行扫描，保存扫描到的page no，然后使用异步IO，发起这些leaf page的预读。  实现\n 在row_search_for_mysql进行moves_up的过程中进行logical read-ahead branch节点扫描的cursor保存到trx结构中，生命周期到一个sql语句结束 branch cursor扫描用户可配置的page count，临时保存到数组中，对page_no进行排序 使用libaio发起异步IO读取，完成logical read-ahead。 logical read-ahead很好的提升了离散存储数据的吞吐能力，Facebook在其MySQL的逻辑备份过程中，对于大表的dump备份开启了此特性，备份速度有非常大的提升。   页的更新 #  页的更新流程 #  页的更改流程为：页面的更新通过FIX rules进行保护，并在FIX期间生产变更的日志（mini-transaction）存入private repo，并在mtr.commit时写入redo log buffer，然后依赖force-log-at-commit在事务提交时将日志持久化。而实际的脏页随后再以异步的方式写回磁盘。\n加入flush list #  flust list中保存的是虽有未写回磁盘原处的脏页，这些脏页在mtr.commit时加入flush list（release_latches），并释放latch（具体流程在mini-transaction中详细介绍）。一旦写入到flush list中，flush list的页面顺序就不再更改，即flush list的顺序是按照mtr.commit，也就是LSN的顺序排列的（oldest_modification），这从语义上代表着写入序（变更序）。如果页面再次发生修改，则更新newest_modification。\n页的刷回 #  WAL和检查点 #  开篇已经提到，buffer pool设计是为了协调CPU和磁盘之间的速度鸿沟，因此页的操作都是先在buffer pool完成的，然后再刷新回磁盘。\n如果页每次变化都立即刷回磁盘，这个开销是巨大的，并且，如果热点集中在某几个页上，性能会变得更差。另外，如果从缓冲池中将变更后的页（新版本）刷新到磁盘时发生了宕机，那么数据就会损坏（corrupted）。为了解决以上的问题，数据库系统普遍采用了WAL（Write Ahead Log）策略：当事务提交时，先写redo log，再修改页；如果宕机，则可以通过redo log来完成数据的恢复。这也是事务ACID特性中D的要求。\n理想情况下，如果redo log的空间是无限的，buffer pool可以容纳所有的数据，那么就不需要将buffer pool中页的新版本刷回磁盘，当宕机时，可以通过redo log将数据恢复到宕机发生的时刻（内存态），因此，理想情况可以归纳为两个条件：\n buffer pool可以容纳所有的数据 redo log空间无限  第一个条件在实际的环境中很难满足，这是因为数据是在持续增长的，并且，内存的价格也比磁盘昂贵，从成本上是不合算的。\n第二个条件可以做到，但是要配合checkpoint来定期将数据刷回磁盘，缩短宕机的恢复时间，并回收日志空间。\n因此，checkpoint技术可以解决以下问题：\n 缩短数据库的恢复时间 在buffer pool空间有限时，将脏页刷回磁盘 redo log空间有限时，将脏页刷回磁盘  这里有几个压力点：系统宕机、内存空间不足，磁盘空间不足。\n 数据库宕机时，不需要重做所有的redo log，因为检查点之前的页已经刷回磁盘，只需要对检查点之后的redo log进行恢复，大大减少了恢复时间 buffer pool空间不足时，根据LRU算法淘汰最近最少使用的页，如果此页是脏页，还需要强制执行checkpoint，将页的最新版本刷回磁盘 redo log file常采用循环使用的方式，而不是无限增长（从成本/管理上都会增加困难），则在重用这部分日志的话，需要保证对应的脏页刷到redo log的点位。  checkpoint机制可以有两种实现方式（即页的刷回机制）：\n sharp checkpoint fuzzy checkpoint  sharp checkpoint可以有两种场景：\n 在数据库关闭时将所有的脏页刷回磁盘（默认），即参数innodb_fast_shutdown = 1。 在运行时使用sharp checkpoint，即每次变更就立即刷回磁盘，这会极大的影响数据库的性能  InnoDB（也是大多数关系型数据库的选择）使用fuzzy checkpoint进行页的刷回，即只刷回一部分脏页，而不是刷回所有脏页。\nInnoDB中触发fuzzy checkpoint的地方有：\n master thread checkpoint async/sync flush checkpoint flush_lru_list checkpoint  master thread checkpoint是指master thread以每秒/每10秒为单位将flush list中的脏页以一定比例异步刷回磁盘，用户线程不会阻塞。\nasync/sync flush checkpoint是指当redo log空间即将用尽时，需要强制将flush list的一部分脏页刷回磁盘，以腾挪出空闲的redo log空间：async的点位为75%，sync为90%，checkpoint_age = redo_file_lsn - last_checkpoint_lsn。即\n \u0026lt; 75%，无动作 75% ~ 90%，异步刷回 \u0026gt; 90%，同步刷回  这种方式是保证重做日志的可用性（可循环使用），保证宕机可恢复性，也是为了做到D。\nflush_lru_list checkpoint是指InnoDB要保证LRU链表和free链表中至少要保留一些页（BUF_FLUSH_FREE_BLOCK_MARGIN + BUF_FLUSH_EXTRA_MARGIN个可被立即使用的页（146））。因此，在页被读取后，都会调用buf_flush_free_margin保证这点。如果free链表中没有足够的数量，就需要从LRU链表中进行脏页的刷回，一旦完成刷回，页就被放入LRU链表的尾部，以保证下次页的分配使用。\n这里留给读者一个小问题：可能出现flush的dirty page位于flush list中部，而不是按flush list刷脏吗？会有问题吗？  异步刷脏 #  从上面可以看出，如果脏页堆积，则会对性能造成严重的影响，所以InnoDB需要高效的刷回flush链表中的脏页（异步刷脏），尽可能的避免用户线程参与刷脏（同步刷脏）。在MySQL 5.6之前，刷脏的工作是由master thread负责的。在MySQL 5.6之后，刷脏交由专门的page cleaner线程负责。为了提高刷脏效率，在MySQL 5.7.4中，采用多个page cleaner进行刷脏（innodb_page_cleaners：默认4 1C3W）。\n我们下面以MySQL 5.7.4后的多线程刷脏为例来介绍异步刷脏机制。\npage cleaner线程分为一个协调线程和多个工作线程，同时协调线程本身也是工作线程，也需要进行刷脏的工作。工作队列长度为缓冲池instance的个数，使用一个全局slot数组表示（page_clean_t→slots: page_cleaner_slot_t）。page cleaner线程并未和缓冲池instance绑定，而是每次进入REQUESTED状态后，寻找一个空闲的slot进行刷脏。\n其中的核心数据结构是page_cleaner_t、page_cleaner_slot_t和page_cleaner_state_t以及两种线程buf_flush_page_cleaner_coordinator、buf_flush_page_cleaner_worker。\n整体纵览 #  我们以一张图的方式从整体和细节上来纵览：\n协调线程 #  buf_flush_page_cleaner_coordinator协调线程的主循环主线程以最多1s的间隔或者收到buf_flush_event事件来发起进行一轮刷脏。协调线程首先会调用pc_request()函数，这个函数的作用就是为每个slot所代表的缓冲池instance计算要刷脏多少页，然后把每个slot的state置为PAGE_CLEANER_STATE_REQUESTED，并且唤醒buf_flush_page_chealner_worker。由于协调线程也会和工作线程一样做具体的刷脏操作，所以在唤醒工作线程之后，自己调用pc_flush_slot()，和其它的工作线程并行去做刷脏页操作。完成自己的刷脏操作后，调用pc_wait_finished()等待所有的工作线程完成本轮的刷脏。在完成这一轮的刷脏之后，协调线程会收集一些统计信息，比如这轮刷脏所用的时间，以及对LRU和flush_list队列刷脏的页数等等。然后会根据当前的负载计算应该sleep的时间、以及下次刷脏的页数，为下一轮的刷脏做准备。在主循环线程跳过与多线程刷脏不相关的部分，主循环的核心主要就集中在pc_request()、pc_flush_slot()以及pc_wait_finished()三个函数的调用上。精简后的部分代码如下：\nwhile (srv_shutdown_state == SRV_SHUTDOWN_NONE) { if (ret_sleep != OS_SYNC_TIME_EXCEEDED \u0026amp;\u0026amp; srv_flush_sync \u0026amp;\u0026amp; buf_flush_sync_lsn \u0026gt; 0) { /* woke up for flush_sync */ mutex_enter(\u0026amp;page_cleaner-\u0026gt;mutex); lsn_t\tlsn_limit = buf_flush_sync_lsn; buf_flush_sync_lsn = 0; mutex_exit(\u0026amp;page_cleaner-\u0026gt;mutex); /* Request flushing for threads */ pc_request(ULINT_MAX, lsn_limit); /* Coordinator also treats requests */ while (pc_flush_slot() \u0026gt; 0) {} /* Wait for all slots to be finished */ ulint\tn_flushed_lru = 0; ulint\tn_flushed_list = 0; pc_wait_finished(\u0026amp;n_flushed_lru, \u0026amp;n_flushed_list); if (n_flushed_list \u0026gt; 0 || n_flushed_lru \u0026gt; 0) { buf_flush_stats(n_flushed_list, n_flushed_lru); MONITOR_INC_VALUE_CUMULATIVE( MONITOR_FLUSH_SYNC_TOTAL_PAGE, MONITOR_FLUSH_SYNC_COUNT, MONITOR_FLUSH_SYNC_PAGES, n_flushed_lru + n_flushed_list); } n_flushed = n_flushed_lru + n_flushed_list; } } 工作线程 #  buf_flush_page_cleaner_worker工作线程的主循环启动后就等在page_cleaner_t的is_requested事件上，一旦协调线程通过is_requested唤醒所有等待的工作线程，工作线程就调用pc_flush_slot()函数去完成刷脏动作。\n核心函数 #  pc_request这个函数的作用主要就是为每个slot代表的缓冲池实例计算要刷脏多少页；然后把每个slot的state设置PAGE_CLEANER_STATE_REQUESTED；把n_slots_requested设置成当前slots的总数，也即缓冲池实例的个数，同时把n_slots_flushing和n_slots_finished清0，然后唤醒等待的工作线程。这个函数只会在协调线程里调用，其核心代码如下：\nstatic void pc_request( ulint\tmin_n, lsn_t\tlsn_limit) { mutex_enter(\u0026amp;page_cleaner-\u0026gt;mutex);\t// 由于page_cleaner是全局的，在修改之前先获取互斥锁\t page_cleaner-\u0026gt;requested = (min_n \u0026gt; 0);\t// 是否需要对flush_list进行刷脏操作，还是只需要对LRU列表刷脏 \tpage_cleaner-\u0026gt;lsn_limit = lsn_limit;\t// 设置lsn_limit, 只有小于lsn_limit的数据页的oldest_modification才会刷盘  for (ulint i = 0; i \u0026lt; page_cleaner-\u0026gt;n_slots; i++) { page_cleaner_slot_t* slot = \u0026amp;page_cleaner-\u0026gt;slots[i]; // 为两种特殊情况设置每个slot需要刷脏的页数，当为ULINT_MAX表示服务器比较空闲，则刷脏线程可以尽可能的把当前的所有脏页都刷出去；而当为0是，表示没有脏页可刷。 \tif (min_n == ULINT_MAX) { slot-\u0026gt;n_pages_requested = ULINT_MAX; } else if (min_n == 0) { slot-\u0026gt;n_pages_requested = 0; } /* slot-\u0026gt;n_pages_requested was already set by page_cleaner_flush_pages_recommendation() */ slot-\u0026gt;state = PAGE_CLEANER_STATE_REQUESTED; // 在唤醒刷脏工作线程之前，将每个slot的状态设置成requested状态 \t} // 协调线程在唤醒工作线程之前，设置请求要刷脏的slot个数，以及清空正在刷脏和完成刷脏的slot个数。只有当完成的刷脏个数等于总的slot个数时，才表示本轮的刷脏结束。 \tpage_cleaner-\u0026gt;n_slots_requested = page_cleaner-\u0026gt;n_slots; page_cleaner-\u0026gt;n_slots_flushing = 0; page_cleaner-\u0026gt;n_slots_finished = 0; os_event_set(page_cleaner-\u0026gt;is_requested); mutex_exit(\u0026amp;page_cleaner-\u0026gt;mutex); } pc_flush_slot是刷脏线程真正做刷脏动作的函数，协调线程和工作线程都会调用。由于刷脏线程和slot并不是事先绑定对应的关系。所以工作线程在刷脏时首先会找到一个未被占用的slot，修改其状态，表示已被调度，然后对该slot所对应的缓冲池instance进行操作。直到所有的slot都被消费完后，才进入下一轮。通过这种方式，多个刷脏线程实现了并发刷脏缓冲池。一旦找到一个未被占用的slot，则需要把全局的page_cleaner里的n_slots_rqeusted减1、把n_slots_flushing加1，同时这个slot的状态从PAGE_CLEANER_STATE_REQUESTED状态改成PAGE_CLEANER_STATE_FLUSHING。然后分别调用buf_flush_LRU_list() 和buf_flush_do_batch() 对LRU和flush_list刷脏。刷脏结束把n_slots_flushing减1，把n_slots_finished加1，同时把这个slot的状态从PAGE_CLEANER_STATE_FLUSHING状态改成PAGE_CLEANER_STATE_FINISHED状态。同时若这个工作线程是最后一个完成的，则需要通过is_finished事件，通知协调进程所有的工作线程刷脏结束。 其核心代码如下：\nstatic ulint pc_flush_slot(void) { ulint\tlru_tm = 0; ulint\tlist_tm = 0; int\tlru_pass = 0; int\tlist_pass = 0; mutex_enter(\u0026amp;page_cleaner-\u0026gt;mutex); if (page_cleaner-\u0026gt;n_slots_requested \u0026gt; 0) { page_cleaner_slot_t*\tslot = NULL; ulint\ti; for (i = 0; i \u0026lt; page_cleaner-\u0026gt;n_slots; i++) { // 由于slot和刷脏线程不是事先定好的一一对应关系，所以在每个工作线程开始要先找到一个未被处理的slot \tslot = \u0026amp;page_cleaner-\u0026gt;slots[i]; if (slot-\u0026gt;state == PAGE_CLEANER_STATE_REQUESTED) { break; } } buf_pool_t* buf_pool = buf_pool_from_array(i); // 根据找到的slot，对应其缓冲池的实例  page_cleaner-\u0026gt;n_slots_requested--; // 表明这个slot开始被处理，将未被处理的slot数减1 \tpage_cleaner-\u0026gt;n_slots_flushing++; // 这个slot开始刷脏，将flushing加1 \tslot-\u0026gt;state = PAGE_CLEANER_STATE_FLUSHING; // 把这个slot的状态设置为flushing状态  if (page_cleaner-\u0026gt;n_slots_requested == 0) { // 若是所有的slot都处理了，则清除is_requested的通知标志 \tos_event_reset(page_cleaner-\u0026gt;is_requested); } if (!page_cleaner-\u0026gt;is_running) { slot-\u0026gt;n_flushed_lru = 0; slot-\u0026gt;n_flushed_list = 0; goto finish_mutex; } mutex_exit(\u0026amp;page_cleaner-\u0026gt;mutex); /* Flush pages from end of LRU if required */ slot-\u0026gt;n_flushed_lru = buf_flush_LRU_list(buf_pool);// 开始刷LRU链表  if (!page_cleaner-\u0026gt;is_running) { slot-\u0026gt;n_flushed_list = 0; goto finish; } /* Flush pages from flush_list if required */ if (page_cleaner-\u0026gt;requested) { // 刷flush_list链表  list_tm = ut_time_ms(); slot-\u0026gt;succeeded_list = buf_flush_do_batch( buf_pool, BUF_FLUSH_LIST, slot-\u0026gt;n_pages_requested, page_cleaner-\u0026gt;lsn_limit, \u0026amp;slot-\u0026gt;n_flushed_list); list_tm = ut_time_ms() - list_tm; list_pass++; } else { slot-\u0026gt;n_flushed_list = 0; slot-\u0026gt;succeeded_list = true; } finish: mutex_enter(\u0026amp;page_cleaner-\u0026gt;mutex); finish_mutex: page_cleaner-\u0026gt;n_slots_flushing--; page_cleaner-\u0026gt;n_slots_finished++; slot-\u0026gt;state = PAGE_CLEANER_STATE_FINISHED; if (page_cleaner-\u0026gt;n_slots_requested == 0 // 当所有的工作线程都完成了刷脏，要通知协调进程，本轮刷脏完成 \t\u0026amp;\u0026amp; page_cleaner-\u0026gt;n_slots_flushing == 0) { os_event_set(page_cleaner-\u0026gt;is_finished); } } ulint\tret = page_cleaner-\u0026gt;n_slots_requested; mutex_exit(\u0026amp;page_cleaner-\u0026gt;mutex); return(ret); } pc_wait_finished函数的主要由协调线程调用，它主要用来收集每个工作线程分别对LRU和flush_list列表刷脏的页数。以及为每个slot清0次轮请求刷脏的页数和重置它的状态为NONE。\nstatic bool pc_wait_finished( ulint*\tn_flushed_lru, ulint*\tn_flushed_list) { bool\tall_succeeded = true; *n_flushed_lru = 0; *n_flushed_list = 0; os_event_wait(page_cleaner-\u0026gt;is_finished); // 协调线程通知工作线程和完成自己的刷脏任务之后，要等在is_finished事件上，直到最后一个完成的工作线程会set这个事件唤醒协调线程  mutex_enter(\u0026amp;page_cleaner-\u0026gt;mutex); for (ulint i = 0; i \u0026lt; page_cleaner-\u0026gt;n_slots; i++) { page_cleaner_slot_t* slot = \u0026amp;page_cleaner-\u0026gt;slots[i]; ut_ad(slot-\u0026gt;state == PAGE_CLEANER_STATE_FINISHED); *n_flushed_lru += slot-\u0026gt;n_flushed_lru; // 统计每个slot分别通过LRU和flush_list队列刷出去的页数 \t*n_flushed_list += slot-\u0026gt;n_flushed_list; all_succeeded \u0026amp;= slot-\u0026gt;succeeded_list; slot-\u0026gt;state = PAGE_CLEANER_STATE_NONE; // 把所有slot的状态设置为NONE  slot-\u0026gt;n_pages_requested = 0; // 每个slot请求刷脏的页数清零 \t} page_cleaner-\u0026gt;n_slots_finished = 0; // 清零完成的slot刷脏个数，为下一轮刷脏重新统计做准备  os_event_reset(page_cleaner-\u0026gt;is_finished); // 清除is_finished事件的通知标志  mutex_exit(\u0026amp;page_cleaner-\u0026gt;mutex); return(all_succeeded); } 其他flush参数 #  MySQL还提供了一些参数来控制page cleaner的flush行为：\ninnodb_adaptive_flushing_lwm\ninnodb_max_dirty_pages_pct_lwm\ninnodb_flushing_avg_loops\ninnodb_io_capacity_max\ninnodb_lru_scan_depth\n 总的来说，如果你发现redo log推进的非常快，为了避免用户线程陷入刷脏，可以通过调大innodb_io_capacity_max来解决，该参数限制了每秒刷新的脏页上限，调大该值可以增加page cleaner线程每秒的工作量。如果你发现你的系统中free list不足，总是需要驱逐脏页来获取空闲的block时，可以适当调大innodb_lru_scan_depth 。该参数表示从每个buffer pool instance的lru上扫描的深度，调大该值有助于多释放些空闲页，避免用户线程去做single page flush。\npage cleaner主要负责flush List的刷脏，避免用户线程同步刷脏页。每隔一定时间或者接到buf_flush_event事件去刷脏页。每次执行刷的脏页数量是自适应的，计算过程有点复杂（page_cleaner_flush_pages_if_needed）。其依赖当前系统中脏页的比率，日志产生的速度以及几个参数。innodb_io_capacity和innodb_max_io_capacity控制每秒刷脏页的数量，前者可以理解为一个soft limit，后者则为hard limit。innodb_max_dirty_pages_pct_lwm和innodb_max_dirty_pages_pct_lwm控制脏页比率，即InnoDB什么脏页到达多少才算多了，需要加快刷脏频率了。innodb_adaptive_flushing_lwm控制需要刷新到哪个lsn。innodb_flushing_avg_loops控制系统的反应效率，如果这个变量配置的比较大，则系统刷脏速度反应比较迟钝，表现为系统中来了很多脏页，但是刷脏依然很慢，如果这个变量配置很小，当系统中来了很多脏页后，刷脏速度在很短的时间内就可以提升上去。这个变量是为了让系统运行更加平稳，起到削峰填谷的作用。相关函数，af_get_pct_for_dirty和af_get_pct_for_lsn。\n这里给读者留一个小问题，既然page cleaner主要负责flush list刷脏，那什么场景下进行LRU list刷脏？  hazard pointer #  Hazard Pointer提供了一种lock-free的机制：\nEach reader thread owns a single-writer/multi-reader shared pointer called \u0026ldquo;hazard pointer.\u0026rdquo; When a reader thread assigns the address of a map to its hazard pointer, it is basically announcing to other threads (writers), \u0026ldquo;I am reading this map. You can replace it if you want, but don\u0026rsquo;t change its contents and certainly keep your **delete**ing hands off it.\u0026rdquo;\n但在InnoDB中，这个概念不太一样，一个线程可以随时访问Hazard Pointer，但是在访问后，他需要调整指针到一个有效的值，便于其他线程使用，即用Hazard Pointer来加速逆向的逻辑链表遍历。\n从之前的page clean工作细节我们可以知道，可能同时有多个线程操作刷脏 ，同时为了减少锁占用的时间，在刷脏时会释放buffer pool mutex，每次刷完一个page后需要回溯到flush链表尾部，使得扫描链表的时间复杂度最差为O（N*N）。\n两个因素叠加在一起导致同一个刷脏线程刷完一个数据页A，就需要回到flush list尾部（因为A之前的脏页可能被其他线程给刷走了，之前的脏页可能已经不在flush list中了），重新扫描新的可刷盘的脏页。另一方面，数据页刷盘是异步操作，在刷盘的过程中，我们会把对应的数据页IO_FIX住，防止其他线程对这个数据页进行操作。我们假设某台机器使用了非常缓慢的机械硬盘，当前flush list中所有页面都可以被刷盘（buf_flush_ready_for_replace返回true）。我们的某一个刷脏线程拿到队尾最后一个数据页，IO fixed，发送给IO线程，最后再从队尾扫描寻找可刷盘的脏页。在这次扫描中，它发现最后一个数据页（也就是刚刚发送到IO线程中的数据页）状态为IO FIXED（磁盘很慢，还没处理完）所以不能刷，跳过，开始刷倒数第二个数据页，同样IO FIXED，发送给IO线程，然后再次重新扫描flush list。它又发现尾部的两个数据页都不能刷新(因为磁盘很慢，可能还没刷完)，直到扫描到倒数第三个数据页。所以，存在一种极端的情况，如果磁盘比较缓慢，刷脏算法性能会从O（N）退化成O（N*N）。  最本质的方法就是当刷完一个脏页的时候不要每次都从队尾重新扫描。我们可以使用Hazard Pointer来解决，方法如下：遍历找到一个可刷盘的数据页，在锁释放之前，调整Hazard Pointer使之指向flush list中下一个节点，注意一定要在持有锁的情况下修改。然后释放锁，进行刷盘，刷完盘后，重新获取锁，读取Hazard Pointer并设置下一个节点，然后释放锁，进行刷盘，如此重复。当这个线程在刷盘的时候，另外一个线程需要刷盘，也是通过Hazard Pointer来获取可靠的节点，并重置下一个有效的节点。通过这种机制，保证每次读到的Hazard Pointer是一个有效的flush list节点，即使磁盘再慢，刷脏算法效率依然是O（N）。 这个解法同样可以用到LRU list的evict算法上，提高evict效率。\n于是，在MySQL 5.6中针对flush list的扫描做了一定的修复，使用一个指针来记录当前正在flush的page，待flush操作完成后，再看一下这个指针有没有被别的线程修改掉，如果被修改了，就回溯到链表尾部，否则无需回溯。但这个设计并不完整，在最差的情况下，时间复杂度依旧不理想。\n因此，在MySQL 5.7版本中对这个问题进行了重新设计，使用多个名为hazard pointer的指针，在需要扫描LRU和flush链表时，存储下一个即将扫描的目标page，根据不同的目的分为几类：\n flush_hp：用作批量刷FLUSH LIST lru_hp：用作批量刷LRU LIST lru_scan_itr：用于从LRU链表上驱逐一个可替换的page，总是从上一次扫描结束的位置开始，而不是LRU尾部 single_scan_itr：当buffer pool中没有空闲block时，用户线程会从FLUSH LIST上单独驱逐一个可替换的page 或者 flush一个脏页，总是从上一次扫描结束的位置开始，而不是LRU尾部。  后两类的hp都是由用户线程在尝试获取空闲block时调用，只有在推进到某个buf_page_t::old被设置成true的page (大约从Lru链表尾部起至总长度的八分之三位置的page)时， 再将指针重置到Lru尾部。\n这些指针在初始化buffer pool时分配，每个buffer pool instance都拥有自己的hp指针。当某个线程对buffer pool中的page进行操作时，例如需要从LRU中移除Page时，如果当前的page被设置为hp，就要将hp更新为当前Page的前一个page。当完成当前page的flush操作后，直接使用hp中存储的page指针进行下一轮flush。\n社区优化 #  一如既往的，Percona Server在5.6版本中针对buffer pool flush做了不少的优化，主要的修改包括如下几点：\n 优化刷LRU流程buf_flush_LRU_tail 该函数由page cleaner线程调用。 原生的逻辑：依次flush 每个buffer pool instance，每次扫描的深度通过参数innodb_lru_scan_depth来配置。而在每个instance内，又分成多个chunk来调用； 修改后的逻辑为：每次flush一个buffer pool的LRU时，只刷一个chunk，然后再下一个instance，刷完所有instnace后，再回到前面再刷一个chunk。简而言之，把集中的flush操作进行了分散，其目的是分散压力，避免对某个instance的集中操作，给予其他线程更多访问buffer pool的机会。 允许设定刷LRU/FLUSH LIST的超时时间，防止flush操作时间过长导致别的线程（例如尝试做single page flush的用户线程）stall住；当到达超时时间时,page cleaner线程退出flush。 避免用户线程参与刷buffer pool 当用户线程参与刷buffer pool时，由于线程数的不可控，将产生严重的竞争开销，例如free list不足时做single page flush，以及在redo空间不足时，做dirty page flush，都会严重影响性能。Percona Server允许选择让page cleaner线程来做这些工作，用户线程只需要等待即可。出于效率考虑，用户还可以设置page cleaner线程的cpu调度优先级。 另外在Page cleaner线程经过优化后，可以知道系统当前处于同步刷新状态，可以去做更激烈的刷脏(furious flush)，用户线程参与到其中，可能只会起到反作用。 允许设置page cleaner线程，purge线程，io线程，master线程的CPU调度优先级，并优先获得InnoDB的mutex。 使用新的独立后台线程来刷buffer pool的LRU链表，将这部分工作负担从page cleaner线程剥离。 实际上就是直接转移刷LRU的代码到独立线程了。从之前Percona的版本来看，都是在不断的强化后台线程，让用户线程少参与到刷脏/checkpoint这类耗时操作中。  同步刷脏 #  从上面的介绍可以看到，用户线程如果从free list中获取不到空闲的block，会进行single page同步刷脏（single page flush），这时性能会受到严重的挑战：\n 一旦大量线程进入这个状态，就会导致严重性能下降：超频繁的fsync，激烈的dblwr竞争，线程切换等等。 同样，当redo log可用空间不足时，用户线程也会进入page flush进行同步刷脏，这在高负载场景下很常见（系统运行一段时间后，性能急剧下降。这是因为redo log产生太快，而page flush又跟不上，导致checkpoint无法推进。用户线程做fuzzy checkpoint。到这时性能就基本无法接受了）。 dblwr成为重要的单点瓶颈。 如果文件系统不支持16K原子写，必须打开double write buffer。写入ibdata的doublewrite段中，这里还有锁的开销（single page flush和batch flush），即使采用multiple page cleaner，最终扩展性还是受限于dblwr。 没有专用的LRU evict线程，LRU evict和刷脏都是page cleaner线程负责。那么，如果缓冲池已满，又有大量的脏页，page cleaner则会因为IO的开销忙于刷脏，用户线程无法从free链表中获得free page，从而进入single page flush场景。  在这几个方面可以参考percona的方案，他们向来擅长优化Io bound场景的性能，并且上述几个问题都解决了，尤其是dblwr，他们做了多分区的改进。\n从上面可以看出来同步刷脏对性能影响巨大。另外，还有一些场景也会进行同步刷脏。\n删除指定表空间所有的数据页 #  有些场景需要批量清理缓冲池中指定表空间的数据页（buf_LRU_remove_pages），这里分为三种：\n BUF_REMOVE_ALL_NO_WRITE（ALL-no_flush）：删除缓冲池中所有这个类型的数据页（LRU list和flush list），并且flush list中的脏页不写回磁盘，，用于适合rename table和5.6引入的表空间传输特性，因为space_id可能会被复用，所以需要清除内存中的一切，防止后续读取到错误的数据。 BUF_REMOVE_FLUSH_NO_WRITE（FLUSH-no_flush）：仅仅删除flush list中的数据页，也不写回磁盘，用于drop table场景。LRU不清理是因为不会被访问到，会随着时间的推移而evict。 BUF_REMOVE_FLUSH_WRITE（FLUSH）：将flush list中的脏页都都刷回磁盘，用于表空间关闭场景（比如数据库正常关闭）。  这里需要注意：由于对逻辑链表的变动需要加锁，并且删除指定表空间数据页是一个大操作，容易造成其他请求被饿死，所以InnoDB做了一个小优化（buf_flush_try_yield），即每删除BUF_LRU_DROP_SEARCH_SIZE个数据页（1024）就会释放一下buf_pool-\u0026gt;flush_list_mutex，便于其他线程执行。\n部分写 #  当将脏页刷回磁盘时，需要考虑部分写（partial write）的问题。这是因为操作系统只保证block（512B/4K）单位的写是原子操作，而InnoDB页的大小是16K。而重做日志也无法解决部分写的问题，这是因为InnoDB的重做日志是物理逻辑日志，页面内的变更是逻辑的，部分写已经造成了页的损坏（不完整），无法在损坏的页上应用逻辑操作。\n为此，InnoDB实现了double write（dblwr），其设计思想是：通过shadow page+双副本的方式，当页刷回磁盘时，先通过memcpy写到doublewrite buffer中（2MB），然后doublewrite buffer再分两次，每次1MB顺序的写入磁盘上共享表空间的doublewrite段中，然后马上调用fsync进行同步。然后再将doublewrite buffer中的页刷回磁盘中原数据页的位置。这样，如果发生部分写，则可以通过doublewrite中的页进行恢复。\n整个流程如下图所示：\n通过double write解决了部分写的问题，但是也同时引入了两次fsync的开销，现在更多的存储（文件系统）已经支持16K乃至更大的原子写粒度，这样，就可以完全关闭double write，提升写入性能。另外，每次flush最多64个页。\ndoublewrite segment的管理在事务一章讲述。\nMySQL 8.0改进 #  MySQL在宕机时，会生成巨大的core文件。在MySQL 8.0.14中，引入了innodb_buffer_pool_in_core_file在core文件中剔除缓冲池页面，极大的缩小了core文件的大小。（需要linux kernel 3.4以上，支持MADV_DONTDUMP non-POSIX extension to madvise()）\n把全局大锁buffer pool mutex拆分了，各个链表由其专用的mutex保护，大大提升了访问扩展性。实际上这是由percona贡献给上游的，而percona在5.5版本就实现了这个特性（WL#8423: InnoDB: Remove the buffer pool mutex 以及 bug#75534）。\n原来的一个大mutex被拆分成多个为free_list, LRU_list, zip_free, 和zip_hash单独使用mutex:\n LRU_list_mutex for the LRU_list; zip_free mutex for the zip_free arrays; zip_hash mutex for the zip_hash hash and in_zip_hash flag; free_list_mutex for the free_list and withdraw list. flush_state_mutex for init_flush, n_flush, no_flush arrays.  由于log system采用lock-free的方式重新实现，flush_order_mutex也被移除了，带来的后果是flush list上部分page可能不是有序的，进而导致checkpoint lsn和以前不同，不再是某个log record的边界，而是可能在某个日志的中间，给崩溃恢复带来了一定的复杂度（需要回溯日志）\nlog_free_check也发生了变化，当超出同步点时，用户线程不再自己去做preflush，而是通知后台线程去做，自己在那等待(log_request_checkpoint), log_checkpointer线程会去考虑log_consider_sync_flush，这时候如果你打开了参数innodb_flush_sync的话, 那么flush操作将由page cleaner线程来完成，此时page cleaner会忽略io capacity的限制，进入激烈刷脏\n8.0还增加了一个新的参数叫innodb_fsync_threshold，，例如创建文件时，会设置文件size,如果服务器有多个运行的实例，可能会对其他正常运行的实例产生明显的冲击。为了解决这个问题，从8.0.13开始，引入了这个阈值，代码里在函数os_file_set_size注入，这个函数通常在创建或truncate文件之类的操作时调用，表示每写到这么多个字节时，要fsync一次，避免对系统产生冲击。这个补丁由facebook贡献给上游。\n其他 当然也有些辅助结构来快速查询buffer pool:\nadaptive hash index: 直接把叶子节点上的记录索引了，在满足某些条件时，可以直接定位到叶子节点上，无需从根节点开始扫描，减少读的page个数 page hash: 每个buffer pool instance上都通过辅助的page hash来快速访问其中存储的page，读加s锁，写入新page加x锁。page hash采用分区的结构，默认为16，有一个参数innodb_page_hash_locks，但很遗憾，目前代码里是debug only的，如果你想配置这个参数，需要稍微修改下代码，把参数定义从debug宏下移出来 change buffer: 当二级索引页不在时，可以把操作缓存到ibdata里的一个btree(ibuf)中，下次需要读入这个page时，再做merge；另外后台master线程会也会尝试merge ibuf。\n"},{"id":27,"href":"/docs/MySQL/InnoDB/9_change_buffer/","title":"change buffer","section":"Inno Db","content":"change buffer是InnoDB所特有的，设计思想非常先进，首先详细介绍change buffer的设计机制和实现细节，然后介绍其为了避免死锁所做的考虑。\n设计 #  change buffer目的是减少随机访问磁盘，提升non-unique secondary index（以下简称NUSI）的变更效率，在IO-bound workload下很有必要。\n在InnoDB中，数据顺序存储在clustered index的leaf node，secondary index leaf node则是离散的，因此，对seconary index leaf node的变更操作是随机IO。为了提升secondary index的磁盘IO效率，InnoDB的做法是采用写缓冲机制，将NUSI的变更操作缓冲下来（leaf page oriented），减少随机IO，并在合适的时机将多次对同一页的操作进行merge，这块缓存称为change buffer。\n当导入大量数据时，DBA常见的一个做法是，先disable keys，然后倒入数据后再enable keys，这也是减少随机IO手段的一个体现。  这里借用官方一张图，可以给大家带来直观的理解：\n在MySQL 5.5之前，因为只支持缓存insert操作，所以叫做insert buffer，后面支持了更多的操作类型（一共有insert、delete mark、delete），才改叫change buffer。\n那为什么只对non-unique进行cache呢？这是因为unique index要保证唯一性约束，在插入记录时要判断是否唯一，势必要读取secondary page leaf node，而这样做违反了change buffer的设计目的。但是，对于unique secondary index的delete操作，是可以缓存在change buffer中的。\nchange buffer也是一颗B+ tree，这颗B+ tree的page也会使用buffer pool的内存空间（但是会限制其占用的比例，最多25% CHANGE_BUFFER_DEFAULT_SIZE）。\n在更新NUSI时，首先判断leaf page是否在buffer pool中，如果在，则直接在内存中更新page；否则满足缓存条件后将变更缓存到change buffer中（保存位置+op+data），然后在后台异步将change buffer缓存写回NUSI的leaf page中，GC并推进LWN。\n从这里可以看到，secondary index的变更的路径分为两个分支：\n 如果NUSI的leaf page在buffer pool中：这个和之前B+ tree中页的更新完全一样，不再详述，可以保证A、D，也可以保证index tree页操作的一致性（SMO） 如果不在，变更先缓存到change buffer中，然后再写回原处：这里要保证change buffer的A、D  这里更新NUSI的场景有：\n 用户线程的DML 后台purge线程  对于读取来说，直接通过物理读读取NUSI的leaf page后，需要判断change buffer中是否已缓存变更操作（缓存该页的change buffer record），如果进行了缓存，要先merge change buffer，然后再读取leaf page中的record。\n那么接下来看一下InnoDB中的change buffer的具体实现细节。\n实现 #  首先，change buffer也是一颗B+ tree，page大小也是16K，我们先从page和tree的组织谈起，然后再进入到page内部的记录，接着看merge是如何写回原处的，最后是gc，也就是purge线程推进LWM。\nchange buffer tree #  change buffer tree虽然也是一个B+ tree，但是其和之前的index tree有异同之处。\n相同的地方：\n 叶子和非叶节点存储的数据和B+ tree一样，也是branch node（node pointer，存储的是key + page_no）和leaf node（key+data）  差异的地方：\n change buffer tree全局只有一颗，类型为DICT_CLUSTERED | DICT_IBUF，index_id = 0xFFFFFFFF00000000ULL change buffer tree的index root page是固定的，位于系统表空间的（0, 4）位置 空闲页通过链表管理，链表在index root page（PAGE_BTR_IBUF_FREE_LIST），复用了原来的leaf segment header（PAGE_BTR_SEG_LEAF），空闲页的页类型为FIL_PAGE_IBUF_FREE_LIST，空闲页通过链表节点串联前后位置（PAGE_BTR_IBUF_FREE_LIST_NODE），也复用了原来的leaf segment header 因为#3复用了leaf segment header，所以用一个特殊的change buffer header page（0, 3）用于change buffer tree page的管理，其segment entry位于IBUF_TREE_SEG_HEADER（PAGE_BTR_SEG_LEAF PAGE_DATA + 0：38 + 0）  change buffer record #  change buffer record历史上有过多次版本演进，这里以MySQL 5.7.26中使用记录格式为例介绍。\nchagne buffer tree中的leaf node（页）中的记录格式为：\n其中，space_id+page_no+count可以表示页的变更序（INSERT x, DEL MARK x, INSERT x），在构建change buffer record时：\n 首先将count设为0xFFFF 然后再change buffer tree的leaf node中通过page cursor LE模式查找定位（space_id, page_no, 0xFFFF）到最后一条已存在的记录 取该记录的count+1作为此次插入记录的count  这三元组也是change buffer tree的key。\nop有3种：insert、delete mark、delete\ntypedef enum { IBUF_OP_INSERT = 0, IBUF_OP_DELETE_MARK = 1, IBUF_OP_DELETE = 2, /* Number of different operation types. */ IBUF_OP_COUNT = 3 } ibuf_op_t; change buffer bitmap page #  change buffer会将更改的辅助索引记录缓存起来，但是其必须保证merge一定成功，即保证缓存的NUSI leaf page在merge change buffer record后不会引发页的合并或分裂。\n为了保证这一点，change buffer就需要tracking NUSI leaf page上的空闲空间，存储这个信息的页被称为change buffer bitmap page。该页在每个XDE page后一页出现（每256M一个，第2个页），每个index page用4个bit来描述，整个占用8192字节。\nibuf bitmap page的页面布局如下：\nchange buffer bitmap中保存的针对每个页的bitmap中位信息如下：\n   字段 占用位 说明     IBUF_BITMAP_FREE 2 记录UNSI leaf page的剩余空间   IBUF_BITMAP_BUFFERED 1 该页是否已被chagne buffer cache，用于物理读NUSI leaf page后判断是否需要merge   IBUF_BITMAP_IBUF 1 该页是否为change buffer page，主要用于异步IO（ibuf_thread）的读操作（ibuf_page_low）    其中IBUF_BITMAP_FREE表示的剩余空间含义为：\n 0： 0 1： 512 bytes 2：1024 bytes 3：2048 bytes  从上面可以看出，因为change buffer bitmap只能追踪一个UNSI leaf page最多2KB的空闲空间，因此change buffer一次最多缓存的记录总量为2KB。\nIBUF_BITMAP_FREE的信息在每次UNSI leaf page更改时都会更新。\n缓存 #  对于change buffer，可以缓存的操作有：\n/** Allowed values of innodb_change_buffering */ static const char* innobase_change_buffering_values[IBUF_USE_COUNT] = { \u0026quot;none\u0026quot;, /* IBUF_USE_NONE */ \u0026quot;inserts\u0026quot;, /* IBUF_USE_INSERT */ \u0026quot;deletes\u0026quot;, /* IBUF_USE_DELETE_MARK */ \u0026quot;changes\u0026quot;, /* IBUF_USE_INSERT_DELETE_MARK */ \u0026quot;purges\u0026quot;, /* IBUF_USE_DELETE */ \u0026quot;all\u0026quot; /* IBUF_USE_ALL */ }; 这些操作对应到DML为：\n insert  乐观插入：inserts   update  非主键更新  乐观更新（delete mark+insert）：changes     delete  delete mark：deletes purge：purges    这里前3种都为用户线程的DML触发，而最后一种为后台purge线程触发。\n缓存条件 #  只有真正删除UNSI leaf page上的物理记录才会导致页的合并，而这只有purge线程才做这个动作。因此，只有purge线程准备插入op=IBUF_OP_DELETE的change buffer record时，才进行判定：先预估在merge完成后该page上的所有change buffer record后，还会剩下多少记录（ibuf_get_volume_buffered）。如果只剩下一条记录，为了避免成为empty page而触发页的合并，则不进行cache，而走普通的的修改流程（将UNSI leaf page读入buffer pool，merge change buffer record，然后再进行页的修改）。\n在进行UNSI操作时，满足下面条件，才进行cache（ibuf_should_try）：\n 开启了innodb_change_buffering且innodb_change_buffer_max_size不为0 必须是UNSI leaf node（delete mark不要求unique） 表没有进行flush操作（通过dict_table_t::quiesce 标识）：flush table xxx with read lock/for export  满足以上条件后，通过buf_page_get_gen尝试获取数据页：\n BUF_GET_IF_IN_POOL：只在buffer pool中查找，不在直接返回null BUF_GET_IF_IN_POOL_OR_WATCH：purge线程使用，只在buffer pool中查找（在：加LRU list+线性预读，不在：设置watch）  如果不在buffer pool中，则判断如果不会导致页的分裂、合并后，进行缓存。\n为了避免页的分裂，则通过space_id+page_no获取对应的change buffer bitmap page，然后找到该数据页的空闲空间，如果超出，则进行异步merge。\nibuf_insert_low\nif (op == IBUF_OP_INSERT) { ulint bits = ibuf_bitmap_page_get_bits( bitmap_page, page_id, page_size, IBUF_BITMAP_FREE, \u0026amp;bitmap_mtr); if (buffered + entry_size + page_dir_calc_reserved_space(1) \u0026gt; ibuf_index_page_calc_free_from_bits(page_size, bits)) { /* Release the bitmap page latch early. */ ibuf_mtr_commit(\u0026amp;bitmap_mtr); /* It may not fit */ do_merge = TRUE; ibuf_get_merge_page_nos(FALSE, btr_pcur_get_rec(\u0026amp;pcur), \u0026amp;mtr, space_ids, page_nos, \u0026amp;n_stored); goto fail_exit; } } change buffer merge #  正如前面介绍的，change buffer最多使用buffer pool的25%，所以变更的辅助索引记录不可能一直停留在change buffer索引树中，记录的变化最终还是要存储到UNSI leaf page中，这个操作称为merge。\n根据处理方式的不同，merge可以分为主动和被动两类。\n主动merge #  主动merge是指由用户线程主动发起的UNSI leaf page页的读取操作，这时会将记录merge到leaf page上，并且，由于leaf page已经读取到buffer pool，那么之后对于该页的更改操作将不会再缓存到change buffer，为同步IO。\n被动merge #   已经为NUSI leaf page buffer了太多change buffer record，可能导致NUSI leaf page分裂，发起异步IO读取leaf page后merge，最多merge8个leaf page（IBUF_MERGE_AREA） change buffer tree size超过最大值10以上（ibuf→max_size + 10 IBUF_CONTRACT_DO_NOT_INSERT），进行同步IO merge（ibuf_contract），随机merge 8个leaf page 可能引起change buffer tree上page的分裂  ibuf→size \u0026lt; ibuf→max_size，no-op ibuf→size \u0026gt;= ibuf→max_size +5（IBUF_CONTRACT_ON_INSERT_SYNC），进行同步IO merge，随机 在二者之间，进行异步IO merge，随机   后台master thread周期性merge（ibuf_merge_in_background）  IDLE：100% io_capacity ACTIVE：5% io_capacity   slow shutdown，全部merge（srv_master_do_shutdown_tasks） flush table：flush table xxx for export/with read lock; 表级别merge  死锁 #  change buffer的最大挑战是对死锁的处理。之前我们介绍过，当我们将辅助索引页从磁盘读取到buffer pool中是，需要进行change buffer的merge，而merge可能会引起ibuf tree的收缩。因此需要持有相应的latch进行并发控制：\n需要持有fsp x-latch是因为b+树索引发生合并时需要对文件存储模块进行整理。但在之前介绍过的B+树索引中，获得latch的顺序如下：\nlatch的规定顺序如下：\nenum latch_level_t { ... SYNC_FSP_PAGE, SYNC_FSP, SYNC_TREE_NODE, SYNC_TREE_NODE_FROM_HASH, SYNC_TREE_NODE_NEW, SYNC_INDEX_TREE, ... }; 如果将change buffer作为一颗普通的B+树来对待，将会导致死锁。\n还有一种情况也会导致change buffer产生死锁：所有的I/O线程都在异步读取操作，读取到的都是辅助索引页并且都需要进行change Buffer的合并，那么这时候将没有空闲的I/O线程处理对于change Buffer树的读取操作，从而导致死锁。\n通过上面的介绍可以发现，change buffer对于锁的层次设计才是其难点与技巧所在。为了避免上述死锁情况发生，InnoDB将页逻辑的分为三个层次（level）：\n 非change buffer页 除change buffer bitmap页之外的change buffer页，包括change buffer索引内存对象的latch，change buffer页的latch change buffer bitmap页  当持有某层的latch时不得持有其上层的latch。这就意味着，在处理change buffer对象时，fsp模块相关的latch的优先级要高于Insert Buffer索引树的latch！！！既然fsp模块相关的latch要比索引树的latch优先级要高，那么处理B+树索引的合并与扩展就要与普通索引树不同。\nInsert Buffer索引拥有自己的存储管理，可以发现在Insert Buffer索引的root页中存在free list链表。这样的设计可以使得Insert Buffer在合并与扩展时不需要通过fsp相关模块的latch保护，从而使得fsp模块的latch优先级可以高于Insert Buffer的设计。图11-4显示了在InnoDB引擎中，普通B+树索引（也就是用户表）与Insert Buffer的B+树索引树在文件空间管理上的不同：\n普通的B+树索引其root页保存有非叶子节点段的段头（segment header）信息和叶子节点段头。B+树索引操作是自上往下，自左往右进行加锁。当需要进行存储空间的扩展与收缩操作时，通过持有fsp模块的相关latch进行操作。这就是图11-3中显示的latch顺序。\nInsert Buffer虽然也是B+树索引，但是其只有一个段，段头保存在独立的页中（Insert Buffer header page）。另外其有独立的文件空间管理，Insert Buffer段所申请到的页都保存在root页的free list链表中。当需要进行B+树的扩展或收缩时，首先通过判断Insert Buffer的root页中的freelist链表是否有足够的空闲页。这样的设计使得在操作Insert Buffer索引树时不需要持有fsp模块的相关latch，只需要向free list链表中添加或删除页即可。也就是原fsp模块的相关latch与Insert Buffer索引树的相关存储操作分离，即之前所说的fsp模块的latch优先级高于Insert Buffer相关对象的情况，从而避免死锁问题的产生。\n"},{"id":28,"href":"/docs/MySQL/InnoDB/7_latch/","title":"latch","section":"Inno Db","content":"latch数据结构 #  先看一下整体的latch数据结构，以及之间的关系：\n下面逐个展开。\nlatch基本信息 #  latch ID #  用于标识latch，数据结构为latch_id_t。\nlatch ordering #  在数据库中，从狭义视角看，latch用于保护修改的内存对象；从全局视角看，全局操作不同对象也需要遵循特定的顺序，因此有了latching order。数据结构为latch_level_t。\n比如\nenum latch_level_t { ... SYNC_BUF_BLOCK, SYNC_BUF_PAGE_HASH, ... } latch counter #  LatchCounter负责进行latch计数，计数信息包括：spin、waits、calls，并可以动态开启/关闭计数功能（MutexMonitor），也可以通过传入的callback函数用计数值进行运算（比如在innodb中统计latch信息）。\n数据结构说明\n   变量/函数 说明     变量/函数 说明   Count spin waits calls enabled 是否开启计数   vector\u0026lt;Count*\u0026gt; m_counters m_counters   bool m_active 是否开启计数   enable/disable/reset 开启/关闭/重置计数   single_register/single_deregister 注册/注销单例计数器   sum_register/sum_deregister 注册/注销聚合计数器   iterate 迭代计数器，调用callback函数    latch元信息 #  latch元信息由以上3类数据聚合而成（ID、ordering、counter），数据结构为latch_meta_t。\n全局latch元信息 #  用于全局latch计数器统计并展示，其类型为latch_meta（vector\u0026lt;latch_meta_t*\u0026gt;）。\n全局latch计数器 #  MutexMonitor用于show engine innodb status显示全局latch计数信息。\n函数说明：\n   函数 说明     enable 开启所有LatchCounter计数   disable 关闭所有LatchCounter计数   reset 重置所有LatchCounter计数   iterate 遍历LatchMetaData，执行函数    使用场景：\n sync_check_init() mutex_monitor = UT_NEW_NOKEY(MutexMonitor()); innodb_monitor_set_option() switch (set_option) { case MONITOR_TURN_ON: mutex_monitor-\u0026gt;enable(); case MONITOR_TURN_OFF: mutex_monitor-\u0026gt;disable(); case MONITOR_RESET_VALUE: mutex_monitor-\u0026gt;reset(); innodb_show_mutex_status() ShowStatus collector; mutex_monitor-\u0026gt;iterate(collector); sync_check_close() UT_DELETE(mutex_monitor); latch状态 #  mutex_state_t，主要用于表示不同mutex的状态，参见下面的TAS mutex。\nlatch #  系统互斥量 OSMutex #  OSMutex封装了系统的mutex，在WIN32上是CriticalSection，在Linux上是pthread_mutex_t。\nOSMutex也是唯一一个未封装Policy（统计信息）的latch。\n其方法包括：\n   函数 说明     ctor dtor /   init destroy 创建 销毁   enter try_lock exit lock trylock unlock    用法：\n1. 声明 Mutex m_mutex; 2. 初始化 ctor m_mutex.init(); 3. 使用 m_mutex.enter(); .... m_mutex.exit(); 4. 销毁 dtor m_mutex.destroy(); 条件互斥量 OSEvent #  参见os0event\nTAS mutex #  在这里通过PolicyMutex模板TAS互斥量，其中包括：\n MutexImpl：原子操作的实现方式，包括system mutex、TAS、event TAS、TTAS Policy：统计信息，包括NoPolicy（NoPolicy）、GenericPolicy（单项统计）、BlockMutexPolicy（聚合统计）  这4种TAS mutex的主要区别如下图所示：\n通过以上的二元组合产生TAS mutex：\n使用场景：\nib_mutex_t、ib_bpmutex_t在InnoDB中广泛使用，其MutexImpl可以是system mutex、event TAS和TTAS，而TAS没有使用，具体场景参见下面的表格：\n   Mutex 使用场景     SysMutex file aio arrayos thread countsync array   ib_mutex_t fts_cachefile_systeminsert buffermaster keyrow loghash table   ib_bpmutex_t BPageMutex buf_block_t    reader-writer latch #  参见下节rw latch\nrw latch #  前面已经提到，MySQL封装了system mutex和system condition，但是没有封装system rwlock，MySQL自己通过TAS和osevent封装了一个rw latch。同时，rw latch还支持写操作的递归锁，即同一个线程可以多次获得写锁（依然不能同时获得读锁和写锁）。另外，为了公平的竞争（和system mutex的PTHREAD_MUTEX_ADAPTIVE_NP异曲同工），没有设计等待队列，不按照FIFO的等待顺序进入critical section（OSEvent调用的是broadcast），而是采用写者优先的方式，记录第一个等待的写者，优先通知。\nrw latch提供以下函数：\n   函数 说明     rw_lock_create/rw_lock_free 创建/销毁rw latch   rw_lock_?_lock (s/x/sx) 加latch   rw_lock_?_unlock (s/x/sx) 释放latch   rw_lock_x_lock_wait rw_lock_?_lock_nowait (x/s/sx) 等待 不等待    函数命名规则\n具体实现加上_func后缀，底层实现加上_ow后缀（TAS），附PSI信息加上pfs_前缀\n lock_word设计 #  从上图中可以看到，rw latch中的lock_word和TAS mutex不同，自旋锁的lock_word取值只有0、1，而rw latch的lock_word取值范围是(-(2 * X_LOCK_DECR), X_LOCK_DECR]，并且0x20000000为5亿+，足够使用。而且，在这个设计中，从lock_word的区间可以直接知道latch的状态，以及持有latch和等待latch的数量。\n   lock_word取值 说明     X_LOCK_DECR latch空闲   (0, X_LOCK_DECR) 有X_LOCK_DECR-lock-word个读锁   0 有1个写锁   (-X_LOCK_DECR, 0) 有-lock-word个读锁，同时还有1个写锁在等待   (-2X_LOCK_DECR, -X_LOCK_DECR] 递归写锁，有 |lock_word|-X_LOCK_DECR个写锁    数据结构 #  通过上面的介绍，这里再理解数据结构就比较容易了，所以只列出几个关键的变量。\n waiters：是否处于等待中 waiter_thread：第一个等待的写者 recursive 是个bool 变量，用来表示当前的读写锁是否支持递归写模式，在某些情况下，例如需要另外一个线程来释放这个读写锁（insert buffer需要这个功能）的时候，就不要开启递归模式了。  这里的数据结构比较简单\nrecursive 是个bool 变量，用来表示当前的读写锁是否支持递归写模式，在某些情况下，例如需要另外一个线程来释放这个读写锁（insert buffer需要这个功能）的时候，就不要开启递归模式了。\n加锁/解锁 #  等待队列 #  等待队列的设计 #  在互斥量（InnoDB spin lock, rw latch）等待时，设计了一个等待队列机制。\n设计思想是：\n因为每个线程只会有两种状态：要么等待，要么已经拿到latch（granted），所以可以设计一个等待队列，其队列中的元素个数为线程数（OS_THREAD_MAX_N）即可。\n同时为了避免sync_arra→mutex成为热点，将队列分为等待队列组（默认为1组，可以通过innodb_sync_array_size调整），在latch需要等待时，通过放置等待对象（sync_cell_t）的方式进行等待。在latch释放时通过osevent进行通知。\n在这里，为了保护sync_array_t，通过封装的system mutex（SysMutex）来保护，而不是latch，以此来避免可能存在的递归死锁情况的发生。\n在放置等待对象时，采用random方式从组中随机挑选一个等待队列。同时，在等待队列内分配slot时，采用如下策略：\n 首先尝试用中间的空洞（first_free_slot） 用单调向前推进的free slot（next_free_slot）  同时，sync_cell_t采用一次性分配，latch等待时设置，latch释放时置空的方式复用。\n为了提高性能的考虑，在acquire latch不成功时，首先进行spin操作，然后再放到等待队列中。\n整体设计如下图所示：\n设计细节 #  数据结构和实现细节如下图所示：\n死锁检测和死锁预防 #  因为latch的死锁和lock的死锁不同，只能靠程序来保证。所以需要有相应的debug机制和检测机制来一定程度上辅助开发者避免死锁的发生。\n在latch的设计中也包含了完善的死锁预防机制和死锁检测机制。\n在每次需要latch等待时（sync_array_wait_event），即调用os_event_wait之前，需要启动死锁检测机制来保证不会出现死锁，从而造成无限等待。\n在每次加锁成功（rw_lock_lock_word_decr，lock_word 递减后，函数返回之前）时，都会启动死锁预防机制，降低死锁出现的概率。\n另外，因为死锁预防和死锁检测需要扫描比较多的数据，算法上也有递归操作，所以只在debug模式下开启。\n死锁检测 #  死锁检测机制（sync_array_detect_deadlock）通过等待队列（sync_array_t）中的等待对象（sync_cell_t）上保存的等待latch（latch链表（lock-\u0026gt;debug_list））和等待thread来判断是否形成有向无环图来确定是否存在死锁。\n如果开启了多个等待队列，则该检测机制有缺陷，其只在单个等待队列上进行遍历，将无法发现死锁。\n另外，在InnoDB中还会通过srv_error_monitor_thread后台线程来定时检测（1s），来处理无限等待和长时间等待的latch。\n出现无限等待的场景是因为在lock_word操作和osevent通知直接可能会出现race condition，在这里通过判断latch是否已经可用（sync_arr_cell_can_wake_up），然后进行通知进行补偿。\n如果出现了长时间的等待，InnoDB也会干预：当latch等待超过240秒，会输出到错误日志中；如果同一个latch被检测到等到超过600秒且连续10次被检测到，则InnoDB会通过assert来自杀。\n死锁预防 #  死锁预防机制（LatchDebug）通过线程及其已持有的latching order来进行进行检测。同一个线程的加锁顺序必须从优先级高到低，即如果一个线程目前已经加了一个低优先级的锁A，在释放锁A之前，不能再请求优先级比锁A高(或者相同)的锁。\n通过锁优先级可以低死锁发生的概率，但是不能完全消除。原因是可以把锁设置为SYNC_NO_ORDER_CHECK 这个优先级，这是最高的优先级，表示不进行死锁预防检查，如果上层的程序员把自己创建的锁都设置为这个优先级，那么InnoDB 提供的这套机制将完全失效，所以要养成给锁设定优先级的好习惯。\n为了支持latch的debug，定义了一组基本数据结构：\n Latch_Level：为了避免AB-BA问题造成的死锁定义了order（上面已经介绍） sync_check_functor_t提供了基于order的acquire比较方法 CreateTracker：跟踪latch的创建信息（file、line、thread_id） MutexDebug：跟踪latch的持有信息（file、line、thread_id） Latched：每个线程所持有的latch Latches：所有线程所持有的latches LatchDebug：死锁预防检测器  sync_check_functor_t #  sync_check_functor_t作为模板方法，提供了基于latch ordering的比较，供调用线程（calling thread）检查是否持有某些latch。\n有3个具体的模板方法：\n btrsea_sync_check 是否持有btr search mutex相关的latch dict_sync_check 是否持有dictionay latching相关的latch sync_allowed_latches 是否持有某些latch  "},{"id":29,"href":"/docs/MySQL/InnoDB/10_lock/","title":"lock","section":"Inno Db","content":"在InnoDB中，采用悲观并发控制结合多版本技术，即MV2PL来提供事务的并发控制。并且，通过nextkey locking在RR级别下解决了幻读。最后，InnoDB没有锁升级机制，这得益于其行锁对象及implicit lock的设计。\n1 锁与事务 #  我们在前面谈过latch，并且比较过在并发控制上二者的区别，但是，锁的用途不仅仅如此，锁和事务戚戚相关。\n1.1 隔离性 #  与锁相关的概念有：\n 并发控制 concurrency control 序列化 serializability 隔离性 isolation  这里用ACID特性中的隔离性（I）来阐述锁。锁是用来实现事务一致性和隔离性的一种常用技术。\n事务的隔离性要求每个读/写事务对象对其他事务的操作可以互相分离，即该事务提交前对其他事务不可见。\nFirst Law of Concurrency Control ：\nConcurrent execution should not cause application programs to malfunction.\n 这也是事务隔离性的要求，即当数据库系统中的事务并发运行时，每个事务的运行不会受到其他事务的影响，好像每个并发事务都是\u0026quot;单线程\u0026quot;的在运行。\n实现隔离性有许多种方式，最为广泛使用的就是加锁（locking）技术。不过，即使采用加锁技术，仍然可以有多种实现方式，因此就有了并发控制的第二准则：\nSecond Law of Concurrency Control ：\nConcurrent execution should not have lower throughput or much higher response times than serial execution.\n 如果采用加锁技术实现的数据库并发系统的影响时间大于串行运行的方式，那么这也不是一种能被接受的加锁方式，即并发控制的第二准则要求一种简单的算法或者开销较小的方式来实现加锁技术。\n最简单的加锁技术是对每个要访问的对象加上一个锁。当事务访问一个对象，数据库自动请求并加上一个锁，在事务结束后释放该锁。若请求时该对象上已经被其他事务持有锁，则该事务等待对象上锁的释放。由此可见，锁提供了一种串行机制，用来保证同一时刻一个对象仅能被一个事务访问。\n通过多粒度（fine granular）锁可以用来提高数据库系统的并发性。比如，可以让不同事务访问同一页中的不同记录，从而提高数据库的并发度。\nInnoDB中锁的实现和Oracle数据库非常类似，提供一致性的非锁定读、行级锁支持，但InnoDB中的行级锁没有相关额外的开销，可以在保证数据一致性的基础上同时获得较高的并发性。\n1.2 事务的隔离级别 #  在DBMS发展初期，大部分的数据库系统都没有提供真正的隔离性，最初是因为系统的设计者并没有真正理解这些问题。现在各种读写异常的场景已经清晰，但DBMS需要在正确性和性能之间做出妥协。ISO和ANSI SQL标准制定了四种事务隔离级别的标准，但是很少有数据库厂商遵循这些标准，比如Oracle数据库就不支持READ UNCOMMITTED和REPEATABLE READ的事务隔离级别。\nSQL标准定义的四个隔离级别为：\n READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE  READ UNCOMMITTED称为浏览访问（browse access），仅仅对于只读事务而言的。\nREAD COMMITTED称为游标稳定（cursor stability）。\nREPEATABLE READ是2.9999的隔离，没有幻读的保护。\nSERIALIZABLE称为隔离，或3的隔离。\nSQL和SQL2标准的默认事务隔离级别是SERIALIZABLE。\nInnoDB存储引擎默认的隔离级别是REPEATABLE READ，但是与标准SQL不同的是，InnoDB存储引擎在REPEATABLE READ事务隔离级别下，使用next-key locking的锁算法，可以避免幻读的产生。这与其他数据库系统（比如Microsoft SQL Server数据库）是不同的。所以，InnoDB存储引擎在默认的REPEATABLE READ的事务隔离级别下已经能完全保证事务的隔离性要求，即达到SQL标准的SERIALIZABLE隔离级别。\n从理论上说，隔离级别越低，事务请求的锁越少或者保持锁的时间就越短。这也是为什么大多数数据库系统默认的事务隔离级别是READ COMMITTED的缘故。\n此外，大家会质疑SERIALIZABLE隔离级别带来的性能问题，但是根据Jim Gray在Transaction Processing：Concepts and Techniques一书中指出，两者的开销几乎是一样的，甚至SERIALIZABLE可能更优！！！因此在InnoDB存储引擎中选择REPEATABLE READ的事务隔离级别并不会有任何性能的损失。同样的，即使使用READ COMMITTED的隔离级别，性能也不会有大幅度的提升。\n1.3 幻读 #  解决幻读是通过一种称为谓词锁（predict lock）的方法，其锁住的不是单个记录，而是一个\u0026quot;条件\u0026quot;。通常来说，一个事务发出一个锁请求如下所示：\n\u0026lt;t, [xlock | slock], predict\u0026gt;\n谓词锁存在一些性能上的问题，而key-range locking算法是谓词锁的一种改进实现。其锁定的不是“条件”，而是范围。根据锁定的边界不同，又可以分为next-key locking和previous-key locking。假设有记录W、Y、Z，则根据next-key locking算法，可将其锁定的范围可有：\n(-∞，W]，(W，Y]，(Y，Z]，(Z，+∞)\n若插入记录X，则可锁定的范围变为：\n(-∞，W]，(W，X]，(X，Y]，(Y，Z]，(Z，+∞)\n因此在next-key locking算法下，其锁定的不是记录，而是一个范围。例如锁定记录Y其实锁定的是 (W，Y] 这个范围，当这个范围被锁定时会阻止其他事务向这个范围内插入新的记录，从而避免了幻读问题的产生。\n2. 锁的设计 #  2.1 锁模式、类型 #  InnoDB实现了如下两种标准的行级锁：\n 共享锁（S Lock），允许事务读一行数据 排他锁（X Lock），允许事务删除或者更新一行数据  但是，在数据库中，数据对象本身存在层次关系（如下图所示），如果只通过行锁来控制page、table以及database，一是锁对象多（本身资源宝贵），二是组合语义会导致设计复杂。为此，需要提供一套抽象的并发控制语义，即多粒度锁。\n在InnoDB中，为了支持多粒度锁定，提供了表锁和行锁两个粒度，允许同一事务在行锁和表锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB支持意向锁（Intention Lock）。意向锁将锁定的对象分为多个层次，意味着事务希望在更细粒度（fine granularity）上进行加锁。\n如果将上锁的对象看成一棵树的话，如果对最下层的对象上锁，也就是对最细粒度的对象进行上锁，那么首先需要对粗粒度的对象上锁。如上图所示，需要对页上的记录record上x-lock，那么首先需要依次对数据库database、表table、页page加ix-lock，最后才对记录record加x-lock。如果在这个过程中的任何一步发生等待，那么该操作需要等待粗粒度锁的完成。举例来说，在对记录record1加x-lock之前，如果已经有其他事务对table1加了s-lock，而当前事务需要对table1加ix-lock，由于ix和s不相容，所以当前事务需要等待其他事务的表锁释放后才能继续推进。\nInnoDB的意向锁设计为表级别，即在一个事务中揭示下一行将被请求的锁类型，共有两种意向锁：\n 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁 意向排他锁（IX Lock），事务想要获得一张表中某几行的排他锁  意向锁不会阻塞除全表扫描以外的任何请求，锁兼容性列表如下：\n/* LOCK COMPATIBILITY MATRIX * IS IX S X * IS + + + - * IX + + - - * S + - + - * X - - - - 从上面的表格中可以看出，意向锁互相之间完全兼容，S锁和S以及IS锁兼容，X锁和所有意向锁不兼容。\n在行锁上，InnoDB提供3种行锁算法：\n record lock：单个索引记录上的锁 gap lock：间隙锁，锁定一个范围，但不包括记录本身 next-key lock：gap lock + record lock，锁定一个范围+记录本身  InnoDB的行锁实质上是一个索引记录锁（the row-level locks are actually index-record locks），锁定的是索引上的记录。此外，根据锁的实现方式不同，还可以分为隐式锁（implicit lock）和显式锁（explicit lock）。\n2.2 锁对象 #  锁对象细分为行锁和表锁。\n2.2.1 行锁对象 #  行锁在InnoDB中使用的最为频繁，因此，InnoDB在行锁的设计上尽量减少开销（通过page粒度来构造锁对象，并通过隐式锁，infimum记录锁来降低锁的开销），因此不需要支持锁升级。行锁的数据结构用lock_rec_t表示：\n/** Record lock for a page */ struct lock_rec_t { ib_uint32_t space; // space id ib_uint32_t page_no; // page no ib_uint32_t n_bits; // bitmap位数 // 实际的bitmap }; 从上面可以看到，InnoDB中的行锁是以page为单位来进行组织管理的，page中的所有行的锁信息都以位图（bitmap）的方式表示，位图的索引与记录的heap no一一对应，这也是heap no顺序分配的原因，以保证不变性。并且，lock bitmap是根据页中的记录数量来进行分配内存空间的，所以不显式地对其进行定义。变量n_bits表示需要bitmap占用多少位以用于管理。由于page中的记录可能在之后还会进行增加，因此这里额外预留LOCK_PAGE_BITMAP_MARGIN（64）个记录的位图信息，以便在少量记录增长的情况下不需要重新分配bitmap。\nbitmap的大小\n/* Safety margin when creating a new record lock: this many extra records can be inserted to the page without need to create a lock with a bigger bitmap */ static const ulint LOCK_PAGE_BITMAP_MARGIN = 64; /** Create record locks */ class RecLock { /** Calculate the record lock physical size required, non-predicate lock. @param[in] page For non-predicate locks the buffer page @return the size of the lock data structure required in bytes */ static size_t lock_size(const page_t* page) { ulint n_recs = page_dir_get_n_heap(page); /* Make lock bitmap bigger by a safety margin */ return(1 + ((n_recs + LOCK_PAGE_BITMAP_MARGIN) / 8)); } }; 比如page(20, 100)有250条记录，则n_bit=250+64=314，那么实际位图需要额外40个字节用于位图的管理（n_bytes=1+314/8=40）。如下图所示，页中heap_no为2、4、5的记录都已经上锁：\n根据page粒度来组织行锁是一个高效的设计，这样可以极大的减少行锁的开销。因为锁是稀有资源，在传统的数据库系统设计中，如果事务占用太多的锁资源，会采用锁升级（lock escalation）的方式，将大量的细粒度锁升级为更粗粒度的锁（页锁/表锁），而在InnoDB中，这样的设计避免了锁升级。\n2.2.2 表锁对象 #  InnoDB的表锁分为意向锁和自增锁，用lock_table_t表示：\n/** A table lock */ struct lock_table_t { dict_table_t* table; // DD中的table UT_LIST_NODE_T(lock_t) locks; // 同一个table上的lock列表 }; 通常情况下，如果要在行记录上加行锁，首先需要在相应的表上加IX意向锁：\nlock_rec_lock( { ... ut_ad((LOCK_MODE_MASK \u0026amp; mode) != LOCK_S || lock_table_has(thr_get_trx(thr), index-\u0026gt;table, LOCK_IS)); ut_ad((LOCK_MODE_MASK \u0026amp; mode) != LOCK_X || lock_table_has(thr_get_trx(thr), index-\u0026gt;table, LOCK_IX)); ... } 2.3 锁子系统 #  锁是在事务的推进过程中产生的，因此也需要在每个事务中描述相应的锁对象（行锁/表锁），各个事务统一由事务子系统进行管理；同时，锁对象也统一由锁子系统进行管理。因此，InnoDB抽象了一个锁对象（lock_t），来描述事务-锁的对应关系、锁对象的具化（行锁/表锁）、行锁对象所对应的索引、行锁/表锁在全局系统中的元信息，以及具体的锁元信息（type_mode）：\n/** Lock struct; protected by lock_sys-\u0026gt;mutex */ struct lock_t { trx_t* trx; // 该锁对象对应的事务 UT_LIST_NODE_T(lock_t) trx_locks; // 活跃事务的锁链表的节点 dict_index_t* index; // 行锁所对应的索引 lock_t* hash; // 全局锁列表，对应lock_sys-\u0026gt;rec_hash union { // 具体的锁对象 lock_table_t tab_lock; // 表锁 lock_rec_t rec_lock; // 行锁 } un_member; ib_uint32_t type_mode; // 具体的锁的辕信息，锁模式\u0026amp;锁类型\u0026amp;行锁类型， lock mode、type, LOCK_GAP or LOCK_REC_NOT_GAP, LOCK_INSERT_INTENTION, wait flag, ORed }; 由锁子系统（log_sys_t）来提供全局锁视角：\n/** The lock system struct */ struct lock_sys_t{ hash_table_t* rec_hash; // 全局行锁哈希表 hash_table_t* prdt_hash; // 全局gis谓词锁哈希表（gis） hash_table_t* prdt_page_hash; // 全局page谓词锁哈希表（gis） ... }; 锁子系统的初始化：\nsrv_lock_table_size = 5 * (srv_buf_pool_size / UNIV_PAGE_SIZE); // hash cell为5倍buffer pool的页数 innobase_start_or_create_for_mysql lock_sys_create(srv_lock_table_size); 在进行全局的行锁查询时，首先通过space_id+page_no（block→lock_hash_val）在log_sys_t→rec_hash中找到page粒度的行锁对象rec_lock_t，然后根据heap no和n_bits在bitmap中判断某行是否已加锁。\n从上面可以看到，对于每个锁对象（lock_t），存在两个维度：\n 事务维度，每个事务都有获得的锁，以及等待中的锁（trx→lock→trx_locks/table_locks，trx→wait_lock） 全局维度：所有行锁都保存在锁子系统的哈希表中（lock_sys→rec_hash），lock_rec_t中的（space_id+page_no）保证了同一页的记录锁都会被hash到同一个桶中，表锁在DD的表对象中（dict_table→locks）  锁相关的数据结构关系如下图所示：\n2.4 锁的元信息 #  锁的元信息存储在lock_t.type_mode中，包括lock_mode、lock_type、record_lock type三类，如下图所示：\n从后往前看，第1-4位表示锁的模式（lock_mode）：\n#define LOCK_MODE_MASK 0xFUL /*!\u0026lt; mask used to extract mode from the type_mode field in a lock */ /* Basic lock modes */ enum lock_mode { LOCK_IS = 0, // 意向共享锁 LOCK_IX, // 意向排他锁 LOCK_S, // 共享锁 LOCK_X, // 排他锁 LOCK_AUTO_INC, // 自增锁（表级排他锁） LOCK_NONE, // consistent read LOCK_NUM = LOCK_NONE, // number of lock modes LOCK_NONE_UNSET = 255 }; 锁的模式使用方式为：\n 行锁加S/X lock 表锁一般加IS/IX lock，LOCK TABLE加S/X lock 在使用SBR binlog格式时，加AI lock  锁的类型（lock_type）细分为行锁和表锁，用第5-8位表示（只使用了2位）：\n#define LOCK_TYPE_MASK 0xF0UL // 用于从type_mode中获取锁类型 /** Lock types */ #define LOCK_TABLE 16 // 表锁 #define LOCK_REC 32 // 行锁 以上的元信息通过宏LOCK_TYPE_MASK、LOCK_MODE_MASK提取对应锁的类型和模式。\n最后一部分是锁属性，包括行锁的锁定范围、插入意向锁和谓词属性另外，在行锁的设计上，还需要通过行锁的锁定范围来保证事务的隔离性。因此，具体的行锁锁定范围算法由以下元信息描述(record_lock type)：\n#define LOCK_WAIT 256 // 处于等待中 /* Precise modes */ #define LOCK_ORDINARY 0 // next-key lock，锁住记录和记录之前的gap #define LOCK_GAP 512 // 锁住记录之前的gap（不锁记录，锁间隙） /*!\u0026lt; when this bit is set, it means that the lock holds only on the gap before the record; for instance, an x-lock on the gap does not give permission to modify the record on which the bit is set; locks of this type are created when records are removed from the index chain of records */ #define LOCK_REC_NOT_GAP 1024 // 锁住记录，不锁记录前面的gap /*!\u0026lt; this bit means that the lock is only on the index record and does NOT block inserts to the gap before the index record; this is used in the case when we retrieve a record with a unique key, and is also used in locking plain SELECTs (not part of UPDATE or DELETE) when the user has set the READ COMMITTED isolation level */ #define LOCK_INSERT_INTENTION 2048 // 插入意向锁 /*!\u0026lt; this bit is set when we place a waiting gap type record lock request in order to let an insert of an index record to wait until there are no conflicting locks by other transactions on the gap; note that this flag remains set when the waiting lock is granted, or if the lock is inherited to a neighboring record */ #define LOCK_PREDICATE 8192 // 谓词锁，用于gis索引 #define LOCK_PRDT_PAGE 16384 // page上的谓词锁，用于gis索引 举个例子，比如有记录：2，4，6，8，10。在next-key locking算法下，如果用户锁住8这条记录，那么其实锁住的是一个范围，即（6，8]（即LOCK_ORDINARY）。如果在锁住记录8的时候，同时将type_mode设置为LOCK_GAP，则这时表示锁住的范围为（6，8），并没有锁8这条记录。\n对于插入操作，需要判断下一个记录是否有锁，但是锁的类型为LOCK_GAP即可。因为不需要对插入记录的下一条记录进行加锁，这样就提高了数据库的并发性。此外，对于supremum记录，其总是为LOCK_GAP，因为其是伪记录，表示一个页中最大的记录，亦可理解该值为正无穷，锁区间为（最后一个用户记录, +∞）。\n2.5 锁的兼容性 #  InnoDB相对于传统的数据库系统，锁矩阵中增加了自增锁（auto-increment lock，AI）。AI和自身、X/S不兼容，和IS、IX兼容。\n锁兼容矩阵和锁强弱矩阵如下：\n/* LOCK COMPATIBILITY MATRIX * IS IX S X AI * IS + + + - + * IX + + - - + * S + - + - - * X - - - - - * AI + + - - - /* STRONGER-OR-EQUAL RELATION (mode1=row, mode2=column) * IS IX S X AI * IS + - - - - * IX + + - - - * S + - + - - * X + + + + + * AI - - - - + 锁的兼容矩阵表示的是\u0026quot;能不能\u0026quot;加，用于判断两个事务之间是否存在冲突，如果存在冲突，则需要等待。而锁的强弱矩阵表示的是\u0026quot;需不需要\u0026quot;加，用于在一个事务中判断如果已持有锁，是否还需要升级锁的强度。\n2.6 locking和MVCC #  InnoDB采用MVCC+SS2PL的方式综合进行事务的并发控制，即locking保证W-W的并发控制，R-W通过MVCC来实现读写之间的non-blocking。\n各个DBMS的MVCC Protocol如下：\n   Vendor Protocol Version Storage Garbage Collection Indexes     Vendor Protocol Version Storage Garbage Collection Indexes   Oracle MV-2PL Delta Vacuum Logical   Postgres MV-2PL/MV-TO Append-Only Vacuum Physical   InnoDB MV-2PL Delta Vacuum Logical   HYRISE MV-OCC Append-Only - Physical   Hekaton MV-OCC Append-Only Cooperative Physical   MemSQL MV-OCC Append-Only Vacuum Physical   SAP HANA MV-2PL Time-travel Hybrid Logical   NuoDB MV-2PL Append-Only Vacuum Logical   HyPer MV-OCC Delta Txn-level    CMU\u0026rsquo;s TDB MV-OCC Delta Txn-level     3. 显式锁和隐式锁 #  3.1 显式锁和隐式锁的区别 #  在InnoDB中，行锁的实现方式，可以有explicit lock（显式锁）和implicit lock（隐式锁）两种，区别如下：\n显式锁（explicit lock）很直接，就是实际有锁。而隐式锁（implicit lock）指的是逻辑上索引记录有X-lock，但在实际的内存对象中并不含有锁信息，这也就意味着implicit lock没有任何内存开销，从而进一步减少了InnoDB的锁开销。\n在InnoDB中，行锁的实现方式，可以有explicit lock（显式锁）和implicit lock（隐式锁）两种，区别如下：\n   锁属性 锁标志 锁范围 锁模式     显式锁 explicit lock gap explicit lock ( ) （只锁范围，通过LOCK_GAP设置） X-lock    no gap explicit lock ( ] （锁范围+记录，通过LOCK_ORDINARY设置）rec （锁记录，通过LOCK_REC_NOT_GAP设置） S-lock   隐式锁 implicit lock /  X-lock    下面举一个具体的例子：\ncreate table t ( a int not null, b int, c int, primary key(a), key(b) ) engine=InnoDB; insert into t values (1,2,3); session 2 begin; update t set b = b + 1 where a = 1; set global innodb_status_output_locks = on; ---TRANSACTION 1819, ACTIVE 698 sec 2 lock struct(s), heap size 1160, 1 row lock(s), undo log entries 1 MySQL thread id 2, OS thread handle 140688668358400, query id 19 localhost root TABLE LOCK table `tjw`.`t` trx id 1819 lock mode IX RECORD LOCKS space id 28 page no 3 n bits 72 index PRIMARY of table `tjw`.`t` trx id 1819 lock_mode X locks rec but not gap Record lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000001; asc ;; // PK (a) 1: len 6; hex 00000000071b; asc ;; // trx id = 1819 2: len 7; hex 340000013901f1; asc 4 9 ;; // rollback pointer 3: len 4; hex 80000003; asc ;; // b 4: len 4; hex 80000003; asc ;; // c 这个例子中，session 2显式开启了事务，然后通过查看锁信息得知：\nactive transaction ID = 1819\n获取了两个explicit lock锁：\n 表锁 tjw.t IX 行锁 page (29, 3) 上，锁了heap_no = 2的记录（where a = 1），锁模式为rec+X  并打印出了行记录的所有详细信息。\n从这里可以看到，secondary index (b)上没有explicit lock。所以，DML并不一定会产生explicit lock，如果加锁需要等待，则产生explicit lock锁对象，因为只有这样才能进行granted lock所释放后的wait-notify唤醒逻辑，如果不需要等待，则可能采用implicit lock。由于implicit lock的存在，在并发下情况下，对索引记录需要将implicit lock转化为explicit lock（lock_rec_convert_impl_to_expl），并将锁信息插入到全局变量lock_sys的哈希表中，以便进行锁的并发控制。\n之前已经提到，行锁本质上是索引记录锁。那么当锁定一行clustered index记录时，若该记录上还有secondary index，根据谓词锁的要求还应该对相应secondary index上的记录进行加锁。因此，implicit lock既可以存在于clustered index记录中，也可以存在于secondary index记录中。\n比如：\n 对于clustered index记录，例如用户插入了一个row id为4的新记录，但事务还未提交。这时row id为4的记录就包含一个implicit lock。但是在全局锁信息（lock_sys）中查询不到此新记录的锁，因此这个锁是隐式的。 对于secondary index记录，例如对row id为4的clustered index记录进行了更改，并且更改的列是secondary index的列，那么在该辅助索引上同样含有一个隐式锁。  那既然是implicit lock，在事务子系统和锁子系统上无据可查，那如何判断是否有implicit lock存在呢？\n3.2 clustered index记录的隐式锁 #  clustered index记录中implicit lock的判断较为简单。因为每个clustered index记录都有一个trx ID的隐藏列，只需要通过该trx ID判断当前其是否为活跃事务就能得知是否有implicit lock。如果是活跃事务，则此clustered index记录上有implicit lock；反之则不含有implicit lock。\n3.3 secondary index记录的隐式锁 #  secondary index记录中不含有trx ID的隐藏列，因此判断辅助索引记录上是否有implicit lock复杂得多。我们在之前的page一章提到过，每个seconary index page的page header都有一个PAGE_MAX_TRX_ID，其保存了一个最大事务ID，每当该secondary index page中的任何记录被更新后，都需要更新PAGE_MAX_TRX_ID。因此，secondary index记录的implicit lock判断分为两个步骤：\n 根据secondary index page的PAGE_MAX_TRX_ID进行判断 根据clustered index记录的trx ID进行判断  因为每次修改secondary index的记录，都需要相应的修改其页上的PAGE_MAX_TRX_ID，因此，如果当前的PAGE_MAX_TRX_ID \u0026lt; 当前活跃事务的最小ID，则此secondary index记录不含有implicit lock，也就是之前已经提交的事务修改了该记录。而如果\u0026gt;=，则存在以下可能：\n 存在活跃事务，修改了此secondary index记录（正在修改了自己） 存在事务（活跃/已经提交），修改了页中的其他secondary index记录（正在/已经修改他人）  因此，对于\u0026gt;=的情况，需要通过此secondary index记录对应的clustered index记录来判断其是否含有implicit lock（row_vers_impl_x_locked），补充函数调用细节。\n这个本质上是因为secondary index上只有page粒度的模糊事务信息，只能进一步通过详细事务信息+rec进行判断。\n判断secondary index记录是否持有implicit lock，只须判断是否存在未提交的活跃事务对记录进行INSERT、DELETE、UPDATE的操作。若有，则必然持有implicit lock，并返回该活跃事务对象trx_t（通过clustered index记录中的trx id隐藏列来获得）。\n我们以图示的方式来说明row_vers_impl_x_locked的判断流程：\n trx_id对应的事务为不活跃的事务，则secondary index记录rec不含有implicit lock（其他已提交的事务insert记录） prev_version = NULL，表示没有之前版本的记录，即其为当前事务插入的记录，则secondary index记录rec含有implicit lock。（当前事务insert记录） rec == entry，两个版本的secondary index记录相等，但是两个记录的delete flag位不同，则表示某活跃事务删除了记录，因此secondary index记录rec含有implicit lock。（当前事务delete记录） rec != entry，两个版本的secondary index不相同，且记录rec的delete flag为0，表示某活跃事务更新了secondary index记录，因此secondary index记录rec含有implicit lock。（当前事务update记录） rec == entry且两个记录的delete flag位相同，则既可能是当前某活跃事务修改了secondary index记录rec，也可能是之前已提交的事务修改了secondary index记录rec。如果trx_id != prev_id，则表示之前的事务已经修改了记录，因此记录rec上不含有implicit lock。否则，需要通过再之前的记录版本进行判断。（其他事务可能修改过）（当前事务、其他事务update记录） rec != entry且rec的delete flag位为1，则既可能是当前某活跃事务修改了secondary index记录rec，也可能是之前已提交的事务修改了secondary index记录rec。如果trx_id != prev_id，则表示之前的事务已经修改了记录，因此记录rec上不含有implicit lock。否则，需要通过再之前的记录版本进行判断。（当前事务、其他事务delete记录）  总结一下，隐式锁的判断逻辑整体如下：\n我们来看几个具体的例子加深理解：\ncreate table t ( a int not null, b int not null, c int not null, primary key(a), index (b) ) engine = InnoDB; insert into t values ((1,2,3); // 事务已提交 case1\nselect b from t where b = 2 for update; 此时secondary index记录的状态如下：\n此时trx_id的事务是不活跃的（insert已提交），所以b=2的secondary index记录不含有implicit lock，不会阻塞session A。\ncase 2\n   session A session B     begin;insert into t vlaues (2, 3, 4);     select b from t where b = 3 for update;（阻塞）    此时secondary index记录的状态如下：\n从上图中可以看到，记录b对应的clustered index记录为(2, 3, 4)，而其事务A是活跃的（未提交），通过clustered index记录构造之前的版本记录是NULL，因此b=3的secondary index记录含有implicit lock，会阻塞session B。\ncase 3\n   session A session B     begin;delete from t where a = 1;     select b from t where b = 2 for update;（阻塞）    此时secondary index记录的状态如下：\n从上图中可以看到，当前记录（b=2）和之前版本的secondary index记录（b=2）的值都是相等的，不同的是二者的delete marker不一样，因此b=2的secondary index记录含有implicit lock，会阻塞session B。\ncase 4\n   session A session B     begin;update t set b = 4 where a = 1;     select b from t where b = 4 for update;（阻塞）    此时secondary index记录的状态如下：\n从上图中可以看到，当前记录（b=4）和之前版本的secondary index记录（b=2）的值不同，并且当前记录的delete marker=0，所以b=4的secondary index记录含有implicit lock，会阻塞session B。\ncase 5\n   session A session B     begin;update t set b = 4 where a = 1;update t set c = 4 where a = 1;     select b from t where b = 4 for update;（阻塞）    此时secondary index记录的状态如下：\n从上图中可以看到，当前记录（b=4）和之前第一个版本的secondary index记录（b=4）的值相同，并且delete marker也相同 ，需要遍历之前的第二个版本。此时出现case 4，所以b=4的secondary index记录含有implicit lock，会阻塞session B。\ncase 6\n   session A session B     begin;update t set b = 4 where a = 1;update t set b = 2 where a = 1;delete from t where a = 1;     select b from t where b = 4 for update;（阻塞）    此时secondary index记录的状态如下：\n从上图中可以看到，当前记录（b=4）和之前第一个版本的secondary index记录（b=2）的值不同，并且delete marker都=1，而这两个记录的trx id相同，需要遍历之前的第二个版本。此时出现case 3，所以b=4的secondary index记录含有implicit lock，会阻塞session B。\n4. 表锁 #  前面提到，InnoDB的锁粒度分为表锁和行锁，所以加锁操作也分为这两类介绍：\n 表锁：通过函数lock_table完成，表加锁是根据事务和表来进行的。 行锁：通过函数lock_rec_lock完成，行记录加锁根据事务和页来进行。这里要注意两点：implicit lock \u0026amp; 锁对象lock_t的重用，下面详述。  首先从表锁开始。\n4.1 表锁的模式 #  表锁的目的是为了防止DDL和DML的并发问题，但在MySQL 5.5引入server层的MDL锁后，InnoDB层的表锁意义就没有那么大了，MDL锁本身已经覆盖了其大部分功能。\n表锁支持所有的lock_mode：\n4.1.1 IX/IS 表锁 #  意向锁，表示\u0026quot;暗示”未来需要什么样的行锁，意向锁之间不冲突，但和S/X表锁冲突（表示DML，DDL互斥语义）。\n4.1.2 X 表锁 #  当加了X表锁后，所有其他的表锁请求都需要等待。在以下场景下需要对表加X锁：\n DDL操作的最后一个阶段（ha_innobase::commit_inplace_alter_table）会对表加LOCK_X锁，以确保没有其他事务持有表级锁。一般情况下，server层的MDL锁已经能保证这一点了，其在DDL的commit 阶段已经加了排他的MDL锁。但是，在外键检查或者在crash recovery时，则需要这个保证。因为这些操作都是InnoDB自治的，不走server层，也就无法通过MDL锁保护。 会话的autocommit = 0，执行LOCK TABLE tablename WRITE会对表加X锁（ha_innobase::external_lock）。 对表空间执行维护时（discard/import），会对表加X锁（ha_innobase::discard_or_import_tablespace）。  4.1.3 S 表锁 #  在以下场景下需要对表加S锁：\n 在DDL的第一个阶段，如果当前DDL不能通过ONLINE的方式执行，则对表加S锁（prepare_inplace_alter_table_dict）。 会话的autocommit = 0，执行LOCK TABLE tablename READ会对表加S锁（ha_innobase::external_lock）。  从上面我们可以看出，对表加X/S的场景并不常见，加的更多的还是互相不冲突的IX/IS意向锁。\n4.1.4 AUTO_INC 表锁 #  自增列在数据库中是一种常见的属性，也是首选的主键方式。在通常情况下，锁在事务提交后才会释放（SS2PL），但是自增锁如果也这么做会极大的影响插入性能，因此，InnoDB中的自增锁在插入操作完成后立即释放（STMT），以提高并发插入的性能。但是，如果单条语句执行长时间的插入操作（insert into select，load data），并发插入的性能还是会受到影响。\n另外，自增锁的增长是以表为单位的，所以自增锁设计为一个表锁，这样，每张表只能有一个自增锁，锁兼容矩阵如下：\n/* LOCK COMPATIBILITY MATRIX * IS IX S X AI * AI + + - - - 从上面可以看出，自增锁和自身、S、X锁都是不兼容的。因此，在一个插入操作正在进行的时候，是不允许除意向锁之外的其他加锁操作的。\n每个表的自增值并不持久化存储到磁盘上，而是每次在InnoDB启动时通过以下SQL读取后保存到内存的表对象（dict_table_t）中：\nselect MAX(auto_inc_column) from t for update; 内存表对象dict_table_t中关于自增的变量有：\n/** Data structure for a database table. Most fields will be initialized to 0, NULL or FALSE in dict_mem_table_create(). */ struct dict_table_t { /** AUTOINC related members. @{ */ /* The actual collection of tables locked during AUTOINC read/write is kept in trx_t. In order to quickly determine whether a transaction has locked the AUTOINC lock we keep a pointer to the transaction here in the 'autoinc_trx' member. This is to avoid acquiring the lock_sys_t::mutex and scanning the vector in trx_t. When an AUTOINC lock has to wait, the corresponding lock instance is created on the trx lock heap rather than use the pre-allocated instance in autoinc_lock below. */ /** A buffer for an AUTOINC lock for this table. We allocate the memory here so that individual transactions can get it and release it without a need to allocate space from the lock heap of the trx: otherwise the lock heap would grow rapidly if we do a large insert from a select. */ lock_t* autoinc_lock; /** Creation state of autoinc_mutex member */ volatile os_once::state_t autoinc_mutex_created; /** Mutex protecting the autoincrement counter. */ ib_mutex_t* autoinc_mutex; /** Autoinc counter value to give to the next inserted row. */ ib_uint64_t autoinc; /** This counter is used to track the number of granted and pending autoinc locks on this table. This value is set after acquiring the lock_sys_t::mutex but we peek the contents to determine whether other transactions have acquired the AUTOINC lock or not. Of course only one transaction can be granted the lock but there can be multiple waiters. */ ulong n_waiting_or_granted_auto_inc_locks; /** The transaction that currently holds the the AUTOINC lock on this table. Protected by lock_sys-\u0026gt;mutex. */ const trx_t* autoinc_trx; } autoinc_lock用来作为自增锁的缓存，因为每张表在同一时刻只能有一个自增锁，所以autoinc_lock可以避免同一表锁对象（自增锁对象）在各个事务中不断的被申请。\nautoinc为表的自增值，为8个字节。\nautoinc_mutex用来保护autoinc（似乎没有必要，因为自增锁为表锁，且互相不兼容）。\n事务对象trx_t也有自增相关的变量：\nstruct trx_t { ulint n_autoinc_rows; /*!\u0026lt; no. of AUTO-INC rows required for an SQL statement. This is useful for multi-row INSERTs */ ib_vector_t* autoinc_locks; // 该事务所持有的所有AI锁，这些锁也存储在trx_locks中。 } 自增值的产生和自增锁的控制通过innodb_autoinc_lock_mode设置（ha_innobase::innobase_lock_autoinc），一共有3种模式：\nAUTOINC_OLD_STYLE_LOCKING（0）\n传统加锁模式（在MySQL 5.1引入innodb_autoinc_lock_mode之前的策略）：在分配自增值之前加上AUTO_INC锁，在SQL结束时释放掉。该模式保证了在STATEMENT复制模式下，备库执行类似INSERT … SELECT这样的语句时的一致性，因为这样的语句在执行时无法确定到底有多少条记录，只有在执行过程中不允许别的会话分配自增值，才能确保主备一致。\n很显然这种锁模式非常影响并发插入的性能，但却保证了一条SQL内自增值分配的连续性。\nAUTOINC_NEW_STYLE_LOCKING（1）\n默认值。\n 普通的insert/replace操作会先加一个dict_table_t::autoinc_mutex，然后去判断表上是否有别的线程加了LOCK_AUTO_INC锁，如果有的话，释放autoinc_mutex，并使用OLD STYLE；否则，在预留好本次插入需要的自增值之后，快速将autoinc_mutex释放掉。很显然，对于普通的并发INSERT操作，都是无需加LOCK_AUTO_INC锁的。因此大大提升了吞吐量 对于无法预计插入量的操作，比如LOAD DATA，INSERT …SELECT等，还需要使用OLD STYLE  和OLD STYLE相比，NEW STYLE也可以保证STATEMENT模式下的复制安全性，但无法保证一条插入语句内的自增值的连续性，并且，如果执行一条混合了显式指定自增值和使用系统分配两种方式的插入语句时，可能存在一定的自增值浪费。\ninsert into t (c1, c2) values (1, 'a'), (NULL, 'b'), (5, 'c'), (NULL, 'd'); 假设当前的AUTO_INCREMENT = 101，当语句执行完成后，OLD STYLE的下一个自增值为103；NEW STYLE的下一个自增值为105。这是因为在NEW STYLE中，预取了[101, 104] 4个自增值（通过插入行的个数），然后将AUTO_INCREMENT设为105，从而导致自增值103和104被浪费掉。\nAUTOINC_NO_LOCKING（2）\n只在分配时加个autoinc_mutex，并在执行完成释放。不能保证批量插入的复制安全性。\n自增锁的bug\n这是Mariadb的Jira上报的一个小bug，在row模式下，由于不走parse的逻辑，我们不知道行记录是通过什么批量导入还是普通INSERT产生的，因此command类型为SQLCOM_END，而在判断是否加自增锁时的逻辑时，是通过COMMAND类型是否为SQLCOM_INSERT或者SQLCOM_REPLACE来判断是否忽略加AUTO_INC锁。这个额外的锁开销，会导致在使用ROW模式时，InnoDB总是加AUTO_INC锁，加AUTO_INC锁又涉及到全局事务资源的开销，从而导致性能下降。\n修复的方式也比较简单，将SQLCOM_END这个command类型也纳入考虑。\n相关的Jira链接\n 4.2 表锁的加锁流程 #  加表锁调用lock_table，调用流程如下：\n 判断本事务的之前获取到的表锁（trx→lock.table_locks）强度，如果有比当前lock_mode强的，直接返回该表锁，无需加锁 lock_mode为IX/X，则事务设为读写事务 判断要加的lock和该table上的已经加上的其他事务的表锁（dict_table→locks）是否存在冲突，如果冲突，则返回冲突的锁，这也意味着需要锁等待（wait_for） 创建表锁对象 如果需要锁等待，则加入等待队列（并进行死锁检测）（lock_table_enqueue_waiting）  在#3中，如果在同一个表上更新的并发度很高，会有很多事务持有IX/IS锁，则这个链表就会非常长。\n阿里RDS做了如下优化：\n基于大多数表锁不冲突的事实，对各种表锁对象进行计数，在检查是否有冲突时，比如当前申请的是意向锁，则如果此时LOCK_S和LOCK_X的锁计数都是0，就可以认为没有冲突，直接忽略检查。由于检查是在持有锁子系统全局大锁lock_sys→mutex下进行的。在单表大并发下，这个优化的效果还是非常明显的，可以减少持有全局大锁的时间。https://mariadb.atlassian.net/browse/MDEV-7578)\n 在第4步，需要创建表锁对象，流程如下：\n  根据lock_mode判断是否需要创建lock_table_t\n自增锁：\n设置表锁的自增锁指向：table-\u0026gt;autoinc_lock\n设置table的自增锁的事务指向：table-\u0026gt;autoinc_trx\n加入到事务的持有自增锁列表中：vecor_push(trx-\u0026gt;autoinc_locks)\nS/X/IS/IX锁：\n先从事务的预分配的表锁池中获取（trx→lock.table_pool），如果池中不够，则alloc创建一个\nS、X表示需要锁住表，即当前层。对表加IS、IX意向锁表示需要对下层进行加锁\n  将表锁加入该事务的锁列表（trx-\u0026gt;lock.trx_locks）和table的锁列表（table→locks）\n  如果需要等待，设置为事务的当前等待的锁（trx-\u0026gt;lock.wait_lock）\n  我们还是用上面的锁子系统的全局视图，在其中标出在创建表锁时，需要改动的对象：\n5 行锁 #  5.1 行锁的类型 #  5.1.1 LOCK_REC_NOT_GAP #  只对记录加锁，不锁之前的gap，即记录锁。在RC下一般都是加的这个类型的记录锁（唯一例外的是unique secondary index上的duplicate key检查，加的是LOCK_ORDINARY，否则无法保证唯一性）\n5.1.2 LOCK_GAP #  只锁记录前的范围，不锁记录本身，即间隙锁（也叫GAP锁）。在RR下使用。可以通过开启innodb_locks_unsafe_for_binlog来避免GAP锁，这样只有在检查外键约束或者duplicate key时才使用GAP锁。5.1.3 LOCK_ORDINARY #  next-key lock，锁记录及其之前的间隙，即区间锁。在RR下使用，用于解决幻读问题，即在一个事务中多次执行相同的查询，会看到不同的结果（注意和不可重复读的区别）：即对\u0026quot;还不存在的数据\u0026quot;做出应对，原来没有，（中间有插入）但却读到了；原来不满足条件，（中间有更新）但却读到了（后满足条件）。这意味着需要在读取时对读取的区间加GAP锁。这也是为什么在插入记录时，需要判断下一条记录是否已加锁（是否锁定前向区间），如果已经加锁，则不让插入，以避免产生幻读。\n5.1.4 LOCK_S #   事务读（auto_commit = 0）在隔离级别为SERIALIZABLE时会给记录加LOCK_S锁，但这也取决于场景：非事务读（auto-commit）在SERIALIZABLE隔离级别下，无需加锁。  mysql_lock_tables lock_external handler::ha_external_lock ha_innobase::external_lock if (trx-\u0026gt;isolation_level == TRX_ISO_SERIALIZABLE \u0026amp;\u0026amp; m_prebuilt-\u0026gt;select_lock_type == LOCK_NONE \u0026amp;\u0026amp; thd_test_options( thd, OPTION_NOT_AUTOCOMMIT | OPTION_BEGIN)) { /* To get serializable execution, we let InnoDB conceptually add 'LOCK IN SHARE MODE' to all SELECTs which otherwise would have been consistent reads. An exception is consistent reads in the AUTOCOMMIT=1 mode: we know that they are read-only transactions, and they can be serialized also if performed as consistent reads. */ m_prebuilt-\u0026gt;select_lock_type = LOCK_S; m_prebuilt-\u0026gt;stored_select_lock_type = LOCK_S; } ha_innobase::general_fetch row_search_mvcc err = sel_set_rec_lock(pcur, rec, index, offsets, prebuilt-\u0026gt;select_lock_type, lock_type, thr, \u0026amp;mtr); 在MySQL 5.7中，show engine innodb status不会打印只读事务的信息，只能从informationschema.innodb_trx表中获取到只读事务持有的锁个数等信息。  一致性的锁定读（select \u0026hellip; in share mode），加S-lock，但在不同的隔离级别下，锁的范围有所不同：  RC隔离级别： LOCK_REC_NOT_GAP | LOCK_S RR隔离级别：如果查询条件为唯一索引且是唯一等值查询时，加的是 LOCK_REC_NOT_GAP | LOCK_S；对于非唯一条件查询，或者查询会扫描到多条记录时，加的是LOCK_ORDINARY | LOCK_S锁    通常INSERT操作是不加锁的，但如果在插入或更新记录时，检查到 duplicate key（或者有一个被标记删除的duplicate key），对于普通的INSERT/UPDATE，会加LOCK_S锁，而对于类似REPLACE INTO或者INSERT … ON DUPLICATE这样的SQL加的是X锁。而针对不同的索引类型也有所不同： 对于聚集索引（参阅函数row_ins_duplicate_error_in_clust），隔离级别小于等于RC时，加的是LOCK_REC_NOT_GAP类似的S或者X记录锁。否则加LOCK_ORDINARY类型的记录锁（NEXT-KEY LOCK）； 对于二级唯一索引，若检查到重复键，当前版本总是加 LOCK_ORDINARY 类型的记录锁(函数 row_ins_scan_sec_index_for_duplicate)。实际上按照RC的设计理念，不应该加GAP锁（bug#68021），官方也事实上尝试修复过一次，即对于RC隔离级别加上LOCK_REC_NOT_GAP，但却引入了另外一个问题，导致二级索引的唯一约束失效(bug#73170)，感兴趣的可以参阅我写的这篇博客，由于这个严重bug，官方很快又把这个fix给revert掉了。\n需要对辅助索引记录的下一条记录进行加锁是为了避免幻读问题。例如下面的语句，其不仅需要锁住辅助索引记录x，还需要锁住x的下一条记录。这样就可以避免其他事务插入同样值为x的记录，从而避免幻读问题：\nSELECT * FROM table WHERE key_column = x FOR UPDATE; 如果key_column有唯一性约束且为等值查询，就再不需要锁定下一个辅助索引记录。\n对于非等值插入，不管辅助索引是否包含唯一约束，都需要锁定下一个索引记录，从而避免幻读问题的产生。如下面的语句：\nSELECT * FROM table WHERE key_column \u0026lt;= x FOR UPDATE; 4.2.2 加行锁 #  前面提到，InnoDB的行锁其实是索引记录锁。也就是根据索引中访问得到的记录进行加锁。InnoDB使用的是索引组织表，每张用户表有一个主键值。辅助索引中包含主键，通过辅助索引访问聚簇索引中的记录，需要通过书签查找，即通过辅助索引记录中对应的主键值再次查询聚簇索引。\n此外，InnoDB还设计了implicit lock，保证并不是每次查询都需要进行加锁。总结来说，加锁的过程如下：\n 通过主键进行加锁的语句，仅对聚集索引记录进行加锁 通过辅助索引记录进行加锁的语句，首先对辅助索引记录加锁，再对聚集索引记录进行加锁 通过辅助索引记录进行加锁的语句，可能还需要对下一个记录进行加锁  我们确定了行锁的具体锁类型后，就可以进行加行锁了。加行锁调用lock_rec_lock，通过函数参数可以知道，我们可以确定在哪个索引的哪一页的哪一行上加什么样的行锁，参数如下：\n impl：表示是否加implicit lock，（true为不加锁） mode：锁类型 block：数据页 heap_no：数据行 index：记录所在的index  在加行锁中，需要着重注意这两点：\nimplict lock的处理：是否为implicit lock，通过函数lock_rec_lock中的参数impl来决定。如果是implicit lock，并且当前没有该锁的冲突模式存在，则不需要产生一个lock_t的对象。否则，通过函数lock_rec_create创建一个lock_t对象，并加入等待队列。这样的设计是为了当不存在冲突模式时，可以减少锁的创建。比如，如果当前只有一个线程在执行UPDATE t SET key_column=x WHERE row_id=1，如果操作不需要发生等待则不需要对辅助索引记录x进行加锁。\n锁对象的重用：重用是为了减少锁的开销，锁重用的前提是同一事务锁住的同一个页面中的记录，并且锁的模式相同。例如：\nBEGIN; SELECT * FROM t WHERE rowid = x FOR UPDATE; SELECT * FROM t WHERE rowid = y FOR UPDATE; 当第一个SQL语句执行的时候，InnoDB会创建一个锁对象lock_t，并将记录x所在的heap no对应的lock bitmap相应位置1，如果记录y和记录x位于同一页，则可以重用锁对象lock_t，将y相应的bit置1。\n此外，如果同一事务访问同一行记录，并且第二次加的锁的强度弱于之前的锁，同样不需要创建锁对象。这也是一种锁重用的场景。\nBEGIN; SELECT * FROM t WHERE rowid = x FOR UPDATE; SELECT * FROM t WHERE rowid = x LOCK IN SHARE MODE; 第一个SQL首先创建一个x lock的锁对象，第二次请求的锁类型为s lock，因此不需要再次创建锁对象，可以重用。\n下面看一下具体的加锁流程。\n行锁的处理流程分为快速加锁（lock_rec_lock_fast）和慢速加锁（lock_rec_lock_slow）两种。\n快速加锁\n对于冲突少的场景，这时比较普遍的加锁方式，以下场景可以走fast lock：\n 记录所在的页上没有任何记录锁（没有explicit lock），即没有锁对象，创建锁对象，加入lock_sy→rec_hash。 记录所在的页上只存在一个记录锁，并且属于当前事务，锁模式相同，直接设置对应的记录的bit即可。  对于不满足fast lock的，走slow lock。\n慢速加锁\n如果行数据上没有更强级别的锁，也没有冲突的锁，并且加的不是隐式锁，就加入行锁队列。核心思想是复用锁对象，如果要加锁的行数据上当前没有其它锁等待，并且行所在的数据页上有相似的锁对象（lock_rec_find_similar_on_page）就可以直接设置对应行的 bitmap 位，表示加锁成功。如果有其它锁等待，就重新创建一个锁对象。\n加锁流程如下：\n 判断当前事务是否已经持有了一个强度更高或者相同的锁，如果是的话，直接返回成功（lock_rec_has_expl）; 检查是否存在和当前申请锁模式冲突的锁（lock_rec_other_has_conflicting），如果存在的话，就创建一个锁对象（RecLock::RecLock），并加入到等待队列中（RecLock::add_to_waitq），并进行死锁检测 如果没有冲突的锁，则入队列（lock_rec_add_to_queue）：已经有在同一个Page上的锁对象且没有别的会话等待相同的heap no时，可以直接设置对应的bitmap（lock_rec_find_similar_on_page）；否则需要创建一个新的行锁对象  在上面的第二条判断时，更细节的判断如下：\n如果上述两条都不满足，即不是相同的事务，基本锁类型也不兼容，那么满足下面任意一条，同样返回false，不需要等待；否则返回 true，需要等待：\n 如果当前锁锁住的是supremum或者LOCK_GAP为1并且LOCK_INSERT_INTENTION为0。因为不带LOCK_INSERT_INTENTION 的 GAP 锁不需要等待任何东西，不同的用户可以在 gap 上持有冲突的锁。 如果当前锁 LOCK_INSERT_INTENTION 为 0 并且锁是 LOCK_GAP 为 1。因为行锁（LOCK_ORDINARY LOCK_REC_NOT_GAP）不需要等待一个 gap 锁。 如果当前锁 LOCK_GAP 为 1，锁 LOCK_REC_NOT_GAP 为 1。同样的，因为 gap 锁没有必要等待一个 LOCK_REC_NOT_GAP 锁。  如果锁 LOCK_INSERT_INTENTION 为 1。此处是最后一步，说明之前的条件都不满足，源码中备注描述如下：\n/* No lock request needs to wait for an insert intention lock to be removed. This is ok since our rules allow conflicting locks on gaps. This eliminates a spurious deadlock caused by a next-key lock waiting for an insert intention lock; when the insert intention lock was granted, the insert deadlocked on the waiting next-key lock. Also, insert intention locks do not disturb each other. */ 比如，如果行数据上已经加了 LOCK_S | LOCK_REC_NOT_GAP, 再尝试去加 LOCK_X | LOCK_GAP，LOCK_S 和 LOCK_X 本身是冲突的，但是满足上述第 3 个条件，返回 FALSE，不需要等待。\n我们还是用上面的锁子系统的全局视图，在其中标出在创建行锁时，需要改动的对象：\n4.2 释放锁及唤醒 #  根据隔离级别不同，事务中锁的释放时机也不同，但是写锁都是在事务提交时释放的（除RU外），即SS2PL。但有两种例外场景，写锁的释放会提前：\n 自增锁，前面已经提到了，在插入语句结束就释放（lock_unlock_table_autoinc） 在RC隔离级别下执行DML语句时，从引擎层返回到Server层的记录，如果不满足where条件，则需要立刻unlock掉（ha_innobase::unlock_row）。  自增锁的释放\n/*****************************************************************//** Commits a transaction in an InnoDB database or marks an SQL statement ended. @return 0 or deadlock error if the transaction was aborted by another higher priority transaction. */ static int innobase_commit(...) { ... /* We just mark the SQL statement ended and do not do a transaction commit */ /* If we had reserved the auto-inc lock for some table in this SQL statement we release it now */ if (!read_only) { lock_unlock_table_autoinc(trx); } /* Store the current undo_no of the transaction so that we know where to roll back if we have to roll back the next SQL statement */ trx_mark_sql_stat_end(trx); } 写锁的释放由函数lock_trx_release_locks完成，释放时依次遍历trx→lock.trx_locks即可。在释放时InnoDB还做了优化，如果该事务持有的行锁对象太多，每释放1000（LOCK_RELEASE_INTERVAL）个锁对象，会暂时释放下lock_sys-\u0026gt;mutex再重新持有，防止InnoDB hang住。\n对于行锁的释放（lock_rec_dequeue_from_page），除了将其从全局锁列表（log_sys→rec_hash）中移除外，还需要将后续在等待队列中的其他不冲突的锁唤醒（lock_rec_grant）。比如释放某个记录的X-lock，那么会唤醒其行上等待中的下一个S-lock。\n从行锁的释放和唤醒可以看到，其中的逻辑开销比较大，尤其是大量线程在等待少量几个行锁时。当某个行锁从hash链上移除时，InnoDB实际上通过遍历相同page上的所有等待的行锁，并判断这些行锁等待是否可以被唤醒。而判断唤醒的逻辑又会遍历一次。这里的核心原因是因为行锁的链表维护是基于\u0026lt;space_id, page no\u0026gt;的，并不基于行的heap no构建的。这个问题官方讨论过，可以参阅bug#53825。提到使用\u0026lt;space_id, page no, heap no\u0026gt;来构建链表，移除bitmap会浪费更多的内存，但效率更高，而且现在的内存也没有以前那么昂贵。\n表锁对象的释放（lock_table_dequeue）会遍历表锁链表（un_member.tab_lock.locks \u0026amp; LOCK_WAIT），也会唤醒后续的下一组不冲突的表锁。比如释放X，等待队列中是S, IX, IS, X，则会将S, IX, IS这三个等待中的表锁一起唤醒。\n另外在Query Cache中，如果表级锁的类型不为LOCK_IS，且当前事务修改了数据，就将表对象的dict_table_t::query_cache_inv_id设置为当前最大的事务id。在检查是否可以使用该表的Query Cache时会使用该值进行判断（row_search_check_if_query_cache_permitted），如果某个用户会话的事务对象的low_limit_id（即最大可见事务id）比这个值还小，说明它不应该使用当前table cache的内容，也不应该存储到query cache中。\n锁的等待（lock_wait_suspend_thread）和唤醒实际上是线程的等待和唤醒，并通过OS_EVENT机制来实现唤醒和锁超时。\n5. 行锁的维护 #  各个事务在其加锁操作完成后，内存中可能产生多个行锁对象，这些行锁对象通过（space_id, page_no）映像到锁子系统的lock_sys→rec_hash中。但是，页也随着事务的推进时刻发展着变化，比如B+树的分裂和合并，这需要对已经存在的行锁对象进行维护。\n下面将对各个数据变更的场景分别来看如何来维护行锁对象。\n5.1 插入 #  插入操作的步骤如下：\n 首先对表加上IX表。 根据查询模式PAGE_CUR_LE定位记录next_rec。 判断记录next_rec是否有锁，有的话等待锁的释放，否则直接插入。  插入操作需要定位插入记录的下一条记录，这是next-key locking算法所要求的，因为该算法锁定的是区间，而不仅仅是记录本身。\n比如表中有如下记录：\n1、2、3、4、5、7、8 如果要insert 6，则\n  首先对表加IX锁\n  根据查询模式PAGE_CUR_LE定位到记录5\n  判断记录5的下一条记录（7）是否有锁（lock_rec_insert_check_and_lock，参数inherit用来判断是否在插入完成后调用函数lock_update_insert来对已经锁定的范围进行更新）。根据next-key locking算法，锁定范围应该是(5, 7]或者(5, 7)（GAP标记位=1）\n如果记录7上已经有锁，则不允许在这个范围内进行插入，所以操作将阻塞（lock_rec_enqueue_waiting），等待记录7上锁的释放。这时会产生锁对象，锁定的记录位next_rec（7），锁的类型为LOCK_X | LOCK_GAP\n如果记录7上没有锁，则直接插入，不产生任何锁对象\n  另外，如果下一条记录next_rec上有锁，不管持有该锁的是否为插入操作事务本身，当插入操作完成后（无需事务提交），需要更新锁定的范围（lock_update_insert）。比如在这个例子中，如果插入了6这条记录，则将原来锁定的范围从 (5, 7] 更新为 (5,6) ，(6, 7]，这样就可以阻止其他事务在（5，6）的范围内进行插入操作。\n另外需要注意，如果插入的表上有辅助索引，那么还需要对辅助索引记录进行锁的判断，其方法与步骤2、3相同。只是在判断可以进行插入后，还需要额外更新辅助索引页中的page header.PAGE_MAX_TRX_ID。\n5.2 更新 #  当事务需要对记录进行更新（包括删除）前，首先尝试对更新的记录加上X（implicit lock），如果待更新的记录上存在其他锁，则事务被阻塞，需要等待记录上的锁被释放。\nlock_clust_rec_modify_check_and_lock和lock_rec_rec_modify_check_and_lock分别对聚集索引和辅助索引进行加锁。两者的过程基本相同，都是调用lock_rec_lock对更新的记录进行加锁操作，不同的是：\n 聚集索引记录加锁前首先需要将记录上implicit lock转化为explicit lock 辅助索引记录加锁成功后，还需要更新辅助索引页的page header.PAGE_MAX_TRX_ID  在事务对记录进行更新的过程中，如果记录不能进行原地更新，则需要对锁进行维护，其步骤如下：\n 将更新记录的锁信息移动到页的infimum记录上（lock_rec_store_on_page_infimum） 删除原记录 插入更新完成后的记录 将页的infimum记录上的锁重新移动到新插入的记录上  这里可以发现伪记录infimum的作用，诣在更新时用于临时保存更新记录上的锁信息。这样做的好处是可以提高更新锁的效率，否则需要先删除老的锁对象然后再创建新的锁对象。\n5.3 purge #  InnoDB进行记录的删除时，分为标记删除和真正删除两步。首先将delete flag置为1，然后通过后台的purge线程将记录真正的删除。\n当purge线程真正删除记录操作完成后，删除记录的下一个记录需要继承删除记录的锁定范围，并且其模式是GAP，同时释放并重置删除记录上等待锁的信息（lock_update_delete）。\n另外，对于标记为delete flag的加锁记录，InnoDB存储引擎在定位记录后需要进一步扫描记录才能确定查询是否为结束。这里举个例子：\ncreate table t( a int primary key, b varchar(30) ) engine=InnoDB; insert into t values (1, 'a'); 接着执行下面的操作：\nbegin; update t set b = repeat('a', 30) where a = 1; select * from t where a \u0026lt;=1 for update; 首先执行第一个update语句，因为记录1的大小发生了变化，不能进行原地更新（varchar之前只存储了\u0026rsquo;a'，放不下更新后的30个\u0026rsquo;a'），因此对于这个update操作，首先进行标记删除，然后再插入一条新记录。当purge操作发生前，这个页中包含了两个PK=1的记录，这也符合InnoDB的MVCC。但在此时，先执行了select，InnoDB会对PK=1的两个记录都加上一个X-lock。因为第一次加锁的记录，其record header的delete flag被标记为了1，因此还需要访问record header中的next record来判断是否已经结束的扫描。\n然而这有时会导致一些用户可能难以理解的问题，比如下面这个例子：\ndrop table if exists t; create table t( a int primary key, b varchar(30) )engine=InnoDB; insert into t values (1, 'a'); // heap_no: 2 insert into t values (2, 'b'); // heap_no: 3 insert into t values (3, 'c'); // heap_no: 4 insert into t values (4, 'd'); // heap_no: 5 首先进行会话A：\nbegin; select * from t where a = 4 for update; 接着进行会话B：\nbegin; select * from t where a \u0026lt;= 2 lock in share mode; delete from t where a = 3; select * from t where a \u0026lt;= 2 lock in share mode; // blocking 用户会“惊奇地”发现会话B中的第二个SELECT语句会被阻塞，而同样第一次的SELECT操作是已经加锁成功的。其实导致这个现象的原因是SELECT游标锁定的最大记录被标记为了删除（未被真正PURGE删除），因此，当第二次再次执行SELECT操作时，需要进一步锁定记录（a = 4)，而该记录已经在session A的事务中被锁定了。\n之前已经说过，同一范围的GAP类型锁是可以互相兼容的，例如两个事务分别持有（2，4）范围的GAP锁是被允许的，GAP锁只是用来阻止在这个范围内进行插入，甚至是持有GAP锁的事务本身。例如有2、3、4这三个记录，事务A持有（2，3）这个GAP锁，类型为X。而事务B持有（3，4）这个GAP锁，类型为S。不同事务可能持有不同兼容性的GAP锁如图9-12所示。\n图9-12\n从图9-12可以看到，若事务A、B开始持有不同类型的GAP锁，若事务C删除了记录3，并且事务提交后进行了purge操作。这时会对锁定的范围进行合并，而事务A和B这时都锁定了（2，4）这个范围，然而持有锁的类型完全不同。\n5.4 一致性的锁定读 #  默认情况下，InnoDB使用一致性的非锁定读（consistent non-locking read），即读取不会被阻塞。但是，某些情况下用户希望通过锁定读取（locking reads）的方式来保证数据的一致性，即通过语法lock in share mode和for update主动的对读取进行加锁操作，这样的一种方式称为一致性的锁定读（consistent locking read）。lock_clust_rec_read_check_and_lock和lock_sec_rec_read_check_and_lock分别用来对聚集索引记录和辅助索引记录进行加锁。\n对于聚集索引记录，只需对主键值进行加相应的锁就可以了。而对于辅助索引记录，除了需要对辅助索引记录本身加锁，还需要对主键索引记录加锁。\n比如对前面的表t，加上一个辅助索引：\nALTER TABLE t ADD KEY idx_b(b); 然后运行\nSELECT * FROM t WHERE b='a' LOCK IN SHARE MODE; 那么需要做如下加锁操作：\n  辅助索引加锁：b=\u0026lsquo;a\u0026rsquo;的记录加S-lock\n  聚集索引加锁：a=1 的记录加S-lock\n  防止幻读\n如果辅助索引有唯一性约束，则无须此步（唯一性保证不会有2个记录的b=‘2’）\n如果没有，则需要对辅助索引记录\u0026rsquo;a\u0026rsquo;的下一条记录加上S-lock。（为了避免其他事务插入b等于‘a’的记录，从而导致幻读问题的产生）\n  更详细的行锁type_mode参见行锁的锁类型一节\n5.5 页的分裂 #  当一个页中的记录数增加到一定程度后，就需要存储到多个页中，即产生页的分裂，从而导致页中的锁信息发生变化，插入操作会引起页的分裂。\n举个例子：\ncreate table t( a int not null primary key, b blob )engine=InnoDB; insert into t values (1, repeat('a', 7000)); insert into t values (2, repeat('b', 7000)); 此时，页中的2条记录存储在一个页中（假设为(20, 100)）。\n紧接着，进行如下操作：\nbegin; select * from t where a = 1 lock in share mode; select * from t where a = 2 lock in share mode; 此时行锁对象在内存中的状态为：\n如果再插入新的记录：\ninsert into t values (3, repeat('c', 7000)); 这时就会产生页的分裂，第2个记录从第一个页移动到了第二个页中，并且需要对锁信息进行维护。\nInnoDB在实际进行页分裂时，可以往左或者往右进行，在这个例子中，页是往右分裂的。维护锁信息的步骤（lock_update_split_right）如下：\n 确定分裂点记录split_rec 将记录split_rec到尾部（supremum）之间的所有锁移动到新页中，修改lock bitmap的值（lock_rec_move） 将原来页中supremum记录持有的锁移动到新页的supremum记录上（lock_rec_move） 将新页中的第一条记录的锁继承给原页的记录supremum（类型为gap）（lock_rec_inherit_to_gap）  如图所示，灰色表示持有锁的记录：\n接下来详细解释一下上面的步骤成因，因为innoDB使用的是next-key locking算法，因此锁定的是区间，比如页中有以下记录：\nPAGE: R1, R2, R3, R4, …… Rn-1, Rn\n那么可以锁定的范围有：\nPAGE: ( infimum , R1 ], ( R1 , R2 ], ( R2, R3 ], ( R3, R4 ] …… ( Rn-1, Rn ], ( Rn, supremum)\n如果根据记录R3往右分裂，分裂后的区间为：\nPAGE: ( infimum , R1 ], ( R1 , R2 ], ( R2, supremum)\nRIGHT PAGE: ( infimum, R3 ], ( R3, R4 ] …… ( Rn-1, Rn ], ( Rn, supremum)\n分裂完成后，需要将split_rec R3及之后的所有记录的锁都移动到RIGHT PAGE，同时更新当前页的锁对象lock_rec_t，并产生RIGHT PAGE上的新的锁对象lock_rec_t。\n如果原来PAGE的supremum记录有锁（肯定是gap锁），则分裂后RIGHT PAGE的supremum记录也必须有锁，这是为了防止其他事务在这个范围进行修改操作，也就是步骤3的作用。\n在分裂时，如果原来PAGE的split_rec R3上有锁，即表示范围( R2, R3 ]的区间内不允许有其他事务进行修改操作。那么在分裂后，上述的区间分裂为两个区间： ( R2, supremum ) 和 ( infimum, R3 ]，因此分裂完成后还需要对 ( R2, supremum )这个区间加锁，这就是步骤4的作用。\n往左分裂（lock_update_split_left）和往右分裂大致相同，但是不需要步骤3，这是因为在往左分裂时，supremum记录的锁是稳定的。如下图所示：\n5.6 页的合并 #  和页的分裂一样，页的合并（merge）操作也可以分为往右和往左两种，也同样需要对行锁信息进行维护。\n往左合并的步骤（lock_update_merge_left）如下：\n 记录LEFT PAGE合并前最大的用户记录orig_pred 将RIGHT PAGE中的用户记录复制到LEFT PAGE，同时更新对应的行锁信息（page_copy_rec_list_start） 将LEFT PAGE的supremum记录上的锁继承给记录orig_pred指向的下一条记录（lock_rec_inherit_to_gap） 将RIGHT PAGE的supremum记录上的锁移动到LEFT PAGE的supremum记录上（lock_rec_move）  如下图所示：\n往右合并（lock_update_merge_right）和往左合并大致相同，但是不需要步骤4，这是因为在往右合并时，supremum记录的锁是稳定的。如下图所示：\n6. 死锁 #  6.1 死锁的概念 #  死锁是指两个或两个以上的事务在执行过程中，因争夺锁资源而造成的一种互相等待的现象。如果没有外力的作用，这些事务都将无法推进下去。\n解决死锁的方法有三种：\n 不要有等待，任何等待转化为回滚，并重新开始事务。但这会导致性能的下降，极端情况下任何事务都无法推进下去，而这所带来的问题远比死锁更为严重，因为这样很难被发现并且浪费资源。 超时。当多个事务互相等待时，设置一个锁超时的时间，由用户决定是回滚还是继续推进直至提交。 死锁检测。  死锁检测比超时更为主动。DBMS普遍采用等待图（wait-for graph）的方式进行死锁检测，这就要求在数据库中保存以下两种信息：\n 锁的信息链表 事务等待链表  通过上述链表可以构造出一张图，如果是一个有向无环图（DAG - Directed Acyclic Graph），则表示不存在死锁；如果在图中存在环（回路），那么就表示存在死锁。在图中，事务为节点，事务T1指向T2的边的定义为：\n 事务T1等待事务T2所占用的资源 事务T1最终等待T2所占用的资源，也就是事务之间在等待相同的资源，而事务T1发生在事务T2的后面  下面来看一个例子，当前事务和锁的状态如下图所示：\n从上图中可以看到，在事务等待列表中共有T1、T2、T3、T4 4个事务，所以在wait-for graph中有4个节点。持有锁的情况：T2对row1持有X-lock，T1对row2持有S-lock，T4对row2持有S-lock。而有三个锁等待，冲突的锁等待的关系表示为边（等待→持有，等待→ 等待），这样一共有6条边。可以看出T2→T4→T3形成了环，出现了死锁。\n6.2 死锁概率 #  死锁应该极少发生，如果经常发生，则系统会处于不可用的状态。另外，死锁还应该少于等待，因为至少2次等待才会产生一次死锁。\n下面将从数学的概率来分析，死锁的发生概率非常小。\n假设数据库中一共有n+1个线程执行，即一共有n+1个事务。假设每个事务所作的操作相同，每个事务由r+1个操作组成，每个操作从R行数据中随机操作一行数据，并持有相应的行锁。事务在执行完最后一步后释放所占用的所有锁资源。假设nr \u0026laquo; R，即所有事物操作的数据只占所有数据的一小部分。\n在上述模型中，事务获得\n6.3 死锁检测 #  6.3.1 死锁检测的场景 #  前面提到，出现等待是死锁的先决条件。所以，在InnoDB中，当表锁或者行锁需要等待时，会进行死锁检测，函数调用链如下：\nRecLock::add_to_waitq RecLock::deadlock_check DeadlockChecker::check_and_resolve lock_table lock_table_enqueue_waiting DeadlockChecker::check_and_resolve 我们下面以行锁为例，分析死锁检测的流程。\n6.3.2 死锁检测流程 #  当发现有冲突的行锁时，就需要把冲突的行锁放入事务的等待队列中（RecLock::add_to_waitq）。在实际放入等待队列之前：\n 如果持有冲突锁的线程是MySQL内部的后台线程，则不会被高优先级事务取消。这是为了优先保证内部线程正常运行。 比较当前会话和持有锁的会话的事务优先级，调用函数trx_arbitrate 返回被选作牺牲者的事务；  当前发起请求的会话是后台线程，但持有锁的会话设置了高优先级时，选择当前线程作为牺牲者； 持有锁的线程为后台线程时，在第一步已经判断了，不会选作牺牲者； 如果两个会话都设置了优先级，低优先级的被选做牺牲者，优先级相同时，请求者被选做牺牲者(thd_tx_arbitrate)；   如果当前会话的优先级较低，或者另外一个持有锁的会话为后台线程，这时候如果当前会话设置了优先级，直接报错，并返回错误码DB_DEADLOCK；  默认不设置优先级时，请求锁的会话也会被选作victim_trx，但只创建锁等待对象，不会直接返回错误；   当持有锁的会话被选作牺牲者时，说明当前会话肯定设置了高优先级，这时候会走RecLock::enqueue_priority的逻辑；  如果持有锁的会话在等待另外一个不同的锁时，或者持有锁的事务不是readonly的，当前会话会被回滚掉； 开始跳队列，直到当前会话满足加锁条件（RecLock::jump_queue）；  请求的锁对象跳过阻塞它的锁对象，直接操作hash链表，将锁对象往前挪； 从当前lock，向前遍历链表，逐个判断是否有别的会话持有了相同记录上的锁（RecLock::is_on_row），并将这些会话标记为回滚（mark_trx_for_rollback）,同时将这些事务对象搜集下来，以待后续处理（但直接阻塞当前会话的事务会被立刻回滚掉）；   高优先级的会话非常具有杀伤力，其他低优先级会话即使拿到了锁，也会被它所干掉。    到目前为止，MySQL 5.7还不支持用户端设置线程优先级，所以，目前所有用户线程的事务优先级都是一样的。\nInnoDB的死锁检测算法是深度优先搜索wait-for graph，如果在搜索过程中发现有环，就说明发生了死锁。同时，为了避免死锁检测的开销过大，如果搜索深度超过了200（LOCK_MAX_DEPTH_IN_DEADLOCK_CHECK），也同样认为发生了死锁（victim是发起者）。\nInnoDB最早使用的是递归方式搜索，之后为了减少栈空间的开销，改为使用入栈的方式。\n死锁检测相关的数据结构：\n/** Deadlock checker. */ class DeadlockChecker { /** DFS state information, used during deadlock checking. */ struct state_t { // 栈中的元素，即状态 const lock_t* m_lock; /*!\u0026lt; Current lock */ const lock_t* m_wait_lock; /*!\u0026lt; Waiting for lock */ ulint m_heap_no; // 如果为行锁，则行锁对应的记录heap no }; /** Used in deadlock tracking. Protected by lock_sys-\u0026gt;mutex. */ static ib_uint64_t s_lock_mark_counter; 局部的死锁计数器，在当前发起死锁检测时设置 ulint m_cost; // 遍历过的节点计数 const trx_t* m_start; // 发起者 bool m_too_deep; // 是否深度遍历达到最大深度 const lock_t* m_wait_lock; // ctx等待的锁（第一次是发起者申请的锁） /** Value of lock_mark_count at the start of the deadlock check. */ ib_uint64_t m_mark_start; // 局部的死锁计数器，在当前发起死锁检测时设置 size_t m_n_elems; // state_t入栈次数 static state_t s_states[MAX_STACK_SIZE]; // 预分配的栈结构，避免malloc/free开销 } DeadlockChecker.state_t就是辅助的栈结构，分配了s_states[MAX_STACK_SIZE]个（4096）。\nDeadlockChecker.m_start是第一个请求锁的事务，即死锁检测的发起者，如果深度搜索的过程中所对应的事务等于m_start，那么就说明产生了环。m_wait_lock表示搜索中的事务等待的锁。\n我们在这里举个例子，如下图所示：\n事务A、B、C各自已经获得了记录1、2、3上的X锁。事务A对记录2的X锁请求因为B而等待，同样，事务B对记录3的X锁请求因为C而等待。当事务C对记录1的发起X锁请求时，发生死锁：\n m_start初始化为C，m_wait_lock初始化为C.X1（表示数据1上的X锁）（都为死锁检测的发起者，以及发起者相应需要等待的锁） 根据m_wait_lock = C.X1，拿到数据1上的第一个锁lock（DeadlockChecker::get_first_lock）：事务A锁持有的X1锁 A.X1 判断lock对应的事务（A）是否也在等待其它锁（lock-\u0026gt;trx-\u0026gt;lock.que_state == TRX_QUE_LOCK_WAIT）。因为事务A确实在等待X2锁，把当前的lock入栈（state.lock = A.X1，state.m_wait_lock = C.X1） ctx中的m_wait_lock更新为lock-\u0026gt;trx-\u0026gt;lock.wait_lock, 也就是 X1 锁的持有者事务 A 所等待的锁 X2（A.X2） 同步骤2，根据m_wait_lock=A.X2, 拿到加在数据2上的第一个锁赋值给lock，也就是事务B持有的X2锁（B.X2）。此时完成一次循环。 再次进入循环，lock 对应的事务（B）同样在等待其它锁（state.lock = B.X2，state.m_wait_lock = A.X2），所以把当前的 lock 入栈。 ctx中的m_wait_lock 更新为lock-\u0026gt;trx-\u0026gt;lock.wait_lock, 也就是 X2 锁持有者事务 B 所等待的锁 X3（B.X3） 同步骤 2，根据m_wait_lock=B.X3, 拿到加在数据3上的第一个锁赋值给lock，也就是事务C持有的X3锁（C.X3）。此时完成一次循环。 再次进入循环，此时 lock-\u0026gt;trx = C = ctx-\u0026gt;start。死锁形成。  具体流程如下图所示：\n具体的程序代码如下：\nDeadlockChecker::check_and_resolve(const lock_t* lock, trx_t* trx) { const trx_t* victim_trx; /* Try and resolve as many deadlocks as possible. */ do { DeadlockChecker checker(trx, lock, s_lock_mark_counter); // new死锁检测器，这里只设置发起者和发起者申请的锁  victim_trx = checker.search(); // 进行死锁检测  /* Search too deep, we rollback the joining transaction only if it is possible to rollback. Otherwise we rollback the transaction that is holding the lock that the joining transaction wants. */ if (checker.is_too_deep()) { // 如果深度搜索超过200，则认定victim为发起者，进行事务回滚  rollback_print(victim_trx, lock); break; } else if (victim_trx != NULL \u0026amp;\u0026amp; victim_trx != trx) { // 如果选出的victim不是当前事务  ut_ad(victim_trx == checker.m_wait_lock-\u0026gt;trx); // 确认成环  checker.trx_rollback(); // 发起事务回滚  lock_deadlock_found = true; } } while (victim_trx != NULL \u0026amp;\u0026amp; victim_trx != trx); /* If the joining transaction was selected as the victim. */ if (victim_trx != NULL) { // 发起者为victim，上报错误，进行事务回滚  print(\u0026#34;*** WE ROLL BACK TRANSACTION (2)\\n\u0026#34;); lock_deadlock_found = true; } trx_mutex_enter(trx); return(victim_trx); } const trx_t* DeadlockChecker::search() { /* Look at the locks ahead of wait_lock in the lock queue. */ ulint heap_no; const lock_t* lock = get_first_lock(\u0026amp;heap_no); // 从发起者的申请锁（m_wait_lock），拿到该申请锁所在的page上的第一个已持有锁  for (;;) { /* We should never visit the same sub-tree more than once. */ ut_ad(lock == NULL || !is_visited(lock)); while (m_n_elems \u0026gt; 0 \u0026amp;\u0026amp; lock == NULL) { // 出栈操作  /* Restore previous search state. */ pop(lock, heap_no); lock = get_next_lock(lock, heap_no); } if (lock == NULL) { // 没有锁需要遍历了，结束死锁检测  break; } else if (lock == m_wait_lock) { // 遍历到的发起者的申请锁  /* We can mark this subtree as searched */ ut_ad(lock-\u0026gt;trx-\u0026gt;lock.deadlock_mark \u0026lt;= m_mark_start); lock-\u0026gt;trx-\u0026gt;lock.deadlock_mark = ++s_lock_mark_counter; /* We are not prepared for an overflow. This 64-bit counter should never wrap around. At 10^9 increments per second, it would take 10^3 years of uptime. */ ut_ad(s_lock_mark_counter \u0026gt; 0); /* Backtrack */ lock = NULL; } else if (!lock_has_to_wait(m_wait_lock, lock)) { // m_wait_lock当前冲突的锁没有和page上有的已获取锁冲突，取page的下一个锁  /* No conflict, next lock */ lock = get_next_lock(lock, heap_no); } else if (lock-\u0026gt;trx == m_start) { // 发现环（发起者和节点一致）  /* Found a cycle. */ notify(lock); return(select_victim()); // 选出victim  } else if (is_too_deep()) { // 遍历深度达到max，将发起者设为victim，退出死锁检测  m_too_deep = true; return(m_start); } else if (lock-\u0026gt;trx-\u0026gt;lock.que_state == TRX_QUE_LOCK_WAIT) { // 如果遍历到的持有的锁对应的事务也有等待的锁，则压栈  /* Another trx ahead has requested a lock in an incompatible mode, and is itself waiting for a lock. */ ++m_cost; // 遍历节点++  if (!push(lock, heap_no)) { // 压栈（等待的锁，冲突对应的持有的锁）  m_too_deep = true; // 如果预留的栈已用尽，将发起者设为victim，退出死锁检测  return(m_start); } m_wait_lock = lock-\u0026gt;trx-\u0026gt;lock.wait_lock; // 将冲突对应的持有的锁的事务等待锁设为ctx.m_wait_lock  lock = get_first_lock(\u0026amp;heap_no); // 获取ctx.m_wait_lock对应的page上的第一个已持有锁  if (is_visited(lock)) { // 如果已访问过该锁，获取该page的下一个已持有锁  lock = get_next_lock(lock, heap_no); } } else { lock = get_next_lock(lock, heap_no); // page上有多个锁，取下一个锁  } } /* No deadlock found. */ return(0); } 6.3.3 选出victim #  当发生死锁后，会选择一个代价较少的事务进行回滚操作（select_victim），从发起者（ctx→m_start）和形成环的最后一条边（ctx→m_wait_lock）中二选一。\n权重的比较（trx_weight_ge）：\n 修改了不支持事务的表为高 持有的undo数量+持有的锁之和大者为高  Tips\n如果业务经过精心设计，是可以从业务上避免死锁的。并且死锁检测本身会持有大锁（log_sys→mutex），代价非常高昂。在高并发的场景下对热点行的更新往往会造成性能雪崩。\n为了避免热点行的死锁检测造成性能雪崩，阿里RDS做过一个feature，在server层对热点行的更新进行排队，以减缓死锁检测带来的性能影响。\n另外，在阿里内部的应用中，因为有专业的团队来保证业务SQL的质量，我们可以选择性的禁止死锁检测来提升性能，尤其是在热点更新场景，带来的性能提升非常明显，极端高并发下，甚至能带来数倍的提升。\n 6.3.4 锁等待的错误码处理 #  当无法获得锁时，会将相应的错误码传到上层进行处理（row_mysql_handle_errors）：\nDB_LOCK_WAIT\n 具有高优先级的事务已经搜集了会阻塞它的事务链表，这时候会统一将这些事务回滚掉（trx_kill_blocking） 将当前的线程挂起（lock_wait_suspend_thread），等待超时时间取决于session级别配置（innodb_lock_wait_timeout），默认50秒 如果当前会话的状态设置为running，一种是被选做死锁检测的牺牲者，需要回滚当前事务，另外一种是在进入等待前已经获得了事务锁，也无需等待 获得等待队列的一个空闲slot。（lock_wait_table_reserve_slot）  系统启动时，已经创建好了足够用的slot数组（类型为srv_slot_t），挂在lock_sys-\u0026gt;waiting_threads上 分配slot时，从slot数组的第一个元素开始遍历，直到找到一个空闲的slot。注意这里存在性能问题：如果挂起的线程非常多，每个新加入挂起等待的线程都需要遍历直到找到一个空闲的slot。 实际上如果每次遍历都从上次分配的位置往后找，到达数组末尾在循环到数组头，这样可以在高并发高锁冲突场景下获得一定的性能提升   如果会话在innodb层（通常为true），则强制从InnoDB层退出，确保其不占用innodb_thread_concurrency的槽位。然后进入等待状态。被唤醒后，会再次强制进入InnoDB层 被唤醒后，释放slot（lock_wait_table_release_slot） 如果被选为victim，返回上层回滚事务；如果等待超时了，则根据参数innodb_rollback_on_timeout的配置，默认为OFF只回滚当前SQL，设置为ON表示回滚整个事务。  DB_DEADLOCK\n直接回滚当前事务\n6.4 死锁的例子 #  两个事务就可以出现死锁，即A等待B，B也在等待A，这种死锁称为AB-BA死锁。\n比如：\n   session A session B     begin;select * from t where a = 1 for update;     begin;select * from t where a = 2 for update;   select * from t where a = 2 for update;等待\u0026hellip;.     select * from t where a = 1 for update;ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction    又比如：\ncreate table t ( a int not null primary key ) engine = InnoDB; insert into t values (1), (2), (4), (5);    session A session B     begin;select * from t where a = 4 for update;     begin;select * from t where a = 4 lock in share mode;   insert into t values (3);ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction     首先session A持有记录4的x-lock，接着session B尝试获取记录4的s-lock，但出现等待，当session 4尝试插入记录3，即要求插入的下一条记录不能含有任何锁，包括gap锁或处于等待的锁，而出现AB-BA死锁。\n为何"},{"id":30,"href":"/docs/MySQL/InnoDB/1_overview/","title":"overview","section":"Inno Db","content":"Heikki Tuuri是InnoDB存储引擎的创始人，1964年生于芬兰赫尔辛基。与著名Linux操作系统的创始人Linus一样毕业于芬兰赫尔辛基大学。从入学时间来看，Heikki Tuuri还是Linus的学长。在1990年取得赫尔辛基大学的数理逻辑博士学位后。\n所以，innodb的代码大部分都是Created mm/dd/YYYY Heikki Tuuri。\nMySQL大事时间表：\n 1995：Heikki Tuuri成立Innobase Oy公司并担任CEO。同年，由David Axmark、Allan Larsson和Michael Monty Widenius在瑞典创办MySQL AB公司。 2001：Innobase公司开始与MySQL AB公司进行合作并开源InnoDB存储引擎的代码。 2005：Oracle公司收购了Innobase公司。 2008：Sun收购MySQL AB公司。 2009：2009年4月20日，Sun 公司董事会通过决议，同意以每股9.5美元的价格将公司出售给Oracle。  "},{"id":31,"href":"/docs/MySQL/InnoDB/4_page/","title":"page","section":"Inno Db","content":"InnoDB使用的是索引组织表，因此聚簇索引的叶子节点中存放完整的数据记录，辅助索引页的叶子节点中存放指向聚簇索引页叶子节点的书签（bookmark），也可以称为路标。\n主要是两部分：\n page layout scan rec with cursor, and then insert, update, delete  页 #  页是InnoDB存储数据的基本单位。页的大小可以设置为4K~64K（4K的倍数），默认为16K。页的大小设置要从IO性能考虑。\n/* Define the Min, Max, Default page sizes. */ /** Minimum Page Size Shift (power of 2) */ #define UNIV_PAGE_SIZE_SHIFT_MIN 12 /** Maximum Page Size Shift (power of 2) */ #define UNIV_PAGE_SIZE_SHIFT_MAX 16 /** Default Page Size Shift (power of 2) */ #define UNIV_PAGE_SIZE_SHIFT_DEF 14 static MYSQL_SYSVAR_ULONG(page_size, srv_page_size, PLUGIN_VAR_OPCMDARG | PLUGIN_VAR_READONLY, \u0026quot;Page size to use for all InnoDB tablespaces.\u0026quot;, NULL, NULL, UNIV_PAGE_SIZE_DEF, UNIV_PAGE_SIZE_MIN, UNIV_PAGE_SIZE_MAX, 0); /** The universal page size of the database */ #define UNIV_PAGE_SIZE ((ulint) srv_page_size) 页和记录一样，也存在两种形态：物理页和逻辑页。物理页存储在外部的存储设备上，逻辑页存在于缓冲池中。一般，用block表示物理页，用page表示逻辑页（注意这里的block和page并不是指的buf_block_t、buf_page_t）。\n缓冲池中的逻辑页在进行读取操作时首先分配一个PCB，并将其FIX，然后将磁盘上的物理页读入内存中，形成逻辑页，然后对逻辑页进行操作（读取或者修改）；修改后，再异步刷入磁盘。这意味着，在某一段时间窗口内物理页和逻辑页可能是不一致的，最新修改的数据在内存中，而不一定体现在磁盘上。而通过事务的ACID特性，可以保证最终某一时间点两者的数据会达到最终一致。\npage layout #  在InnoDB中，页根据用途分为多种类型。而使用最多的就是索引页，所以在这里我们focus在index page。\n索引页（index page）的页面布局如下：\n从这里我们可以看出，index page包含3部分信息：\n （表）空间相关的信息：FIL_HEADER、FIL_TRAIL 页相关的信息：PAGE_HEADER、PAGE_DIRECTORY 数据：记录  我们下面逐个拆解：PAGE_HEADER、PAGE_DIRECTORY和记录的组织。\nFIL_HEADER、FIL_TRAIL存储的是表空间相关的信息，在此不展开，具体信息参见storage management一节。  page header #  page header用于保存页的信息，占用56个字节，位于file header之后：\npage header中存放的是页中数据存储的信息：页维度的存储空间、使用和事务信息。\npage header中的字段含义如下：\n   名称 大小 说明     名称 大小 说明   PAGE_N_DIR_SLOTS 2 page directory中槽的数量   PAGE_HEAP_TOP 2 堆顶，堆中空闲空间的位置（offset），空闲区   PAGE_N_HEAP 2 page中的heap no（单调递增），最高位（第1个bit）表示行格式是new style还是old style   PAGE_FREE 2 指向页中空闲空间的位置（offset），碎片区，放的都是被purge线程回收的记录   PAGE_GARBAGE 2 已删除记录的字节数，即行记录中delete flag=1的记录的总量   PAGE_LAST_INSERT 2 最后插入记录的位置（offset）   PAGE_DIRECTION 2 最后插入记录的方向 PAGE_LEFT 0x01 PAGE_RIGHT 0x02 PAGE_SAME_REC 0x03 PAGE_SAME_PAGE 0x04 PAGE_NO_DIRECTION 0x05   PAGE_N_DIRECTION 2 一个方向上连续插入记录的数量   PAGE_N_RECS 2 页中用户记录的数量   PAGE_MAX_TRX_ID 8 修改当前页的TRX ID，仅在辅助索引和change buffer中使用   PAGE_LEVEL 2 当前页在索引树中的位置，0x00代表叶子节点，即叶子节点总是在第0层（不变性），page_is_leaf   PAGE_INDEX_ID 8 索引ID，标识当前页属于哪个索引   PAGE_BTR_SEG_LEAF 10 B+树叶子节点所在段的segment header，仅在B+树的root页中定义   PAGE_BTR_SEG_TOP 10 B+树非叶子节点所在段的segment header，仅在B+树的root页中定义    PAGE_MAX_TRX_ID及之前的所有字段在页创建的时候时设置，其后的字段在填充具体数据时才设置。\npage directory #  page header中保存了页中记录的存储信息，如果要进行记录的查询，则需要通过page directory。并且，B+树的查找（node pointer.page_no）只能定位到记录所在的页，精确定位到记录（记录在页中的位置）则需要通过page directory。\n这种数据结构可以保证比较高的插入删除和查找效率：是一种经典的读写分离模型：\npage directory由槽（slot）组成，每个槽占用2个字节，指向记录在页中的偏移量。槽根据指向记录的主键顺序逆序存放，可以通过二叉查找算法快速定位到查询的记录。但是，为了提高存储以及插入的效率，InnoDB对于槽的设计采用了稀疏（sparse）方式，槽和记录不是1:1的，而是1:n的。每个槽对应一个记录，这个记录中记录n，即用n_owned属性记录所对应的槽中拥有的记录数量。槽所指向的记录被称为owner record，即从后往前找的n_owned不为0的那个record，表示其前面n个记录归其index。\n insert：heap no++, owned++, （optional） 第8个split page directory slot delete：link摘除, owned\u0026ndash;, （optional） \u0026lt;=3, merge page directory slot owned record, special  Because the overhead of inserts is so small, we may also increase the page size from the projected default of 8 kB to 64 kB without too much loss of efficiency in inserts. Bigger page becomes actual when the disk transfer rate compared to seek and latency time rises. On the present system, the page size is set so that the page transfer time (3 ms) is 20 % of the disk random access time (15 ms).  除了首尾两个槽外，每个槽总是包含4~8条记录（PAGE_DIR_SLOT_MIN_N_OWNED ~ PAGE_DIR_SLOT_MAX_N_OWNED），第一个槽（即infimum记录）仅包含一个记录（因为要临时保存锁信息，所以只描述自己），最后一个槽（即supremum记录）可以包含1~8个记录。\nAssuming a page size of 8 kB, a typical index page of a secondary index contains 300 index entries, and the size of the page directory is 50 x 4 bytes = 200 bytes.  page_dir_split_slot和page_dir_balance_slot用来保证每个槽所包含的记录在上述的定义范围内。如果超出范围，则调用page_dir_split_slot来产生新的slot，如果低于这个范围，则调用page_dir_balance_slot来平衡slot中的记录，可能删除slot，或者对相邻的slot进行merge。\n在查找记录时，如果是通过记录找下一条记录，可以直接根据next_record得出。否则，需要先定位记录所在的slot（从supremum slot开始从后往前查找slot，直至找到记录相应的slot），然后在两个slot之间在查找记录。\nrecord organization #  在record中讲过，每个页都有2个伪记录，如图所示。在infimum记录和supremum记录之间的被称为用户记录（page_rec_is_user_rec_low）。因此在page header中用PAGE_N_HEAP和PAGE_N_RECS标识堆和页中的记录数量，PAGE_N_RECS表示一页上真实的记录数，PAGE_N_HEAP表示从HEAP上分配出去的记录数。\n在old-style和new-style行格式下，infimum和supremum记录稍有区别，如下图所示：\n在PAGE_HEADER中：\n 如果PAGE_N_RECS = 0，则认为是空页（page_is_empty） PAGE_GARBAGE != 0，则认为页中有碎片区，可以回收（page_has_garbage）  页中的可用空间可以分为两种：\n 空闲区：一块连续的未使用空间，用PAGE_HEAP_TOP表示 碎片区：已删除的记录构成的不连续的回收空间，用PAGE_FREE（指明位置，链表）和PAGE_GARBAGE（指明可用字节）表示  当记录被删除时，将其空间放入可用回收空间的首部，即PAGE_FREE指向最近删除的记录空间。接着通过记录中record header中的next record可以得到后续的已删除记录，这样就构成了一个可用回收空间链表。这个可用空间的大小用PAGE_GARBAGE表示。\n当记录在页中申请空间时，首先尝试使用碎片区（PAGE_FREE），空间不足再尝试从堆顶的空闲区（PAGE_HEAP_TOP）中分配。\n对于碎片区的空间检查，InnoDB只检查第一个可重用空间，而不是遍历整个可用回收空间链表。比如可用空间链表为100 -\u0026gt; 400 -\u0026gt; 300，当要插入的记录大于100字节时，就不会重用碎片区。这是因为InnoDB具有页整理（re-organize）的功能，即在页空间不足时会首先对页进行重新组织（btr_page_reorganize_low），按照页中的记录主键顺序重新整理，回收碎片空间。\nOn top of it are the index records in a heap linked into a one way linear list according to alphabetic order.  页中的记录按照索引键顺序排列的，这个排序是逻辑的，不是物理的，完全按照物理排序的代价非常巨大。因此，页只是一个存储记录的堆，即记录是按照堆号逻辑排序的。页中的记录的实际组织形式由record.next_record串联起来，如下图所示：\nPAGE_LAST_INSERT、PAGE_DIRECTION、PAGE_N_DIRECTION用于页的分裂（split）操作。在传统的B+树中，分裂操作都是向左进行的。但是InnoDB会根据以上三个值来判断插入的方向，是顺序升序插入、顺序降序插入、还是无序的随机插入，而采用不同的分裂策略。\n为了获得更好的顺序存储性，InnoDB将叶子节点（leaf page）和非叶子节点（non leaf page，不包含root page）的数据通过聚簇的方式存放到两个不同的segment中。\nMySQL 3.23.49中使用page_template作为初始化页的模板供page_create调用：当该模板初始化完成后，之后对于页的初始化仅需要复制此模板即可。后续的版本废弃了这种做法，将需要初始化的静态部分声明为字符串常量。  page_move_rec_list_start（移动一个）和page_move_rec_list_end（移动一个及以后所有）用于将页中的记录迁移到新的页中。在记录的页迁移过程中，需要更新锁、page header中的PAGE_MAX_TRX_ID以及对应的自适应哈希索引。这两个函数主要用于B+树的分裂。\n下面我们结合一个具体的例子来看一下InnoDB是如何保存page header、page directory和实际的record。\ncreate table t ( a int not null auto_increment primary key, b char(3) default null ) enngine = InnoDB, row_format = dynamic; insert into t (b) values ('aaa'); insert into t (b) values ('bbb'); insert into t (b) values ('ccc'); insert into t (b) values ('ddd'); insert into t (b) values ('eee'); insert into t (b) values ('fff'); insert into t (b) values ('ggg'); insert into t (b) values ('hhh'); insert into t (b) values ('iii'); insert into t (b) values ('jjj'); 整理出page header如下：\nPAGE_N_DIR_SLOTS 00 03 3 PAGE_HEAP_TOP 01 86 PAGE_N_HEAP 80 0c 32780 PAGE_FREE 00 00 PAGE_GARBAGE 00 00 PAGE_LAST_INSERT 01 72 PAGE_DIRECTION 00 02 PAGE_RIGHT PAGE_N_DIRECTION 00 09 9 PAGE_N_RECS 00 0a 11 PAGE_MAX_TRX_ID 00 00 00 00 00 00 00 00 PAGE_LEVEL 00 00 PAGE_INDEX_ID 00 00 00 00 00 00 00 2c PAGE_BTR_SEG_LEAF 00 00 00 1a 00 00 00 02 00 f2 PAGE_BTR_SEG_TOP 00 00 00 1a 00 00 00 02 00|32 接着后面就是插入的10条用户记录\n | |extra info | ROW ID | Trx ID | undo pointer | 数据 |01 00 02 00 1c| | | |69 6e 66 69 6d 75 6d 00 |07 00 0b 00 00| | | |73 75 70 72 65 6d 75 6d 03|00|00 00 10 00 1b|80 00 00 01|00 00 00 00 05 33|a6 00 00 01 1a 01 10|61 61 61 03 00 00 00 18 00 1b 80 00 00 02 00 00 00 00 05 34 a7 00 00 01 1b 01 10 62 62 62 03 00 00 00 20 00 1b 80 00 00 03 00 00 00 00 05 38 aa 00 00 01 1e 01 10 63 63 63 03 00 04 00 28 00 1b 80 00 00 04 00 00 00 00 05 3a ab 00 00 01 1f 01 10 64 64 64 03 00 00 00 30 00 1b 80 00 00 05 00 00 00 00 05 3b ac 00 00 01 20 01 10 65 65 65 03 00 00 00 38 00 1b 80 00 00 06 00 00 00 00 05 3c ad 00 00 01 21 01 10 66 66 66 03 00 00 00 40 00 1b 80 00 00 07 00 00 00 00 05 3d ae 00 00 01 22 01 10 67 67 67 03 00 00 00 48 00 1b 80 00 00 08 00 00 00 00 05 3e af 00 00 01 23 01 10 68 68 68 03 00 00 00 50 00 1b 80 00 00 09 00 00 00 00 05 3f b0 00 00 01 24 01 10 69 69 69 03 00 00 00 58 fe fe 80 00 00 0a 00 00 00 00 05 40 b1 00 00 01 25 01 10 6a 6a 6a 该页上所有记录extra info的heap_no、n_owned和record type信息：\nheap_no n_owned record type 0 1 010 infimum record 1 7 011 supremum record 2 0 000 conventional record 3 0 000 conventional record 4 0 000 conventional record 5 4 000 conventional record 6 0 000 conventional record 7 0 000 conventional record 8 0 000 conventional record 9 0 000 conventional record 10 0 000 conventional record 11 0 000 conventional record 从上面可以看出，一共有3个槽位（PAGE_N_DIR_SLOTS = 3），infimum记录的n_owned是1，第4个用户记录的n_owned是4，supremum记录的n_owned是7。\n页面的最后部分是page directory + fil trailer（8个字节）：\nPage Directory 00 70 00 d0 00 63 Fil Trailer 84 a2 0d 2f 00 14 b1 6f fil trailer之前就是page directory，每个槽位占2个字节，这里可以看出一共有3个槽位，并且槽是逆序存放的，因此，整理后（正向排序）的page directory为：\n(00 63), (00 d0), (00 70) 这里的槽位表示记录在页中的偏移量，比如通过（00 d0）可以直接定位到row id为（80 00 00 04）的记录。此外，infimum和supremum记录的位置不会改变，相对应的槽的位置总是（00 63）和（00 70），如下图所示：\n数据页的完整性 #  数据也的完整性通过checksum保证，在InnoDB中，checksum值的计算方法通过innodb_checksum_algorithm配置。\n目前提供三种计算checksum的方法：\n crc校验：这种是一种比较新的计算方法，可以使用cpu硬件指令来加速（buf_calc_page_crc32） InnoDB校验：InnoDB自己开发的一种计算方法，有新老两种变体 none模式：使用一个指定的值填充checksum字段（等于没有\u0026hellip;）  另外还有strict前缀的选项，这表示在读取数据页时必须通过指定的校验方式的校验值。提供strict选项是为了兼容老版本的MySQL，防止校验算法被修改而导致的数据不可用。\npage cursor #  上面介绍了page中的数据组织，接下来介绍如何定位记录和对记录进行变更。\n定位记录 #  page cursor更为准确的说是index page cursor（索引页的游标），即一个用来指向记录所在位置的游标，包括三元组：所属的索引（index）、定位的page（block+offsets）、记录（rec），定义如下：\n/** Index page cursor */ struct page_cur_t{ const dict_index_t* index; rec_t* rec; /*!\u0026lt; pointer to a record on page */ ulint* offsets; buf_block_t* block; /*!\u0026lt; pointer to the block containing rec */ }; 在InnoDB中有btr cursor（B-tree cursor）和page cursor和rec，关系为btr cursor→page cursor→record，即搜索路径为：\n B+树层的搜索 page层的搜索 在page中定位到record  B-tree cursor在index一章介绍。\npage cursor用来在page内定位（lookup）记录，内部通过查询模式进行向前或者向后的记录扫描（scan）。\n查询模式定义如下：\n/* Page cursor search modes; the values must be in this order! */ enum page_cur_mode_t { PAGE_CUR_UNSUPP = 0, PAGE_CUR_G = 1, // 大于 PAGE_CUR_GE = 2, // 大于等于 PAGE_CUR_L = 3, // 小于 PAGE_CUR_LE = 4, // 小于等于 /* PAGE_CUR_LE_OR_EXTENDS = 5,*/ /* This is a search mode used in \u0026quot;column LIKE 'abc%' ORDER BY column DESC\u0026quot;; we have to find strings which are \u0026lt;= 'abc' or which extend it */ /* These search mode is for search R-tree index. */ PAGE_CUR_CONTAIN = 7, PAGE_CUR_INTERSECT = 8, PAGE_CUR_WITHIN = 9, PAGE_CUR_DISJOINT = 10, PAGE_CUR_MBR_EQUAL = 11, PAGE_CUR_RTREE_INSERT = 12, PAGE_CUR_RTREE_LOCATE = 13, PAGE_CUR_RTREE_GET_FATHER = 14 }; 我们这里只关注G\\GE\\L\\LE。\n查找的流程如下：\n 通过二分查找定位记录所在的槽 通过顺序查找扫描槽中的记录 定位记录  结合查询模式，具体的查询如下图所示：\n比如，对于一个主键的范围查询，首先需要定位第一个记录，然后进行记录的顺序扫描。然而，根据不同的查询模式所定位的记录可能完全不同，例如：\n1, 2, 2, 3, 3, 3, 4, 5, 5, 6, 7 如果查找=3, ≥3的记录，那么page cursor应该定位到第一个记录为3的位置。如果查找\u0026gt;3的记录，则应定位记录为4的位置（前2者都是从左往右）。如果查找\u0026lt;3，则定位到第二个记录为2的位置。如果查找≤3，则定位到最后一个记录为3的位置（后2者从右往左）。\npage_cur_search_with_match通过查询的记录tuple来定位页中的记录，并返回page cursor来标定位置。与此同时，4个变量ilow_matched_fields、ilow_matched_bytes、iup_matched_fields、iup_matched_bytes用来返回使用二叉查找算法进行记录比较时，左右各已经匹配的字段数量和字节数。\n查询记录与页中记录的比较通过函数cmp_dtuple_rec_with_match_low进行。\n下面来看一个查询≥3或者\u0026lt;3的记录的实例：\ncmp_dtuple_rec_with_match_low返回的4个变量为iup_matched_fields = 0、iup_matched_bytes = 3、ilow_matched_fields = 1、ilow_matched_bytes = 0：\n    匹配的字段（fields） 部分匹配的字节数（bytes）     low 0 3   up 1 0    在上面的例子中，比较的记录列的数量为1，如果记录由（a, b, c）3列组成，查询根据（a,b）定位，情况如何呢？\ncmp_dtuple_rec_with_match_low返回的4个变量为iup_matched_fields = 2、iup_matched_bytes = 0、ilow_matched_fields = 0、ilow_matched_bytes = 3。\n    匹配的字段（fields） 部分匹配的字节数（bytes）     low 2 0   up 0 3    插入记录 #  page_cur_insert_rec_low用来插入记录，记录可以是物理记录或者逻辑记录，一般是逻辑记录。变量tuple为要插入的逻辑记录，插入时首先需要将tuple转化为物理记录，cursor指向插入记录之前的记录（一方面是next-key locking，一方面是保证加锁的有序）。\n举例来说，在记录集合（1，3，6，7）中插入一条新纪录5，首先需要定位待插入的位置（page_cur_search_with_match），查询模式为PAGE_CUR_LE，也就是定位到值为3的记录的位置，然后再进行插入。插入完成后需要更新row id 3的next record，然后将插入的row id 5的next record指向row id 6的位置，并返回新插入记录的位置（记录的original offset）。\n具体流程如下：\npage_cur_insert_rec_low\n 获得物理记录的长度 获取空闲空间，从碎片区或者从空闲区（heap）分配 将物理记录拷贝到空闲空间 修改记录的双向链表前后指针，和page header中的PAGE_N_RECS 设置record header（n_owned = 0, heap_no） 更新page header（PAGE_LAST_INSERT、PAGE_DIRECTION、PAGE_N_DIRECTION） 更新slot的n_owned 如果需要，分裂slot 写redo log  插入记录会对页进行修改，所以要产生redo log，redo log的类型是MTR_LOG_INSERT，在每次调用page_cur_insert_rec_low的同时，调用page_cur_insert_rec_write_log将变更写入redo log。MTR_LOG_INSERT的redo log日志格式如图所示：\n   字段 说明     type MTR_LOG_INSERT   space 记录插入到的表空间ID   page no 记录插入的索引页在表空间中的偏移量   cur_rec_offset 插入前，page cursor指向的行记录在页中的偏移量   lex \u0026amp; extra_info_flag 保存redo log body长度和记录的extra_info_flag   info_bits extra_info_flag=1时，保存插入记录的info_bits   original_offset extra_info_flag=1时，保存插入记录的original offset   mis_match_index extra_info_flag=1时，保存插入记录与cur_rec_offset所指向记录的第一个不同的字节（record header不用比较）   original rec body 插入记录的body    MTR_LOG_INSERTS格式说明：\n简单来看，MTR_LOG_INSERTS重做日志就是cursor定位的记录偏移量+插入的记录。为了使redo log尽可能的小（性能），对插入的记录进行优化，比较插入的记录和cursor定位的记录，找出第一个不同的字节位置。这里不需要对record header进行比较，因为record header是在记录插入完成后再进行初始化的。cursor定位的物理记录如图所示：\n上述的例子中cursor定位的记录为（1，\u0026lsquo;aaa\u0026rsquo;），那么当要插入的记录为（2，\u0026lsquo;bbb\u0026rsquo;）时，可以进行压缩。由于插入记录的长度与之前cursor定位的记录长度相同，因此不需要保存到重做日志。此外，列row id的前三个字节也都相同，都是80 00 00，因此同样不需要保存。故重做日志仅需保存插入记录的第13个字节之后（4+6+3）开始的内容。\n在重做日志中，extra_info_flag为1的判断依据为cursor定位的记录与插入记录的info_bits、extra_size、记录大小是否相等。其在源码中为：\nif ((rec_get_info_bits(insert_rec) != rec_get_info_bits(cursor_rec)) || (extra_size != cur_extra_size) || (rec_size != cur_rec_size)) { extra_info_yes = 1; } else { extra_info_yes = 0; } 记录插入通常是将逻辑记录插入到页中，但在函数page_copy_rec_list_*中，插入的记录是物理记录。这是因为这些操作是对页中已存在的记录进行\u0026quot;整理\u0026quot;。而在函数page_copy_rec_list_中将物理记录插入的重做日志定义为MTR_LOG_SHORT_INSERTS。这可以理解为一种较为“快速”的插入。其和MTR_LOG _INSERT不同之处在于不需要在重做日志中保存cursor指向记录的偏移量，因为这是将页中的所有记录插入到新页中的。\n在记录插入完成后，需要对page directory进行维护，看是否需要对槽进行平衡操作。\n删除记录 #  page_cur_delete_rec将cursor所指向的物理记录进行\u0026quot;彻底\u0026quot;地删除，而并不是将记录的delete flag设置为1。具体流程如下\n 记日志 PAGE_LAST_INSERT置空 找到前后记录 更新记录双向链表 更新page directory 更新n_owned 释放记录（PAGE_FREE+，PAGE_GARBAGE+，PAGE_N_RECS-） 如果slot的n_owned小于4，balance slot  相对于插入，删除记录的重做日志就显得简单多了，其只需额外地记录删除记录在页中的偏移量即可。重做日志的结构如图所示：\n并发控制 #  对于索引页的并发控制是在上层的调用中进行的，主要集中在btr模块中。在InnoDB中，btr模块负责对于B+树索引的控制，其中需要涉及索引页的并发控制以及锁信息的管理。\npage模块是对索引页进行操作的最底层函数。InnoDB本身不直接调用该模块中的函数。而且page模块操作页的数据结构为page_t，也就是字节。因此在该模块中并不涉及并发的控制，在源码中也看不到任何对于索引页加s-latch或者x-latch的过程。函数page_copy_rec_list_*中有需要对页上的锁信息进行更新，但这并不是对页进行并发保护。\n页面重组 #  我们知道，对于数据的插入，InnoDB只检查碎片链表的第一个可重用空间。另外，如果频繁的插入删除记录，则会加大碎片的产生，最终可能所有的碎片区空间加起来可以存储很多数据，但却无法将其使用起来。\n页面的碎片过多，会造成以下问题：\n 页面比较多，但实际用于索引的数据较少 大量的碎片导致需要读取更多的页面，产生大量的无效IO 数据库整体性能变差  页面重组（re-organize）的功能，即在页空间不足时会首先对页进行重新组织（btr_page_reorganize_low），按照页中的记录主键顺序重新整理，回收碎片空间，详细流程如下：\n 新建一个page 将碎片页面的数据一条一条的插入到新的page 将旧页面释放  这样新页面中的所有数据都是连续插入进去的，空间完全没有浪费，最后一条记录后面的剩余空间（可用空间）都在PAGE_HEAP_TOP下管理。\n"},{"id":32,"href":"/docs/MySQL/InnoDB/3_record/","title":"record","section":"Inno Db","content":"设计 #  行存 #  MySQL主要面向的是OLTP场景，所以InnoDB中存储的数据采用行存（NSM - n-ary storage model）。\n基于行进行存储有以下几个好处：\n 记录存放在一个页中，存储一条记录需要访问的页面较少 符合传统机械硬盘的访问方式 易于理解，数据的存取就像是对一张二维表进行访问  在整体上看，表中的数据是按照如下形式组织的：\n记录 #  那我们如何来理解记录呢？\n首先，在关系数据库系统理论中，通常用元组（tuple）描述记录，用字段（field）描述列，每个tuple由多个field组成，每个表由多个tuple组成。\n行和tuple在意义上是相等的。但是更愿意将行（row）理解为物理记录，将元组（tuple）理解为逻辑记录。物理记录为行实际存放在物理存储中的格式，其内容由二进制字符串组成，可读性差。逻辑记录则容易理解的多，每张表中的多条记录就像是一个数组。由于其只是“逻辑”上的含义，因此逻辑记录只是物理记录在内存中的表现形式，实际并不占用任何的物理存储空间。\n关系如下图所示：\n物理记录和逻辑记录的差异如下：\n    物理记录 逻辑记录     可读性 差 好   存储位置 磁盘 内存   亲和性 对存储友好（更紧凑） 对查找友好（更易寻址）   存储内容 记录中各个列的数据+一些额外信息 元组（用于比较、展现而组织的列数据）    这两种记录之间可以互相转换。比如，在插入一条记录时，原来没有数据，首先需要根据插入的记录构造一个逻辑记录，然后再转换成物理记录存放到磁盘上。对于读取，要从磁盘上seek出相应的数据页，再将页中的物理记录转换成逻辑记录展现给用户。\n除此之外，在MySQL server层需要在binlog中记录数据的变化，这也是一种行格式（RBR-logging）。因此，在MySQL中，行格式一共有3种存储方式：\n Server层格式：与存储引擎无关，server层的binlog行格式（Row-Base Replication下的binlog格式） 逻辑记录格式：tuple，也称为索引元组格式（因为InnoDB是IOT）。在同一个表中，不同索引对应的元组是不同的 物理记录格式：record，也称为physical record  物理记录的设计 #  物理记录承载着数据的最终存储，因此，我们首先讨论物理记录。\n磁盘上的物理记录需要面向计算机友好，更紧凑、强调IO性能，以及在此基础上支持事务语义。\n目标：\n 描述行存的数据 适配存储引擎的结构 查找快 DML快 事务语义 更少的资源占用（disk、buffer pool、update I/O）  列存采用directory+meta+data的形式组织：\n其中元信息（meta）按照以下维度的场景分类：\n从上图中，我们按照从大到小的维度罗列了需要用到的数据，可以看出，有些元信息存储在了列上，有些则存储在了行、页上。\n对于directory+meta+data来说，具体的行存的physical storage layout（这里以old style格式为例）：\n offset：变长，取决于列的数量（F），F1/F2，因此MySQL规定varchar的最大长度为65535（216=65536） extra info：定长，48 bit 数据：变长，取决于列的数量  列的表现形式：\n 物理：record 内存：tuple（没有事务信息，因为可以从物理中构建出来，并且集中存储在trx_sys中）  InnoDB在行的存储格式上进行过的优化：\n 存储更高效（节省20%）  NULL：not-NULL column list+ NULL bitmap（NULL char/varchar都不占用空间） extern：列长超过127才需要extern marker 无需n_fields，通过null bitmap+非空变长列表（2F）可以算出   workload如果在cache \u0026amp; disk IO上，则更高效，如果是CPU-bound，则会慢 查找 next record：绝对位置→相对位置（减少record re-org的开销） + record type 兼容性  版本标识+启动检查+参数控制   版本  数据页上进行标识    逻辑记录的设计 #  磁盘上的物理记录面向的是计算机友好的。同时，为了性能的考虑，数据也需要常驻内存（buffer pool），所以需要设计相应的内存态数据结构用于表述逻辑记录。\n并且，考虑到InnoDB使用的是IOT（B+树），还需要考虑数据在B+树的表现形式：\n 非叶子节点：node pointer，由（key, page no）组成 叶子节点：data，或（key, PK）  从这里看出，tuple根据不同的index，所存储的数据也各不相同。\n我们从磁盘上读取物理记录后，会将其转换为逻辑记录，用于比较。同时，用户的输入由于需要持久化，也需要转换为物理记录，并且，用户的输入有虚拟化（constant, as）的需求，需要额外的列来描述（虚拟化列）。\n因此，我们在tuple设计上，需要考虑：\n 可比较性 虚拟列 内存布局的友好性（layout、cacheline、大记录）  数据表示\u0026amp;比较 #  记录由列组成，列需要具有类型+属性。\n数据表示：\ndata representation\n integer/bigint/smallint/tinyint  C/C++ representation   float/real vs numeric/decimal  IEEE-754 standard / fixed-point decimals   varchar/varbinary/text/blob  header with length, followed by data types (may add checksum)   time/date/timestamp  32/64-bit integer of (micro) seconds since Unix epoch (most)    数据比较则需要考虑：\n 二进制数据（是否可以按内存序比较） 字符串（字符集/校对字符集 bin/ci/cs） NULL（minimum）：被视为最小值，任何与NULL进行的等值比较返回的值都是NULL  并发控制 #  并发控制有两个层次：\n 内存物理结构的一致 事务语义的一致  首先看一下内存物理结构的一致：在B+树中，access path是从树顶到叶子节点的，所以concurrency control只需要对B+树（index tree + index page）以及叶子节点（data page）进行latch并发控制（SMO \u0026amp; FIX）即可。对于存储在data page内部的行，无需关心latch并发控制。\n事务语义的一致，则是通过悲观（locking）或乐观（TSO、Multi-version）的方式保证的。在MySQL中，是通过MV2PL protocol的方式来实现的。MySQL为了控制锁资源，通过在锁对象内部的page bitmap的方式实现行锁以及type_mode实现谓词锁。\nMVCC #  根据不同的隔离级别，可以读到不同版本的数据，即通过readview确定可视范围（trxID），并提供版本管理（version storage：delta/append only）、GC。\n物理记录的实现 #  InnoDB中的表使用的是索引组织表（Index Organized Table - IOT），这意味着表中的所有数据是按照B+树的方式进行存储的，行数据存在在B+树的叶子节点上，即使创建表时没有显式指定主键索引，也会自动创建一个6字节的隐藏列，用作主键索引（用于unique）。\n此外，InnoDB为了支持事务和多版本并发控制（MVCC），所以每行记录还有一个回滚指针列和记录事务ID的列，这两列都是隐藏列，对用户不可见。回滚指针用来构造当前记录的上一个版本，实现事务回滚和MVCC，事务ID用于判断当前记录对于其他事务是否可见，用来实现事务的隔离性和MVCC。\n以上三个系统隐藏列仅存在于聚簇索引的物理记录中，逻辑记录不含有这些信息，逻辑记录本身只保存每个元组中的实际内容。\n物理记录存储在页中，但是每个页存放的行记录也是有硬性定义的，最多允许存放16KB/2-200行的记录，即7992行记录。每页最少、最多放多少行\n每个数据页中的行数可以通过以下方式计算：\nrow storage layout中的heap_no：13bit，即213=8192，那么16KB的页中每行最少用16*1024/8192 = 2 bytes 存储。\n每行最多有多少列 在实际的InnoDB工程实践中，为了存储格式的演进，需要对行格式进行修改。同时，考虑到兼容性（backward、foward），由此提出了named file format的概念。\nInnoDB 1.0.x之前，提供了Redundant和Compact两种格式来存储行数据：5.0之前的格式是Redundant，5.0引入了Compact，二者也被称为old style和new style，同时又统称为Antelope文件格式。\nInnoDB 1.0.x之后，采用了新的行存储格式（row storage layout），即Barracuda文件格式，新的文件格式兼容之前版本的格式，即包括Antelope文件格式，并加入了2种新的记录存储格式：Dynamic（更有效率的off-page column、larger index key prefix（767→ 3072））和支持记录压缩的Compressed。\nInnoDB中的数据压缩分为三种：\n 页压缩 透明页压缩 行压缩   这2种文件格式以及行格式的关系如下图所示：页是数据库的基本存储单位，所以在数据页上会标识该页所使用的行格式，PageHeader.PAGE_N_HEAP的最高位（第一个bit）标识该页所使用的行格式为new style还是old style，判断函数为page_is_comp。\n这几种行格式的官方说明：\nInnoDB Row Format Overview\n   Row Format Compact Storage Characteristics Enhanced Variable-Length Column Storage Large Index Key Prefix Support Compression Support Supported Tablespace Types Required File Format     REDUNDANT No No No No system, file-per-table, general Antelope or Barracuda   COMPACT Yes No No No system, file-per-table, general Antelope or Barracuda   DYNAMIC Yes Yes Yes No system, file-per-table, general Barracuda   COMPRESSED Yes Yes Yes Yes file-per-table, general Barracuda    从上图可以看出，行格式主要的区别是：\n REDUNDANT：中古版本 COMPACT：更紧凑 DYNAMIC：更有效率的off-page column（768-\u0026gt;127）、index key prefix支持范围更大（767 → 3072，innodb_large_prefix） COMPRESSED：更高的存储效率，对off-page column进行压缩（lz4）  这些版本的兼容性过渡（downgrade）通过以下参数提供，MySQL 5.7后会废弃这些参数（当时是为了5.1的兼容性，现在5.1生命周期已经结束，所以不再需要这些参数）：\n innodb_file_format innodb_file_format_check innodb_file_format_max innodb_large_prefix  Backward Compatibility\nThe only way to “downgrade” an InnoDB tablespace to the earlier Antelope file format is to copy the data to a new table, in a tablespace that uses the earlier format.\nBeginning with version InnoDB 1.0.1, the system tablespace records an identifier or tag for the “highest” file format used by any table in any of the tablespaces that is part of the ib-file set. Checks against this file format tag are controlled by the configuration parameter innodb_file_format_check, which is ON by default. The ability to set innodb_file_format_check is useful (with future releases) if you manually “downgrade” all of the tables in an ib-file set. You can then rely on the file format check at startup if you subsequently use an older version of InnoDB to access the ib-file set.\n首先来看Redundant格式。\nRedundant行格式 #  从上面我们可以得知，物理记录由3部分组成：\n record directory：通过column offset list存储每列的长度（len+data） extra info：行的元信息 行数据：按列存储  InnoDB依靠column offset list和extra info以二进制的形式从数据页中完整的读取一行物理记录，如果需要，还需要将其转化为逻辑记录。其中，实际存储的第一列的位置称为original offset，物理记录总指向这个位置，而非物理记录实际的开始位置。\n首先是column offset list，其根据列的顺序存放，每列的offset都是之前offset的累加（从后往前累加），并且每列offset的前两个bit标识该列是否为空（NULL），以及是否为溢出列（EXTERN）。extra info一共使用了48 bit（6个字节），字段含义见下：\n   字段 大小（bit） 说明     info_bits 4 info_bits信息   n_owned 4 page directory的slot所指向的记录所拥有的记录数量，即槽所代表的区间的记录数量   heap no 13 记录在堆中的序号（伪记录 0 1，用户记录从2开始，顺序分配，保证其不变性）   n_fields 10 记录中列的数量   offset 1byte or not 1 col offset list为1个字节还是2个字节，如果列长度小于127个字节，用1个字节表示，否则用2个字节表示   next record 16 指向下一个记录的original offset位置    除了info_bits，extra info中的其他信息都不存放在逻辑记录中（不需要，因为描述的是physical layout信息）。\ninfo_bits内容如下：\n   字段 大小（bit） 说明     字段 大小（bit） 说明   / 1 未使用   / 1 未使用   deleted_flag 1 记录删除标记   min_rec_flag 1 B+树中非叶子节点的最小记录标记    这里稍微展开一下，InnoDB采用的是索引组织表的形式（B+树）组织数据，也就是说，记录是根据B+树的规则进行存放的，记录存放在页中。但是页中的记录并不是根据索引规则进行排序的，因为如果这样组织行，对行进行数据变更会因为要保持顺序性而不断的调整位置，这样的开销过于巨大。于是，页中将行组织成堆的形式，即页中记录的存放是无序的，heap no标识在页堆中记录的序号，记录和记录之间通过next_record进行逻辑顺序的串联。另外，heap no也用于实现行锁（即行锁以页的方式组织，内部通过heap no的bitmap来标识行锁信息），在锁的介绍中再进行详细说明。\n在B+树的非叶子节点（非最下层）用最左节点（leftmost）来确定每层的左边界（predefined minimum record），即在非叶子层的第一条用户记录时设置该bit（REC_INFO_MIN_REC_FLAG）。\n每层的左边界判断逻辑为：\nPAGE_LEVEL != 0 \u0026amp;\u0026amp; FIL_PAGE_PREV == FIL_NULL 使用场景为：\n B+树split/merge 收集index的统计信息（first n_prefix columns） GIS  下面我们看一个实际的例子，来直观的了解数据的物理存储：\ncreate table t1 ( c1 varchar(10), c2 varchar(10), c3 char(10), c4 varchar(10) ) ROW_FORMAT=REDUNDANT; insert into t1 values ('a', 'bb', 'bb', 'ccc'); insert into t1 values ('d', 'ee', 'ee', 'fff'); insert into t1 values ('d', NULL, NULL, 'fff'); hexdump -c -V t1.ibd\n根据物理记录的格式，可以将hexdump整理为：\nRedundant // ROW 1 37 34 16 14 13 0c 06 // col offset list （逆序） 3 | 30 | 2 | 1 | 7 | 6 | 6 00 00 10 0f 00 ce // extra info（定长 6字节 48 bit） 00 00 00 00 02 03 // ROW ID（定长 6字节） 00 00 00 00 05 22 // Trx ID（定长 6字节） ba 00 00 01 2e 01 10 // 回滚指针（定长 7字节） 61 // 1字节 62 62 // 2字节 62 62 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 // 30字节 63 63 63 // 3字节 // ROW 2 37 34 16 14 13 0c 06 // col offset list （逆序） 3 | 30 | 2 | 1 | 7 | 6 | 6 00 00 18 0f 01 12 // extra info（定长 6字节 48 bit） 00 00 00 00 02 04 // ROW ID（定长 6字节） 00 00 00 00 05 23 // Trx ID（定长 6字节） bb 00 00 01 2f 01 10 // 回滚指针（定长 7字节） 64 // 1字节 65 65 // 2字节 65 65 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 // 30字节 66 66 66 // 3字节 // ROW 3 35 b2 94 14 13 0c 06 // col offset list （逆序） 3 | 30 | N | 1 | 7 | 6 | 6 00 00 20 0f 00 74 // extra info（定长 6字节 48 bit） 00 00 00 00 02 05 // ROW ID（定长 6字节） 00 00 00 00 05 28 // Trx ID（定长 6字节） be 00 00 01 31 01 10 // 回滚指针（定长 7字节） 64 // 1字节 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 // 30字节 66 66 66 // 3字节 从中我们可以看出：\n varchar列如果为NULL，则不占用任何实际的存储空间 char列不管是否为NULL都需要padding补齐全部空间（填充0）。这是为了避免char由NULL改为not-NULL而引发page re-org，只需要调整column offset list中的NULL bit。另外，根据字符集的不同，char类型的实际存储长度也不同：如果字符集为latin1，则每个字符占1个字节，如果字符集为utf8，第三列总长则为10*3 = 30字节。从这里可以看出，char类型会按照规定长度（定义）存储，而不是按照实际数据长度存储。  column offset list的layout布局如下：展开extra info：从这里可以看到，3个记录的heap no依次为2、3、4，这是因为在每个页中都有两个虚拟的伪记录（REC_STATUS_INFIMUM和REC_STATUS_SUPREMUM），其heap no分别是0和1。因此用户记录总是从heap no = 2开始。这两个虚拟的伪记录也起到边界的作用，关于虚拟的伪记录在索引中有详细介绍。\nn_fields标识有7列，由3列系统列和4列用户列组成。\nnext_record标识了本记录相对于下一个记录的偏移量，可以想象成一个小兔子，next_record标识了下一个记录从页头要跳跃的长度（相对于页的绝对偏移量）。\nCompact行格式 #  Compact行格式如下图所示：Compact行格式和Redundant行格式的主要区别如下：\n 不再存储系统列的元信息（ROW ID、Trx ID、回滚指针） 列的NULL值单独提出来一个NULL标志位（CEILING(N/8) bytes），每列1bit表示，varchar/char的NULL值都不再占用存储空间 EXTERN flag只有当字段列表为2个字节时才标识，列长小于127字节没有意义；去掉了1bytes_off_flag，每个字段采用1个字节还是2个字节通过col的第一个bit可以表示 增加了record_type：000=conventional（叶子节点，总在第0层）, 001=node pointer （B+树非叶子节点指针记录）, 010=infimum, 011=supremum, 1xx=reserved 去掉了n_fields，其通过非NULL字段列表（2F）和NULL标记位（NULL bitmap）可以算出列的数量 next record采用相对偏移量代替绝对偏移量  在构建内存tuple的时候，tuple→info_bits会保存，同样，record_type中的node pointer也会在构建时设置到其上：\ndict_index_build_node_ptr dtuple_set_info_bits(tuple, dtuple_get_info_bits(tuple) | REC_STATUS_NODE_PTR); 那我们接着用之前例子，看一下在Compact行格式下的physical layout：\nCompact // ROW 1 03 0a 02 01 // 非空变长字段列表（N=4, varchar index = [03 02 01] char len = 10） 00 00 00 10 00 2d // extra info（定长 5字节 + NULL bitmap（8 bit）） 00 00 00 00 02 00 // ROW ID（定长 6字节） 00 00 00 00 05 11 // Trx ID（定长 6字节） ae 00 00 01 22 01 10 // 回滚指针（定长 7字节） 61 // 1字节 62 62 // 2字节 62 62 20 20 20 20 20 20 20 20 // 10字节 63 63 63 // 3字节 // ROW 2 03 0a 02 01 // 非空变长字段列表（N=4, varchar index = [03 02 01] char len = 10） 00 00 00 18 00 2b // extra info（定长 5字节 + NULL bitmap（8 bit）） 00 00 00 00 02 01 // ROW ID（定长 6字节） 00 00 00 00 05 12 // Trx ID（定长 6字节） af 00 00 01 23 01 10 // 回滚指针（定长 7字节） 64 // 1字节 65 65 // 2字节 65 65 20 20 20 20 20 20 20 20 // 10字节 66 66 66 // 3字节 // ROW 3 03 01 // 非空变长字段列表（N=2, varchar index = [03 01]） 06 00 00 20 ff 96 // extra info（定长 5字节 + NULL bitmap（8 bit）） 00 00 00 00 02 02 // ROW ID（定长 6字节） 00 00 00 00 05 17 // Trx ID（定长 6字节） b2 00 00 01 26 01 10 // 回滚指针（定长 7字节） 64 // 1字节 66 66 66 // 3字节 首先看一下行数据的存储：\nRedundant // ROW 1 61 62 62 62 62 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 63 63 63 // ROW 2 64 65 65 65 65 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 66 66 66 // ROW 3 64 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 66 66 66 Compact // ROW 1 61 62 62 62 62 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 63 63 63 // ROW 2 64 65 65 65 65 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 66 66 66 // ROW 3 64 66 66 66 从上面可以看出，第三行记录显著节省了空间。\n首先是非空可变字段列表：接着是extra info：获取extra info采用位操作：逻辑记录和物理记录的转换 #  逻辑记录和物理记录之间的转换，通过offsets[]数组来进行：详细的函数注释如下：\n/*********************************************************//** Builds a new-style physical record out of a data tuple and stores it beginning from the start of the given buffer. @return pointer to the origin of physical record */ static rec_t* rec_convert_dtuple_to_rec_new( /*==========================*/ byte* buf, /*!\u0026lt; in: start address of the physical record */ const dict_index_t* index, /*!\u0026lt; in: record descriptor */ const dtuple_t* dtuple) /*!\u0026lt; in: data tuple */ { ulint extra_size; // 计算extra_size（not-null dynamic col list + extra info）  ulint status; rec_t* rec; // 指向实际的行数据（original offset）  status = dtuple_get_info_bits(dtuple) \u0026amp; REC_NEW_STATUS_MASK; rec_get_converted_size_comp(index, status, dtuple-\u0026gt;fields, dtuple-\u0026gt;n_fields, \u0026amp;extra_size); rec = buf + extra_size; rec_convert_dtuple_to_rec_comp(rec, index, dtuple-\u0026gt;fields, dtuple-\u0026gt;n_fields, NULL,status, false); // 逻辑记录转换为物理记录  /* Set the info bits of the record */ rec_set_info_and_status_bits(rec, dtuple_get_info_bits(dtuple)); return(rec); } /*********************************************************//** Builds a ROW_FORMAT=COMPACT record out of a data tuple. */ UNIV_INLINE void rec_convert_dtuple_to_rec_comp( /*===========================*/ rec_t* rec, /*!\u0026lt; in: origin of record */ const dict_index_t* index, /*!\u0026lt; in: record descriptor */ const dfield_t* fields, /*!\u0026lt; in: array of data fields */ ulint n_fields, /*!\u0026lt; in: number of data fields */ const dtuple_t* v_entry, /*!\u0026lt; in: dtuple contains virtual column data */ ulint status, /*!\u0026lt; in: status bits of the record */ bool temp) /*!\u0026lt; in: whether to use the format for temporary files in index creation */ { const dfield_t* field; const dtype_t* type; byte* end; byte* nulls; byte* lens; ulint len; ulint i; ulint n_node_ptr_field; ulint fixed_len; ulint null_mask = 1; ulint n_null; ulint num_v = v_entry ? dtuple_get_n_v_fields(v_entry) : 0; ut_ad(temp || dict_table_is_comp(index-\u0026gt;table)); if (temp) { ut_ad(status == REC_STATUS_ORDINARY); ut_ad(n_fields \u0026lt;= dict_index_get_n_fields(index)); n_node_ptr_field = ULINT_UNDEFINED; nulls = rec - 1; if (dict_table_is_comp(index-\u0026gt;table)) { /* No need to do adjust fixed_len=0. We only need to adjust it for ROW_FORMAT=REDUNDANT. */ temp = false; } } else { ut_ad(v_entry == NULL); ut_ad(num_v == 0); nulls = rec - (REC_N_NEW_EXTRA_BYTES + 1); switch (UNIV_EXPECT(status, REC_STATUS_ORDINARY)) { case REC_STATUS_ORDINARY: ut_ad(n_fields \u0026lt;= dict_index_get_n_fields(index)); n_node_ptr_field = ULINT_UNDEFINED; break; case REC_STATUS_NODE_PTR: ut_ad(n_fields == dict_index_get_n_unique_in_tree_nonleaf(index) + 1); n_node_ptr_field = n_fields - 1; break; case REC_STATUS_INFIMUM: case REC_STATUS_SUPREMUM: ut_ad(n_fields == 1); n_node_ptr_field = ULINT_UNDEFINED; break; default: ut_error; return; } } end = rec; if (n_fields != 0) { n_null = index-\u0026gt;n_nullable; lens = nulls - UT_BITS_IN_BYTES(n_null); /* clear the SQL-null flags */ memset(lens + 1, 0, nulls - lens); } /* Store the data and the offsets */ for (i = 0; i \u0026lt; n_fields; i++) { // 将每一列的len、NULL、数据存储到相应的位置  const dict_field_t* ifield; dict_col_t* col = NULL; field = \u0026amp;fields[i]; type = dfield_get_type(field); len = dfield_get_len(field); // 如果是索引指针，则只用4个字节存储（处理后退出循环），即data = fixed 4 bytes  if (UNIV_UNLIKELY(i == n_node_ptr_field)) { ut_ad(dtype_get_prtype(type) \u0026amp; DATA_NOT_NULL); ut_ad(len == REC_NODE_PTR_SIZE); memcpy(end, dfield_get_data(field), len); end += REC_NODE_PTR_SIZE; break; } // 处理null bits  if (!(dtype_get_prtype(type) \u0026amp; DATA_NOT_NULL)) { /* nullable field */ ut_ad(n_null--); if (UNIV_UNLIKELY(!(byte) null_mask)) { nulls--; null_mask = 1; } ut_ad(*nulls \u0026lt; null_mask); /* set the null flag if necessary */ if (dfield_is_null(field)) { *nulls |= null_mask; null_mask \u0026lt;\u0026lt;= 1; continue; } null_mask \u0026lt;\u0026lt;= 1; } /* only nullable fields can be null */ ut_ad(!dfield_is_null(field)); ifield = dict_index_get_nth_field(index, i); fixed_len = ifield-\u0026gt;fixed_len; col = ifield-\u0026gt;col; if (temp \u0026amp;\u0026amp; fixed_len \u0026amp;\u0026amp; !dict_col_get_fixed_size(col, temp)) { fixed_len = 0; } /* If the maximum length of a variable-length field is up to 255 bytes, the actual length is always stored in one byte. If the maximum length is more than 255 bytes, the actual length is stored in one byte for 0..127. The length will be encoded in two bytes when it is 128 or more, or when the field is stored externally. */ if (fixed_len) { // 定长，则不存储长度  } else if (dfield_is_ext(field)) { // off-page column，需要2个字节存储长度  ut_ad(DATA_BIG_COL(col)); ut_ad(len \u0026lt;= REC_ANTELOPE_MAX_INDEX_COL_LEN + BTR_EXTERN_FIELD_REF_SIZE); *lens-- = (byte) (len \u0026gt;\u0026gt; 8) | 0xc0; *lens-- = (byte) len; } else { /* DATA_POINT would have a fixed_len */ ut_ad(dtype_get_mtype(type) != DATA_POINT); ut_ad(len \u0026lt;= dtype_get_len(type) || DATA_LARGE_MTYPE(dtype_get_mtype(type)) || !strcmp(index-\u0026gt;name, FTS_INDEX_TABLE_IND_NAME)); if (len \u0026lt; 128 || !DATA_BIG_LEN_MTYPE( // 变长列，长度\u0026lt;125，使用1个字节存储长度  dtype_get_len(type), dtype_get_mtype(type))) { *lens-- = (byte) len; } else { // 变长列，长度\u0026gt;=125，使用2个字节存储长度  ut_ad(len \u0026lt; 16384); *lens-- = (byte) (len \u0026gt;\u0026gt; 8) | 0x80; *lens-- = (byte) len; } } memcpy(end, dfield_get_data(field), len); end += len; } if (!num_v) { return; } /* reserve 2 bytes for writing length */ byte* ptr = end; ptr += 2; /* Now log information on indexed virtual columns */ for (ulint col_no = 0; col_no \u0026lt; num_v; col_no++) { dfield_t* vfield; ulint flen; const dict_v_col_t* col = dict_table_get_nth_v_col(index-\u0026gt;table, col_no); if (col-\u0026gt;m_col.ord_part) { ulint pos = col_no; pos += REC_MAX_N_FIELDS; ptr += mach_write_compressed(ptr, pos); vfield = dtuple_get_nth_v_field( v_entry, col-\u0026gt;v_pos); flen = vfield-\u0026gt;len; if (flen != UNIV_SQL_NULL) { /* The virtual column can only be in sec index, and index key length is bound by DICT_MAX_FIELD_LEN_BY_FORMAT */ flen = ut_min( flen, static_cast\u0026lt;ulint\u0026gt;( DICT_MAX_FIELD_LEN_BY_FORMAT( index-\u0026gt;table))); } ptr += mach_write_compressed(ptr, flen); if (flen != UNIV_SQL_NULL) { ut_memcpy(ptr, dfield_get_data(vfield), flen); ptr += flen; } } } mach_write_to_2(end, ptr - end); } 物理记录（new style）的构建，如图所示：大记录格式 #  如果列过大，一个数据页会存储不下，这样的记录称之为大记录（big record），大列存储的页称为溢出页（overflow page），列称为off-page columns。对于InnoDB来说，只有类型为BLOB和TEXT才可能是off-page column。如果实际的列值少于1000字节，则放在当前页，否则才采用溢出页存储（多个溢出页组成单向链表）。\n能不能不用page存，而直接采用外部的存储系统存储：这样就脱离了事务控制的范围，无法保证ACID  对于InnoDB表，转为大记录的前提是：\n 当前记录的总字节数 \u0026gt; 空白页可用空间的一半 \u0026gt; 1/2 * page_get_free_space_of_empty()，即8132字节 列大于整页（REC_MAX_DATA_SIZE），即16384字节  可以发现，只要满足第一个条件即可。因此在进行插入操作时将大记录中的off-page column的部分数据存放到溢出页中，直到记录占用的字节数小于一半页的大小（8132字节），这种列的属性为extern，在column offset列的偏移量列表中通过REC_2BYTE_EXTERN_MASK标记。\n一个overflow page只能给一个off-page column使用。这样的设计减少了实现的复杂度，清晰明了。  从上图中可以看到，col 2的数据并不完全存放在记录所在的索引页中。对于extern属性列的存放进行下列“格式化”：仅在当前记录所在的页中存放前127字节，另外20个字节指向溢出页的信息。extern列的格式如下：\n   名称 大小（字节） 说明     column prefix 127 extern列前127字节的数据   BTR_EXTERN_SPACE_ID 4 off-page的SPACE ID   BTR_EXTERN_PAGE_NO 4 off-page的page no   BTR_EXTERN_OFFSET 4 off-page中记录存放的开始位置   BTR_EXTERN_LEN 8 存放在off-page的字节数（前2个bit保留：BTR_EXTERN_OWNER_FLAG + BTR_EXTERN_INHERITED_FLAG）    虽然BTR_EXTERN_LEN的长度为8个字节，但是实际存储列长度仅占用后4个字节，所以BLOB字段最大支持4GB（2 * 231），前4个字节（只用到第一个字节）用于存放其他的extern属性信息，通过下面的宏进行提取：\n** The most significant bit of BTR_EXTERN_LEN (i.e., the most significant bit of the byte at smallest address) is set to 1 if this field does not 'own' the externally stored field; only the owner field is allowed to free the field in purge! */ #define BTR_EXTERN_OWNER_FLAG 128 /** If the second most significant bit of BTR_EXTERN_LEN (i.e., the second most significant bit of the byte at smallest address) is 1 then it means that the externally stored field was inherited from an earlier version of the row. In rollback we are not allowed to free an inherited external field. */ #define BTR_EXTERN_INHERITED_FLAG 64 extern的其他属性包括：\n purge：BTR_EXTERN_OWNER_FLAG：标记对属性为extern的列进行了purge操作 （除extern列外的其他列更新）删除后回滚：BTR_EXTERN_INHERITED_FLAG：标记当前extern列继承（inherited）一个早先的记录版本。若回滚，则不需要对其进行删除。  为什么需要设置一个继承标记位呢？这是因为在对大记录进行更新的时候，可能会遇到主键值更新了，但是extern列却没有更新的情况。在这种情况下，InnoDB首先将会对原主键记录进行伪删除（即delete flag设为1），然后插入一条含有新主键的记录。这时，由于extern列没有更新，则只需要将BTR_EXTERN_*（space+page+offset+len）——指向原来的位置即可。但是当事务发生回滚时会删除新插入的记录，将伪删除记录的delete flag重置为0。而由于extern列是继承前一个较早版本的记录，其实并不需要删除，只需要通过BTR_EXTERN_INHERITED_FLAG进行标识。  当需要将记录转化为大记录对象时（dtuple_convert_big_rec），InnoDB首先选择最大长度的列进行extern格式化。如果格式化后当前页中该记录所占的字节数已经小于1/2 * page_get_free_space_of_empty()时，那么完成转化，否则重新再选择一个列进行extern格式化，直到满足条件。\n下面结合一个实际的例子来看一下溢出页：\ncreate table big_rec_t ( a int not null, b BLOB, primary key(a) )engine=InnoDB; insert into big_rec_t values (1, 'David'); insert into big_rec_t values (2, repeat('y',1000)); insert into big_rec_t values (3, repeat('z',10000)); 前2条记录的长度没有超过1/2 * page_get_free_space_of_empty()，因此记录还是存储在当前数据页中，第3条记录在当前数据页中仅存放前127个字节，其余9873个字节存放在off-page中。\n// 第3条记录 40 a4 00 11 00 0a 00 04 // column offset list 4列 逆序（变长，因为列的总长度超过127字节，所以每列offset采用2个字节存储） 00 00 20 08 00 74 // extra info（定长 6字节） 80 00 00 03 // ROW ID（定长 4字节） 00 00 00 00 27 09 // Trx ID（定长 6字节） 80 00 00 00 2d 00 84 // 回滚指针（定长 7字节） 7a 7a 7a .. 7a // 第4列的前127个字节 00 00 00 00 // BTR_EXTERN_SPACE_ID 00 00 00 35 // BTR_EXTERN_PAGE_NO 00 00 00 26 // BTR_EXTERN_OFFSET 00 00 00 00 00 00 00 26 91 // BTR_EXTERN_LEN 4列的长度按照顺序排列后依次是0x0004、0x000a、0x0011、0x40a4，最后一列的属性是extern，随意最后一列的offset实际上是 0x00a4 \u0026amp; REC_2BYTE_EXTERN_MASK得到的值。通过0x00a4 - 0x0011可以计算出第4列的长度为147（127 + 20），其中前127个字节为第4列的前缀，后20个字节为off-page的信息。通过BTR_EXTERN_LEN可以计算出存放在off-page中的数据为9873 bytes（0x2691）。通过BTR_EXTERN_PAGE_NO和BTR_EXTERN_OFFSET可以定位到溢出页：\n00 00 26 91 // BTR_BLOB_HDR_PART_LEN ff ff ff ff // BTR_BLOB_HDR_NEXT_PAGE_NO 7a 7a 7a ... 7a // 第4列的后面9873个字节 由于仅用1个off-page就可以存下第4列的溢出数据，因此这里BTR_BLOB_HDR_PART_LEN = 之前BTR_EXTERN_LEN的值，即9873，并且BTR_BLOB_HDR_NEXT_PAGE_NO的值为0xffffffff（EOF）。\n大记录对InnoDB的性能会产生影响，例如对于更新、删除操作，其只能进行悲观操作，而这意味着并发性能的下降。这部分在B+树的章节中进行具体的介绍。总之，除非必要，尽量使得表中的记录紧凑，同时尽可能的符合数据库理论的范式要求。\n伪记录 #  在索引页中存在两个伪记录，分别为infimum记录（REC_STATUS_INFIMUM）和supremum记录（REC_STATUS_SUPREMUM）。可以将其理解为页中最小和最大的记录，起一个“边界”的作用，并且infimum记录可以优化锁的性能（诣在更新时用于临时保存更新记录上的锁信息。这样做的好处是可以提高更新锁的效率，否则需要先删除老的锁对象然后再创建新的锁对象）。逻辑记录的实现 #  逻辑记录的内存布局 #  上面已经提到，逻辑记录（dtuple_t）存放在内存中，每个逻辑记录内部又包含多个字段（dfield_t），逻辑记录在内存中的布局如下图所示：dtuple_t对象：\n   变量 说明     变量 说明   info_bits 同物理记录（delete flag、leftmost）   n_fields 记录中的列数量   n_fields_cmp 在记录进行比较时，仅比较这些数量的列在索引中进行搜索时，仅判断这些数量的列，默认和n_fields相同   fields 记录中的列，array of \u0026lt;d_field_t\u0026gt;   n_v_fields 记录中的虚拟列数量   v_fields 记录中的虚拟列，array of \u0026lt;d_field_t\u0026gt;   tuple_list 多个记录的链表节点   magic_n magic number    dfield_t对象：\n   变量 说明     data 列的值   ext:1 是否为off-page column   spatial_status:2 GIS索引使用   len 列的长度   type 列的类型（dtype_t）    构建逻辑记录 #  dtuple_t从mem_heap_t上alloc（dtuple_t + n * dfield_t）个字节，所以内存布局上是连续的，其中dtuple_t位于\u0026amp;tuple[0]，field地址位于\u0026amp;tuple[1]，如图所示：\n列的类型(（dfield_t.type）dtype是对两个记录进行比较的判断基础。\n对于大记录，不用dtuple_t表示，而是用big_rec_t表示：\nbig_rec_t比dtuple_t简单很多，只是多了一个变量heap，该变量的作用为申请一个内存堆之后对big_rec_field_t对象中的data都从该内存堆中进行分配。其余变量和之前介绍相同。\ndtuple_convert_big_rec用于将逻辑记录转化为大记录的格式，当转化完成时，之前的dfield_t只记录列的前127个字节，其余的数据放到big_rec_field_t.data中，即函数调用会首先对dtuple_t的列进行修改，同时又生成了新的big_rec_t对象。同样，可以将大记录对象重新转化为普通逻辑记录，由dtuple_convert_back_big_rec完成。\n记录之间的比较 #  InnoDB在通过B+树搜索时，通过B+树索引只能定位到记录所在的页，不能直接定位到具体的记录（行）。在找到页后，还需要通过二叉查找算法进行搜索，最终定位到查询的记录。因此，在查找时需要对记录进行比较。\n记录的比较可以分为逻辑记录与物理记录之间的比较，以及物理记录之间的比较。通常来说，一般进行的都是逻辑记录和物理记录的比较，这是因为，对于插入操作，本身就不存在物理记录，所以需要构造一个逻辑记录，而对于UPDATE、DELETE操作，首先需要通过SELECT进行定位，这时就会转化为逻辑记录。\n物理记录之间的比较通常用于对索引caridinality（索引散列度）的统计，即统计索引中唯一记录的数量。InnoDB会随机选择一些页，然后对页中的记录进行逐个比较后进行统计，这时需要物理记录之间进行比较。\n即比较矩阵为：\n    逻辑记录 物理记录     逻辑记录 update/delete insert   物理记录 insert 索引统计（caridinality）    记录在进行比较时，实际上是根据列进行比较，而根据列的不同类型（dtype_t），比较的方式也不相同。\ndtype_t的定义如下：\nstruct` `dtype_t { ``ulint mtype:8; ``// main data type ``ulint prtype:32; ``// precise type; MySQL data type ``ulint len:16; ``// maximum byte length of the string data ``ulint mbminmaxlen:5; ``// 字符集存储字符的最大/最小长度 }; mb是multi-byte character的缩写，不同的字符集在存储字符的时候使用的字节数不同。mbminlen和mbmaxlen代表这个字符集存储字符的最大/最小长度  从上述数据结构可以看到，类的类型又分为了mtype和ptype。可以把mtype理解为列的类型，ptype理解为列的属性。\nmtype的类型如下，其中字符串有string和binary string之分：\n   类型 值 说明 string类型 binary string类型 最小长度 最大长度     DATA_VARCHAR 1 latin1字符集VARCHAR类型 Y  0 index_col-\u0026gt;len   DATA_CHAR 2 latin1字符集CHAR类型 Y  index_col-\u0026gt;len index_col-\u0026gt;len   DATA_FIXBINARY 3 BINARY类型 Y Y index_col-\u0026gt;len index_col-\u0026gt;len   DATA_BINARY 4 VARBINARY类型 Y Y 0 index_col-\u0026gt;len   DATA_BLOB 5 大记录类型 Y Y \u0026amp; prtype == BINARY bit （11th-bit） 0 ULINT_MAX   DATA_INT 6 INT类型   index_col-\u0026gt;len index_col-\u0026gt;len   DATA_SYS_CHILD 7 B+树索引中的node pointer中指向的child page no   / /   DATA_SYS 8 系统列类型，比如隐藏的TrxID列、ROW ID列、回滚指针列   index_col-\u0026gt;len index_col-\u0026gt;len   DATA_FLOAT 9 FLOAT类型   index_col-\u0026gt;len index_col-\u0026gt;len   DATA_DOUBLE 10 DOUBLE类型   index_col-\u0026gt;len index_col-\u0026gt;len   DATA_DECIMAL 11 DECIMAL类型   0 index_col-\u0026gt;len   DATA_VARMYSQL 12 非latin1字符集VARCHAR类型 Y  0 index_col-\u0026gt;len   DATA_MYSQL 13 非latin1字符集的CHAR类型 Y  prtype == BINARY bit （11th-bit） index_col→len else mbminmaxlen index_col-\u0026gt;len   DATA_GEOMETRY 14 geometry类型   0 ULINT_MAX   DATA_POINT 15 geo   index_col-\u0026gt;len index_col-\u0026gt;len   DATA_VAR_POINT 16 geo   0 ULINT_MAX   DATA_MTYPE_MAX 63 上限       DATA_ERROR 111 错误类型        记录之间的比较用mtype类型，而不是MySQL提供给用户定义的数据列类型（上层的列类型）：比如没有诸如SMALLINT、TINYINT、ENUM这些，对于VARCHAR类型，mtype有DATA_VARCHAR和DATA_VARMYSQL两种。get_innobase_type_from_mysql_type负责将各种MySQL上层的列类型转化为dtype的mtype类型。\n对于mtype大于DATA_FLOAT的列，对整个列的字节（padding后）进行比较。而对于非latin1字符集的VARCHAR和CHAR类型，则调用cmp_whole_field进行比较。为什么之前的列类型不需要进行整个字节的比较呢？这是因为可以允许下面的情况发生：\nselect \u0026lsquo;a\u0026rsquo; = (\u0026lsquo;a \u0026lsquo;) =\u0026gt; 1\n在latin1字符集下对填充字段不进行判断，视为两个字符串是相等的（字符串\u0026rsquo;a\u0026rsquo;的十六进制为0x61，字符串\u0026rsquo;a \u0026lsquo;的十六进制为0x61202020）.\n在之前的MySQL版本中，有一个小问题，对于类型VARBINARY和BINARY，其存储是按照字节进行存储的，而排序是根据latin1字符集进行排序的，好在后续已经解决了这个问题，即属于了下面的string类型，带有了字符集和校对字符集信息。\n区别参见官方文档：The BINARY and VARBINARY Types，简单来说就是一个是带字符集和校对字符集的字符串，一个是单纯的二进制字符串，即文档中提到的“contain byte strings rather than character strings. This means they have the binary character set and collation, and comparison and sorting are based on the numeric values of the bytes in the values.”，即熟悉的utf8_bin、utf8_general_ci、utf8_general_cs的区别。\nptype表示对于列更为精确的表述，如果为系统列，还可以标识这个系统列的类型，此外，还定义了列的属性。\n通过mtype和ptype就可以对列进行比较了。逻辑记录与物理记录的比较通过cmp_dtuple_rec_*，物理记录之间的比较通过cmp_rec_rec_*。返回值有-1、0、1，分别表示比较的记录1小于、等于、大于记录2。\n对于返回值不等于0的情况（即大于、小于），还通过matched_fields、matched_bytes这两个变量来返回额外的信息。matched_fields标识匹配的记录不相等，但是已经匹配相等的列的个数，matched_bytes标识对于最后一个不匹配的列中，已经匹配的字节数。\nNULL值在InnoDB中被视为最小的值，任何与NULL值进行的等值比较返回的值都是NULL\n1 = NULL =\u0026gt; NULL\nNULL = NULL =\u0026gt; NULL\n然而在InnoDB内部的记录比较中，也就是cmp_dtuple_rec_with_match、cmp_rec_rec_with_match的比较中，两个NULL值被视为是相等的。这也好理解，因为这里所阐述的记录比较是用于索引的排序、记录的查询，因此NULL值相同表示其在索引中所处的位置。\n"},{"id":33,"href":"/docs/MySQL/InnoDB/11_redo_log/","title":"redo log","section":"Inno Db","content":"日志或者说logging schema主要是为crash recovery algorithms服务的，即为了保证事务的ACD特性。crash recovery是通过多种机制协调解决的，这里面包括buffer pool的flush策略，WAL、logging schema和checkpoint。并且，InnoDB采用MV2PL，还通过undo log在MVCC中实现delta storage用于存储行的多版本快照。\n从这里可以看出，日志在事务处理中的重要性。这里的日志主要分为undo log和redo log，undo log在下一章事务中详细介绍，本章聚焦在redo log。crash_recovery在事务一章介绍。\n设计 #  crash recovery algorithms由IBM在90年代发表的一篇ARIES论文提出（Algorithms for Recovery and Isolation Exploiting Semantics）。InnoDB也借鉴了ARIES的思想。思想的核心是利用日志（undo+redo）来实现可恢复性。\nredo log #  redo log是在数据库的运行时，随着数据的更改而产生的变更日志，其目的是两个：\n 体现数据的变更序 通过变更序持久化的能力，可以让数据库系统在crash recovery后恢复到内存态  因此，我们分别来详细描述如何做到这两点。\n首先是体现数据的变更序。\n在数据库中，数据都是通过page为单位组织的。因此，在体现数据变更的时候，这个变更序首先是page的变更序。并且，在InnoDB中，page被组成成B+ tree结构，因此，还要体现SMO，也就是说，还要考虑一个逻辑操作可能会设计B+ tree的多个页面，以及不同B+ tree之间的操作关系，这个关系的组织通过mini-transaction来实现，即提供原子粒度的一组redo log（只保证forwrd）保证动作-页面的一致性。。这些操作组合起来形成了事务，换句话说，事务中的每个逻辑操作产生了一系列page的变更序列，即一组redo log。\n在整体上，数据库中并发事务之间也需要在整体上进行page排序，这一方面依靠FIX Rules保证page间的并发控制，另一方面B+ tree concurrency control protocol也要从数据结构维度维护并发序，加上MV2PL机制，整体上这个page变更序列就确定了。这个全局page的变更序以一个全局的redo log buffer呈现。\n第二点，持久化实际上就是把redo log biffer sync到持久化存储（磁盘）中，sync到磁盘的时机我们称之为sync point。一般来讲，sync point同时也是事务commit点，即WAL机制（force log at commit），也就是保证事务的所有日志（redo+undo）sync到磁盘完成后，事务提交才真正完成，这意味着sync point = commit point。\n这个sync point也有group commit的体现，每个用户线程在各自事务中的一个逻辑操作通过mini-transaction提交到redo log buffer，然后再在sync point时一起持久化。换句话说，每次sync都会把当前sync point之前的所有redo log都持久化到磁盘，这一组（group）持久化的日志包含了各个事务并发修改的数据。\n设计取舍 #  在设计redo log时，需要保证数据的完整性，也要兼顾效率。\n数据的完整性方面，既要考虑磁盘上文件系统所提供的原子写的粒度，不能出现半写（partial-write），也要有校验机制来检验log block的完整，以及是否为异常关闭，还要考虑介质是否会出现损坏。\n在效率方面，从上面可以看出，commit point = sync point，这意味着事务处理的速度取决于redo log sync的效率，即使redo log file为顺序IO，也需要充分利用缓存（page cache+sync），兼以group commit，最大化的提升写入效率。\n从另一方面，事务处理中所产生的redo log量越小，性能越好。因此，大多数数据库系统（也包括InnoDB）在log schema设计上采用物理逻辑日志（Physiological Logging），不仅保证了日志量足够少，而且还提供幂等性（steal、recovery中二次crash）。也是因为log schema的这个设计，page flush需要保证page的完整性，以保证page在可用的基础上才能apply redo log。\ncrash recovery也要考虑恢复的日志量，通过checkpoint来维护LWN，page粒度的日志则可以实现page级的并行恢复。\n下面来介绍InnoDB中的具体实现。\n技术 #  mini-transaciton #  概念 #  MySQL文档上关于mini transaction的定义：\nAn internal phase of InnoDB processing, when making changes at the physical level to internal data structures during DML operations. A mini-transaction (mtr) has no notion of rollback; multiple mini-transactions can occur within a single transaction. Mini-transactions write information to the redo log that is used during crash recovery. A mini-transaction can also happen outside the context of a regular transaction, for example during purge processing by background threads.\n从这里的定义可以看出mini-transaction的几个特征，以下简称mtr：\n 主要用于DML操作的场景，诣在保证在物理层面上对内部数据结构的更改有效（一致）并通过redo log记录 其本质就是WAL（基于redo log），所以不用进行rollback（因为如果mtr写入过程中失败，修改只在内存中，不会落盘，只有mtr commit也就是完成WAL后，数据才会落盘） 一个事务可能包含多个DML，所以会有多个mtr crash recovery依赖flush后的WAL进行恢复  InnoDB中的ACID，最终要和底层的物理存储上对齐，而通过mini transaction来保证并发情况下和crash后逻辑操作对于页上数据变更的一致性，而事务保证多个逻辑操作数据的一致性和持久性需要通过mtr来实现。\n以DML举例，比如插入操作，如果表定义有primary index和secondary index，insert操作需要涉及到两颗index tree的更改，每个index tree可能还会发生SMO，这些操作都要保证物理结构上和DML的一致，即动作-页面的一致性。保证的方式就是通过将一系列页操作包含进一个mini-transaction，可以一起mtr commit到1，否则就是0。\n为了使mtr可以保证物理页在数据结构上的一致性，mtr需要遵循以下几个规则：\n FIX Rules：保证并发操作的隔离性 Write-Ahead Log（WAL）：可以持续写入日志，保证日志在mtr粒度合入redo log Force-log-at-commit：事务提交前必须先写入日志，保证持久性  FIX Rules #  当数据库访问或者修改一个页时，需要持有该页的latch（frame rwlock），以此保证并发情况下数据的一致性。acquire page latch的操作称为fixing the page。当latch granted后，称这个页已经fixed，unlock page latch的操作，称为unfixing。\nFIX Rules定义如下：\n 修改一个页时，需要获得X-latch 访问一个页时，需要获得X-latch或者S-latch 持有latch直到页的修改或者访问完成  如果操作涉及到多个页，那么根据FIX Rules的要求，需要持有多个页的latch，并在所有页的操作完成后，再释放latch。\n这里的latch类型有：\n PAGE_S_FIX：page latch PAGE_X_FIX：page latch PAGE_SX_FIX：page latch BUF_FIX：only buf fix S_LOCK：index latch X_LOCK：index latch SX_LOCK：index latch  关于frame rwlock和index concurrency control参见buffer pool一章和B+ tree一章。\nWrite-Ahead Log #  Write-Ahead-Log要求在把page写入磁盘之前，必须首先将日志写入磁盘中。\nWrite-Ahead Log流程如下：\n 每个页有一个LSN（有次序） 每次页的修改都需要维护该LSN（维护次序），并将页FIX 当一个页写入到持久存储设备时，必须将内存中所有小于该LSN的日志首先写入到磁盘中 当日志写入后，再将页写入到磁盘 在将页写入到持久存储设备时，需要将页FIX，以此保证内存页中数据的一致性  这里的日志就是redo log，redo log写入的磁盘设备是redo log file，page写入到对应的文件表空间。\nForce-log-at-commit #  Write-Ahead Logging保证了写入的顺序：先日志再数据，但是，并不能保证日志”真正“写入到了持久存储设备上。为了严格保证持久性，还需要Force-log-at-commit，即当事务提交时，日志必须“刷新”到持久存储设备上。这样，在日志刷盘后，page刷盘前发生crash，来保证crash recovery时可以通过日志将页回放到修改的内存态。\n事务的D需要Force-log-at-commit来保证，细节在commit point一节介绍。\nlog_write_up_to()调用链\nha_innobase::write_row(buf) innobase_commit(ht, m_user_thd, 1) trx_commit_complete_for_mysql(trx-\u0026gt;commit_lsn, trx)) trx_flush_log_if_needed(lsn) trx_flush_log_if_needed_low(lsn, flush) switch (srv_flush_log_at_trx_commit) case 2: /* Write the log but do not flush it to disk */ flush = false; case 1: /* Write the log and optionally flush it to disk */ log_write_up_to(lsn, flush); return; case 0: /* Do nothing */ return; mtr数据结构 #  struct mtr_t { struct Impl { mtr_buf_t m_memo; // 管理latch信息（以栈的方式，方便释放，遍历从栈顶到栈底，也方便释放保序） mtr_buf_t m_log; // 管理日志信息 bool m_made_dirty; // buffer pool中的page是否为脏页 TRUE (X_FIX/SX_FIX) \u0026amp;\u0026amp; block_dirty 或者FIX \u0026amp;\u0026amp; buf_block.made_dirty_with_no_latch （临时表只有当前会话线程修改） bool m_inside_ibuf; // 是否为insert buffer使用 bool m_modifications; // 是否产生了脏页 ib_uint32_t m_n_log_recs; // 有多少页的日志放入了m_log mtr_log_t m_log_mode; // MTR_LOG_ALL： 记录所有会修改磁盘数据的操作 MTR_LOG_NONE： 不记录redo，脏页也不放到flush list上 MTR_LOG_NO_REDO：不记录redo，但脏页放到flush list上 MTR_LOG_SHORT_INSERTS：插入记录操作REDO，在将记录从一个page拷贝到另外一个新建的page时用到，此时忽略写索引信息到redo log中 (只有page_copy_rec_list_end_to_created_page使用) ulint m_user_space_id // 修改的哪个文件（初始化为TRX_SYS_SPACE） // 这三个表空间是事务持久化到外存的联系纽带 fil_space_t* m_user_space; // 指向被mtr修改的用户表空间 fil_space_t* m_undo_space; // 指向被mtr修改的UNDO日志表空间 fil_space_t* m_sys_space; // 指向被mtr修改的系统表空间 mtr_state_t m_state; // mtr状态，见状态流转图 FlushObserver* m_flush_observer; mtr_t* m_mtr; // 指向外面的mtr_t }; Impl m_impl; lsn_t m_commit_lsn; // mtr commit时的lsn，提交时Command.m_end_lsn = m_commit_lsn class Command; // 负责具体的mtr动作，比如提交，释放latch/资源，unfix page } 从mtr的内部数据结构可以看出mtr最核心的两个功能就是latch（m_memo）和日志（m_log），这两种信息都是通过mtr_buf_t来组织的，内部是动态分配的block_t双向链表(每个block是512字节，也是redo log block的大小)，如下图所示：\nmtr_buf_t的函数列表：\n   函数 说明     函数 说明   ctor() earse() 初始化状态，只分配第一个block，没有分配m_heap   is_small() 判断是否为初始化状态   add_block() 分配m_heap，并分配block加到链表尾部   open(size) 在block中预留空间，返回空闲block的end()   close(ptr) 将m_used重置到ptr位置，m_buf_end归0   has_space(size) block是否有空间存储size个字节   find(pos) 在所有block中的空闲block上查找pos（pos\u0026lt;end），返回该block   for_each_block(functor) for_each_block_in_reverse(functor) 遍历所有block并执行functor(block) 反向遍历所有block并执行functor(block)   at(ptr) find()的block在ptr的位置   push(size) 返回open预留的开始位置（end()）   push(ptr, len) 使用上面的push移动len个字节    mini-transaction的状态流转图：\n使用mtr #  mtr_t mtr; mtr_start(\u0026amp;mtr); // FIX // 变更页 // 写入日志 mtr_commit(\u0026amp;mtr); // UNFIX 用伪代码的形式描述mtr的内部流程：\nmini_transaction_for_single_page() { create mtr object and init // ① ctor \u0026amp; mtr.start() lock the page in exclusive mode // ⓶ mtr_memo_push() transform the page // ③ generate undo and redo log record // ⓸ unlock the page // ⓹ mtr.commit() -\u0026gt; release_latches() } 如果一个mtr包含多个页的修改：\nmini_transaction_for_multiple_page() { create mtr object and init // ① ctor \u0026amp; mtr.start() page1 // ⓶ ③ ⓸ page2 // ⓶ ③ ⓸ ... pageN // ⓶ ③ ⓸ write MLOG_MULTI_REC_END // 记录了多个页的变更日志，合成整体，并入redo log buffer unlock the pageN...2,1 // ⓹ mtr.commit() -\u0026gt; release_latches() } 写入数据到mtr的m_log\nbyte *mlog_open(mtr_t *mtr, ulint size) // 打开mtr的m_log，即定位到最新一个block_t（有m_buf_end可用），并返回上一次写入的位置 mlog_write_initial_log_record_low() // 函数向m_log中写入type，space id，page no，并增加修改的页的数量（m_n_log_recs） mtr-\u0026gt;get_log()-\u0026gt;push() // 按不同的log item type写数据 mlog_close() // 更新m_log.m_size mtr record type有60多种（mlog_id_t），但所有mtr record的头部元素是相同的，都包含type（1字节），space_id（compressed）和page_no（compressed）（mlog_write_initial_log_record_low）\ntype为MLOG_TRUNCATE的示例代码如下：\nmtr_start(\u0026amp;mtr); log_ptr = mlog_open(\u0026amp;mtr, 11 + 8); log_ptr = mlog_write_initial_log_record_low(MLOG_TRUNCATE, m_table-\u0026gt;space, 0, log_ptr, \u0026amp;mtr); mach_write_to_8(log_ptr, lsn); log_ptr += 8; mlog_close(\u0026amp;mtr, log_ptr); mtr_commit(\u0026amp;mtr); mtr commit\nvoid mtr_t::commit() { m_impl.m_state = MTR_STATE_COMMITTING; Command cmd(this); if (m_impl.m_modifications \u0026amp;\u0026amp; (m_impl.m_n_log_recs \u0026gt; 0 || m_impl.m_log_mode == MTR_LOG_NO_REDO)) { cmd.execute(); // 页修改，执行mtr提交 } else { cmd.release_all(); // 否则只单纯的释放资源 cmd.release_resources(); } } mtr_t的整体数据结构如下图所示：\n提交详细流程：\nvoid mtr_t::Command::execute() { // prepare_write  // 1. MTR_LOG_NONE、MTR_LOG_NO_REDO直接将m_end_lsn=m_start_lsn=log_sys-\u0026gt;lsn  // 2. 计算len = m_impl-\u0026gt;m_log.size();  // 3. 如果修改了单页的记录，更新MLOG_SINGLE_REC_FLAG（日志头的type第1个bit）  // 4. 如果修改了多页的记录，写入MLOG_MULTI_REC_END（1字节）  if (const ulint len = prepare_write()) { // finish_write  // 1. 获取全局lsn（log_sys-\u0026gt;lsn）作为m_start_lsn  // 2. 将所有的block_t写入redo log buffer，按照log block粒度拷贝  // 3. 获得m_end_lsn（log_sys-\u0026gt;lsn）  finish_write(len); } //  if (m_impl-\u0026gt;m_made_dirty) { log_flush_order_mutex_enter(); } log_mutex_exit(); // 将mtr的m_commit_lsn设为m_end_lsn  m_impl-\u0026gt;m_mtr-\u0026gt;m_commit_lsn = m_end_lsn; release_blocks(); if (m_impl-\u0026gt;m_made_dirty) { log_flush_order_mutex_exit(); } release_latches(); release_resources(); } 实例 #  以一条insert语句为例：\n第一个 mtr 从 row_ins_clust_index_entry_low 开始 mtr_start(mtr_1) // mtr_1 贯穿整条insert语句 row_ins_clust_index_entry_low mtr_s_lock(dict_index_get_lock(index), mtr_1) // 对index加s锁 btr_cur_search_to_nth_level row_ins_clust_index_entry_low mtr_memo_push(mtr_1) // buffer RW_NO_LATCH 入栈 buf_page_get_gen btr_cur_search_to_nth_level row_ins_clust_index_entry_low mtr_memo_push(mtr_1) // page RW_X_LATCH 入栈 buf_page_get_gen btr_block_get_func btr_cur_latch_leaves btr_cur_search_to_nth_level row_ins_clust_index_entry_low mtr_start(mtr_2) // mtr_2 用于记录 undo log trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_start(mtr_3) // mtr_3 分配或复用一个 undo log trx_undo_assign_undo trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_memo_push(mtr_3) // 对复用（也可能是分配）的 undo log page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_page_get trx_undo_reuse_cached // 这里先尝试复用，如果复用失败，则分配新的 undo log trx_undo_assign_undo trx_undo_report_row_operation trx_undo_insert_header_reuse(mtr_3) // 写 undo log header trx_undo_reuse_cached trx_undo_assign_undo trx_undo_report_row_operation trx_undo_header_add_space_for_xid(mtr_3) // 在 undo header 中预留 XID 空间 trx_undo_reuse_cached trx_undo_assign_undo trx_undo_report_row_operation mtr_commit(mtr_3) // 提交 mtr_3 trx_undo_assign_undo trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_memo_push(mtr_2) // 即将写入的 undo log page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low trx_undo_page_report_insert(mtr_2) // undo log 记录 insert 操作 trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low mtr_commit(mtr_2) // 提交 mtr_2 trx_undo_report_row_operation btr_cur_ins_lock_and_undo btr_cur_optimistic_insert row_ins_clust_index_entry_low /* mtr_2 提交后开始执行 insert 操作 page_cur_insert_rec_low 具体执行 insert 操作 在该函数末尾调用 page_cur_insert_rec_write_log 写 redo log */ page_cur_insert_rec_write_log(mtr_1) // insert 操作写 redo log page_cur_insert_rec_lowpage_cur_tuple_insert btr_cur_optimistic_insert mtr_commit(mtr_1) // 提交 mtr_1 row_ins_clust_index_entry_low 事务提交时也会涉及mini transaction：\n提交事务时，第一个 mtr 从 trx_prepare 开始 mtr_start(mtr_4) // mtr_4 用于 prepare transaction trx_prepare trx_prepare_for_mysql innobase_xa_prepare ha_prepare_low MYSQL_BIN_LOG::prepare ha_commit_trans trans_commit_stmt mysql_execute_command mtr_memo_push(mtr_4) // undo page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_page_get trx_undo_set_state_at_prepare trx_prepare mlog_write_ulint(seg_hdr + TRX_UNDO_STATE, undo-\u0026gt;state, MLOG_2BYTES, mtr_4) 写入TRX_UNDO_STATE trx_undo_set_state_at_prepare trx_prepare mlog_write_ulint(undo_header + TRX_UNDO_XID_EXISTS, TRUE, MLOG_1BYTE, mtr_4) 写入 TRX_UNDO_XID_EXISTS trx_undo_set_state_at_prepare trx_prepare trx_undo_write_xid(undo_header, \u0026amp;undo-\u0026gt;xid, mtr_4) undo 写入 xid trx_undo_set_state_at_prepare trx_prepare mtr_commit(mtr_4) // 提交 mtr_4 trx_prepare mtr_start(mtr_5) // mtr_5 用于 commit transaction trx_commit trx_commit_for_mysql innobase_commit_low innobase_commit ha_commit_low MYSQL_BIN_LOG::process_commit_stage_queue MYSQL_BIN_LOG::ordered_commit MYSQL_BIN_LOG::commit ha_commit_trans trans_commit_stmt mysql_execute_command mtr_memo_push(mtr_5) // undo page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_undo_page_get trx_undo_set_state_at_finish trx_write_serialisation_history trx_commit_low trx_commit trx_undo_set_state_at_finish(mtr_5) // set undo state， 这里是 TRX_UNDO_CACHED trx_write_serialisation_history trx_commit_low trx_commit mtr_memo_push(mtr_5) // 系统表空间 transaction system header page 加 RW_X_LATCH 入栈 buf_page_get_gen trx_sysf_get trx_sys_update_mysql_binlog_offset trx_write_serialisation_history trx_commit_low trx_commit trx_sys_update_mysql_binlog_offset // 更新偏移量信息到系统表空间 trx_write_serialisation_history trx_commit_low trx_commit mtr_commit(mtr_5) // 提交 mtr_5 trx_commit_low trx_commit 至此insert语句涉及的mini transaction全部结束。\nlogging schema #  Physiological Logging的设计思想是：physical-to-a-page, logical-within-a-page（对页是物理的，页内部的操作是逻辑的），其兼顾了物理日志和逻辑日志的优点。\n采用Physiological Logging，意味着既要描述物理信息，也要描述逻辑信息。我们在page那章已经给出了记录在insert、delete下的日志格式。从格式上可以看出，物理信息用space_id+page_no+offset表示，逻辑信息则根据op的不同，因操作而异。换句话说，以page为单位记录变更位置，在page内以逻辑的方式记录具体的数据变化。\n即logging schema如下：\nredo log entry：(op, space_id, page_no, logical description for data) mini-transaction：redo log entry sequence \u0026lt; 1 ... n \u0026gt; // 先保存在private repo中，在mtr commit中合入全局 redo log buffer // 体现内存更改序（提交）：mini-transaction sequence\u0026lt;1 ... n\u0026gt; 同样，由于Physiological Logging的方式，需要解决2个问题：\n 需要基于正确的page状态上re-play redo log 由于在一个page内，REDO是以逻辑的方式记录了前后两次的修改，因此重放redo log必须基于正确的Page状态，也就是page要保证是完整的。然而InnoDB的page是16KB，大于文件系统能保证的原子写粒度（4KB），可能出现partial-write，因此，InnoDB通过shadow page的方式提供了double-write机制来保证partial-write出现时可以通过recovery恢复到正确的page状态。 需要保证redo log re-play的幂等 因为InnoDB的buffer pool policy采用了STEAL+NO_FORCE，因此可能有未提交的事务的redo log flush到了磁盘，在恢复时无需再次重放redo log；另外，可能在crash recovery过程中出现二次故障，这也需要redo log重放提供幂等的能立。因此，InnoDB通过LSN（Log Sequence Number）来维护全局点位，然后在page上也记录一个LSN（page LSN），在恢复时，通过笔记redo log LSN是否小于page LSN来判定是否需要在page上apply redo log，从而实现redo log重放的幂等。  在数据库系统中，redo log可以有以下几种实现方式：\n 物理日志 逻辑日志 物理逻辑日志  物理日志（physical logging）保存一个页中发生改变的字节，也称这种方式为old value-new value logging。通常来说，其数据结构可参考下面的实现：\nstruct value_log{ int opcode; long page_no; long offset; long length; char old_value[length]; char new_value[length]; } 物理日志的好处是：其保存的是页中发生变化的字节。这样重复多次执行该日志不会导致数据发生不一致的问题，这也就意味着该日志是幂等的。然而物理日志最大的问题是，其日志产生的量相对较大。例如对一个页进行重新整理（reorganize），那么这时的日志大小可能就为页的大小。如果一个操作涉及对多个页的修改，那么需要分别对不同页进行记录。\n逻辑日志记录的是对于表的操作，这非常类似于MySQL数据库上层产生的二进制日志。由于是逻辑的，因此其日志的尺寸非常小。例如对于插入操作，仅需类似如下的格式：\n\u0026lt;insert op, table name, record value\u0026gt; UNDO操作仅需对记录的日志操作进行逆操作。例如INSERT对应DELETE操作，DELETE对应INSERT操作。然而该日志的缺点同样非常明显，那就是在恢复时可能无法保证数据的一致性。例如当对表进行插入操作时，表上还有其他辅助索引。当操作未全部完成时系统发生了宕机，那么要回滚上述操作可能很困难。因为，这时数据可能处在一个未知的状态。无法保证回滚之后数据的一致性。\nLSN #  LSN是log sequence number的缩写，代表的是日志序列号，具有单调递增的属性。LSN贯穿整个redo log的生命周期，从mini-transaction提交开始，到redo log buffer及其log block的组织，以及外村中redo log file中的点位，以及page LSN，直至checkpoint。\n在InnoDB中，LSN分配8个字节，具体定义为：\n# log0log.h /* Type used for all log sequence number storage and arithmetics */ typedef ib_uint64_t lsn_t; #define LSN_MAX IB_UINT64_MAX # univ.i /** Maximum value for ib_uint64_t */ #define IB_UINT64_MAX ((ib_uint64_t) (~0ULL)) 在MySQL 5.6.3之后LSN从4个字节扩充为8个字节，随之redo log file group的大小也从4GB扩大到512GB。  在InnoDB中，LSN的初始值由LOG_START_LSN定义：\n/* The counting of lsn's starts from this value: this must be non-zero */ #define LOG_START_LSN ((lsn_t) (16 * OS_FILE_LOG_BLOCK_SIZE)) // 初始化为MAX（这样初始分配从0开始），使用区间是0 ~ 16 * OS_FILE_LOG_BLOCK_SIZE 上面已经提到，LSN贯穿整个redo log的生命周期，其存在于多个对象中，表示的含义各不相同：\n redo log checkpoint page  LSN表示事务写入到redo log的字节量。例如当前redo log的LSN为1000，事务T1写入了100个字节的redo log，那么LSN就变为1100，如果事务T2又写入了200字节的redo log，那么LSN变为1300。由于redo log有内存和外存两种状态，因此会记录两个点位：\n 写入到redo log buffer（log_sys→lsn）的点位 写入redo log file（log_sys→flushed_to_disk_lsn）的点位  LSN还记录在page中，表示page持久化的日志点位，即page LSN。redo log依据page LSN来判断该page是否需要进行恢复操作\ncheckpoint也通过LSN的形式来保存，其表示页已经刷新到磁盘的LSN位置，当数据库重启时，仅需从checkpoint开始进行恢复操作。若checkpoint LSN与redo log file LSN相同，表示所有page都已经刷新到磁盘，不需要进行恢复操作。\nshow engine innodb status --- LOG --- Log sequence number 256862524 // redo log buffer LSN Log flushed up to 256810600 // redo log file LSN Pages flushed up to 256051783 // 最老的dirty page的LSN Last checkpoint at 256050987 // checkpoint LSN，即脏页刷新到磁盘的LSN 内部的数据结构表示和更新时机： log_sys-\u0026gt;lsn // mtr刷入redo log buffer时更新（mtr_commit()） log_sys-\u0026gt;flushed_to_disk_lsn // redo log buffer刷入redo log file时更新（log_write_up_to()） flush_list-\u0026gt;buf_page_t-\u0026gt;oldest_modification // 取自所有buffer_pool里flush_list oldest_modification最小的，buf_page_t-\u0026gt;oldest_modification在页加入到flush list时赋值 log_sys-\u0026gt;last_checkpoint_lsn); // 写入checkpoint后更新（log_checkpoint()） 除此以外，页也有LSN信息： FIL_PAGE_LSN：页最后flush时的LSN redo log layout #  从上面可以看出，redo log分为内存态的数据和外存态的数据。\n在内存中，redo log在mini-transaciton中产生，以stack的结构保存在各个用户线程自己的内存空间中。然后再mini-trnasaction commit时，合并到全局的redo log buffer中，由一个个redo log record组成。\n这一层称为逻辑redo log层。\n外村中的redo log分为两层，最下层是以文件方式存在（redo log file），块设备（文件）中块的粒度就是redo log block（512 bytes），这是为了保证日志的原子写。这层称为redo log文件层。\n为了避免创建文件以及初始化空间、预防文件自增带来的开销，redo log file最常见的一种组织方式是提供n个redo log file首尾相接一个逻辑的文件（ring file），作为redo log表空间，这层称为redo log buffer层。\n内存和外存的一致性通过force log at commit机制保证的。\n如下图所示：在逻辑redo log层用全局唯一递增的SN表示，并在日志子系统中维护当前SN的最大值（log_sys→lsn）\n而在物理层（buffer \u0026amp; 文件）中，抽象出了为了IO读写的log block，并且为了维护log block的元信息（log block header \u0026amp; tailer），在物理层用LSN标识。\nLSN和SN的换算关系为：\nconstexpr inline lsn_t log_translate_sn_to_lsn(lsn_t sn) { return (sn / LOG_BLOCK_DATA_SIZE * OS_FILE_LOG_BLOCK_SIZE + sn % LOG_BLOCK_DATA_SIZE + LOG_BLOCK_HDR_SIZE); } SN加上之前所有的block的header以及tailer的长度就可以换算到对应的LSN，反之亦然。\n而在文件层中，InnoDB在每个文件的开头固定预留4个block来记录一些额外的信息，其中第一个block称为header block，之后的3个block在0号文件上用来存储duplex checkpoint信息，而在其他文件上留空。如下图所示：逻辑redo是真正需要的数据，用SN索引，逻辑redo按固定大小的block组织，并添加block的头尾信息形成物理redo，以LSN索引，这些block又会放到循环使用的文件空间中的某一位置，文件中用offset索引。\n从中可以看出，逻辑redo log层时真正的日志数据，用SN索引；逻辑redo log按固定大小的log block组织在一起，并添加block header+tailer形成物理redo log，用LSN索引；而这些log block又会由循环使用的日志表空间中，在文件中用offset索引，而offset对应的的文件的读写IO。\noffset和LSN的转换关系如下：\nconst auto real_offset = log.current_file_real_offset + (lsn - log.current_file_lsn); 切换文件时会在内存中更新当前文件开头的文件offset，current_file_real_offset，以及对应的LSN，current_file_lsn，通过这两个值可以方便地用上面的方式将LSN转化为文件offset。注意这里的offset是相当于整个redo文件空间而言的，由于InnoDB中读写文件的space层实现支持多个文件，因此，可以将首位相连的多个redo文件看成一个大文件，那么这里的offset就是这个大文件中的偏移。\nredo log buffer #  redo log buffer的大小由参数innodb_log_buffer_size控制，默认大小为16MB。\n从整体上看，redo log buffer可以看作是由redo log block组成的一个线性数组，每个数组尾头存储数组元素的元信息。\nredo log block header中各个字段的含义：\n   字段 大小 说明     LOG_BLOCK_HDR_NO 4 block number，也就是当前log block在redo log buffer（线性数组）中的位置，最高的1个bit用于标识是否flush过，所以整个线性数组的容量是2G (2^（8*4-1） = 2G)   LOG_BLOCK_HDR_DATA_LEN 2 当前log block已写入了多少字节，如果为0x200则已写满（0x200 = 512）   LOG_BLOCK_FIRST_REC_GROUP 2 当前log block中第一个mtr log record的偏移量   LOG_BLOCK_CHECKPOINT_NO 4 log_sys-\u0026gt;next_checkpoint_no的低4位，恢复时使用（从CP1/CP2上读取CP后，CP后的redo log block上的该字段都应该大于\u0026gt;CP）    redo log block tailer中各个字段的含义：\n   字段 大小 说明     LOG_BLOCK_CHECKSUM 4 该log block的checksum，用于recovery时检测log block是否损坏；在3.23.52之前为LOG_BLOCK_HDR_NO    我们以一个具体的例子来说明redo log block的组织：有两个事务T1和T2的redo log写入redo log buffer，事务T1的redo log，也就是mtr log record，为1254字节，下图标记为青色，事务T2的mtr log record为100字节，下图标记为黄色。T1需要3个log block才能盛下，T2小于492字节，且T1写入后第3个log block的剩余空间大于100字节，所以放到第三个log block中。那么在这种情况下，我们来看一下头尾的metadata是如何表示的：\n LOG_BLOCK_HDR_NO：从第一个log block到第四个log block的编号依次为0、512、1024、1536，换算成16进制为0x0000、0x0200、0x0400、0x0600 LOG_BLOCK_HDR_DATA_LEN：第一个和第二个log block都用完了，所以为512字节（0x0200），第三个块写入了370字节（270+100，0x172），第四个快，否则记录真实的字节数，未使用记录为0（0x0000） LOG_BLOCK_FIRST_REC_GROUP：记录该log block中第一个mtr log record的相对偏移量，对于事务T1，第一个log block相对于该log block的偏移量为12（0x000C），第二个log block用满了492字节，且实际相对于第一个log block数据末尾的偏移量为8+12+492=512（0x0200），即LOG_BLOCK_FIRST_REC_GROUP=LOG_BLOCK_HDR_DATA_LEN，可以根据该公式判断该log block是否有新的mtr log record（即该块为上一个mtr log record的延续），在第三个log block中，T1写入了270就结束了（1254-492-492），事务T2（下图标记为橙色）的mtr log record为该log block的第一个记录，其相对于该log block的偏移量为282（270+12，0x011A），第四块log block没有存放任何mtr log record，为0。  if (log_block_get_flush_bit(log_block)) { /* This block was a start of a log flush operation: we know that the previous flush operation must have been completed for all log groups before this block can have been flushed to any of the groups. Therefore, we know that log data is contiguous up to scanned_lsn in all non-corrupt log groups. */ if (scanned_lsn \u0026gt; *contiguous_lsn) { *contiguous_lsn = scanned_lsn; } } data_len = log_block_get_data_len(log_block); if (scanned_lsn + data_len \u0026gt; recv_sys-\u0026gt;scanned_lsn \u0026amp;\u0026amp; log_block_get_checkpoint_no(log_block) \u0026lt; recv_sys-\u0026gt;scanned_checkpoint_no \u0026amp;\u0026amp; (recv_sys-\u0026gt;scanned_checkpoint_no - log_block_get_checkpoint_no(log_block) \u0026gt; 0x80000000UL)) { /* Garbage from a log buffer flush which was made before the most recent database recovery */ finished = true; break; } 数据组织为下图所示：\nredo log files #  redo log日志表空间中的每个redo log file头部会预留2KB（LOG_FILE_HDR_SIZE = (4 * OS_FILE_LOG_BLOCK_SIZE)）信息用于存储文件的元信息（但只有文件0使用），其余部分用于存储log block：\n   存储内容 大小     log file header 512   checkpoint 1 512   / 512   checkpoint 2 512    如下图所示：\n从这里可以看到，redo log file的写入并不完全是顺序IO的，因为在将redo log buffer sync到文件block（append only）后，还需要更新头部的checkpoint信息。\nlog file header内容如下：\n   字段 大小 说明     LOG_HEADER_FORMAT 4 redo log格式版本，目前是v1   LOG_HEADER_START_LSN 8 redo重做日志文件中的第一个日志的lsn   LOG_HEADER_CREATOR 16 MySQL版本版本信息，比如\u0026quot;MySQL 5.7.26\u0026quot;    checkpoint值 #  checkpoint是crash recovery的关键路径（key point），要最大程度上的保证checkpoint的可用性。因此，设计了两个checkpoint的目的是采用交替写入来避免介质失败。\ncheckpoint预留了512字节，但实际上使用了32字节：\n   字段 大小 说明     LOG_CHECKPOINT_NO 8 checkpoint NO，单调递增，每次做完checkpoint+1   LOG_CHECKPOINT_LSN 8 checkpoint的LSN   LOG_CHECKPOINT_OFFSET 8 checkpoint的LSN对应的在redo log file中的偏移量   LOG_CHECKPOINT_LOG_BUF_SIZE 8 做checkpoint时，redo log buffer的大小，无实际意义    在重启恢复时，只需要恢复LOG_CHECKPOINT_LSN后面的redo log。由于有两个checkpoint，重启时读取两个，采用较大的LOG_CHECKPOINT_LSN。\nsync point \u0026amp; commit point #  为了保证redo log buffer刷入redo log file的持久性，每次都要fsync。这是因为InnoDB打开redo log file时并没有使用direct IO（O_DIRECT），所以为了确保数据写入到磁盘上，需要使用fsync的方式保证synchronized IO file integrity completion。而fsync的效率取决于磁盘的性能，而fsync的效率决定了事务提交的性能，也就是写入性能。\nsync的时机如下：\n 事务commit时 写入checkpoint时 当log buffer中有已使用空间超过某个阈值时  另外，配置选项innodb_flush_log_at_trx_commit和 innodb_flush_method分别控制着**何时（when）以及如何（how）**对redo log file进行sync。\ninnodb_flush_log_at_trx_commit #  InnoDB允许用户通过innodb_flush_log_at_trx_commit参数控制redo log buffer写入和sync的时机，即sync point。\ndefault：1\ninnodb_flush_log_at_trx_commit有3个选项，可以根据速度和安全的不同要求加以选择。\n 0：把redo log file每隔1s写入文件，但sync由操作系统控制。换句话说，0不能确保ACID中的D 1：每次事务commit写入文件并sync 2：折衷，每次事务commit写入文件，每隔1s sync  如下图所示：\nDDL changes and other internal InnoDB activities flush the log independently of the innodb_flush_log_at_trx_commit setting.  MySQL · 参数故事 · innodb_flush_log_at_trx_commit 摘自阿里数据库内核月报\n背景\ninnodb_flush_log_at_trx_commit 这个参数可以说是InnoDB里面最重要的参数之一，它控制了重做日志（redo log）的写盘和落盘策略。\n简单说来，可选值的安全性从0-\u0026gt;2-\u0026gt;1递增，分别对应于mysqld 进程crash可能丢失 -\u0026gt; OS crash可能丢失 -\u0026gt; 事务安全。\n以上是路人皆知的故事，并且似乎板上钉钉，无可八卦……\ninnodb_use_global_flush_log_at_trx_commit\n直到2010年的某一天，Percona的CTO Vadim同学觉得这种一刀切的风格不够灵活，最好把这个变量设置成session级别，每个session自己控制。\n但同时为了保持Super权限对提交行为的控制，同时增加了innodb_use_global_flush_log_at_trx_commit参数。 这两个参数的配合逻辑为：\n　1、若innodb_use_global_flush_log_at_trx_commit为OFF，则使用session.innodb_flush_log_at_trx_commit;\n　2、若innodb_use_global_flush_log_at_trx_commit为ON,则使用global .innodb_flush_log_at_trx_commit（此时session中仍能设置，但无效）\n　3、每个session新建时，以当前的global.innodb_flush_log_at_trx_commit 为默认值。\n业务应用\n这个特性可以用在一些对表的重要性做等级定义的场景。比如同一个实例下，某些表数据有外部数据备份，或允许丢失部分事务的情况，对这些表的更新，可以设置 Session.innodb_flush_log_at_trx_commit为非1值。\n在阿里云RDS服务中，我们对数据可靠性和可用性要求更高，将 innodb_use_global_flush_log_at_trx_commit设置为ON，因此修改session.innodb_flush_log_at_trx_commit也没有作用，统一使用 global.innodb_flush_log_at_trx_commit = 1。\n  O_SYNC: requires that any write operations block until all data and all metadata have been written to persistent storage. O_DSYNC: like O_SYNC, except that there is no requirement to wait for any metadata changes which are not necessary to read the just-written data. In practice, O_DSYNC means that the application does not need to wait until ancillary information (the file modification time, for example) has been written to disk. Using O_DSYNC instead of O_SYNC can often eliminate the need to flush the file inode on a write. O_RSYNC: this flag, which only affects read operations, must be used in combination with either O_SYNC or O_DSYNC. It will cause aread() call to block until the data (and maybe metadata) being read has been flushed to disk (if necessary). This flag thus gives the kernel the option of delaying the flushing of data to disk; any number of writes can happen, but data need not be flushed until the application reads it back.   innodb_flush_method #  innodb_flush_method选项为InnoDB数据文件和日志文件的同步策略：\n fsync：默认选项，数据文件和日志文件都使用deafult打开，都通过fsync确保写入成功 O_DIRECT：使用O_DIRECT打开数据文件，使用default打开日志文件，都通过fsync来确保写入成功  fsync函数只对由文件描述符filedes指定的单一文件起作用，并且等待写磁盘操作结束，然后返回。\nopen时的参数O_SYNC有着和fsync类似的语义：使每次write都会阻塞等待硬盘IO完成，因为InnoDB在打开数据文件和日志文件时没有指定O_SYNC，所以需要显式调用fsync已确保元信息和数据刷入磁盘。\nbinlog sync #  在MySQL Server层还有binlog日志，其用来进行point-in-time（PIT）恢复以及在节点间（主从）进行复制。\nMySQL本身通过2PC原子协议保证数据提交时redo log file和binlog file的一致。\nMySQL也有参数提供了何时sync（when）：sync_binlog\ncheckpoint #  从上面我们知道，InnoDB存储引擎为了事务的D，采用write ahead log（WAL）。后续数据页（dirty page）的flush大多数情况下是异步进行的，最低水位线由checkpoint负责，即checkpoint保证其LSN点位之前的所有dirty page都必须持久化到磁盘上。这样，在crash recovery时，只需从checkpoint开始进行redo log apply恢复到内存态即可。\n关于checkpoint的两种方式（sharp \u0026amp; fuzzy），在buffer pool的page flush章节，已经做过介绍，在此不再赘述。\nMySQL 8.0优化（转） #  本节内容转自网络及阿里内核月报。\nWAL流程优化 #  REDO产生 #  事务在写入数据的时候会产生REDO，一次原子的操作可能会包含多条REDO记录，这些REDO可能是访问同一Page的不同位置，也可能是访问不同的Page（如Btree节点分裂）。InnoDB有一套完整的机制来保证涉及一次原子操作的多条REDO记录原子，即恢复的时候要么全部重放，要不全部不重放，这部分将在之后介绍恢复逻辑的时候详细介绍，本文只涉及其中最基本的要求，就是这些REDO必须连续。InnoDB中通过min-transaction实现，简称mtr，需要原子操作时，调用mtr_start生成一个mtr，mtr中会维护一个动态增长的m_log，这是一个动态分配的内存空间，将这个原子操作需要写的所有REDO先写到这个m_log中，当原子操作结束后，调用mtr_commit将m_log中的数据拷贝到InnoDB的Log Buffer。\n写入Log Buffer #  从MySQL 8.0开始，设计了一套无锁的写log机制，其核心思路是允许不同的mtr，同时并发地写Log Buffer的不同位置。不同的mtr会首先调用log_buffer_reserve函数，这个函数里会用自己的REDO长度，原子地对全局偏移log.sn做fetch_add，得到自己在Log Buffer中独享的空间。之后不同mtr并行的将自己的m_log中的数据拷贝到各自独享的空间内。\n/* Reserve space in sequence of data bytes: */ const sn_t start_sn = log.sn.fetch_add(len); 写入Page Cache #  写入到Log Buffer中的REDO数据需要进一步写入操作系统的Page Cache，InnoDB中有单独的log_writer来做这件事情。这里有个问题，由于Log Buffer中的数据是不同mtr并发写入的，这个过程中Log Buffer中是有空洞的，因此log_writer需要感知当前Log Buffer中连续日志的末尾，将连续日志通过pwrite系统调用写入操作系统Page Cache。整个过程中应尽可能不影响后续mtr进行数据拷贝，InnoDB在这里引入一个叫做link_buf的数据结构，如下图所示：\nlink_buf是一个循环使用的数组，对每个lsn取模可以得到其在link_buf上的一个槽位，在这个槽位中记录REDO长度。另外一个线程从开始遍历这个link_buf，通过槽位中的长度可以找到这条REDO的结尾位置，一直遍历到下一位置为0的位置，可以认为之后的REDO有空洞，而之前已经连续，这个位置叫做link_buf的tail。下面看看log_writer和众多mtr是如何利用这个link_buf数据结构的。这里的这个link_buf为log.recent_written，如下图所示：\n图中上半部分是REDO日志示意图，write_lsn是当前log_writer已经写入到Page Cache中日志末尾，current_lsn是当前已经分配给mtr的的最大lsn位置，而buf_ready_for_write_lsn是当前log_writer找到的Log Buffer中已经连续的日志结尾，从write_lsn到buf_ready_for_write_lsn是下一次log_writer可以连续调用pwrite写入Page Cache的范围，而从buf_ready_for_write_lsn到current_lsn是当前mtr正在并发写Log Buffer的范围。下面的连续方格便是log.recent_written的数据结构，可以看出由于中间的两个全零的空洞导致buf_ready_for_write_lsn无法继续推进，接下来，假如reserve到中间第一个空洞的mtr也完成了写Log Buffer，并更新了log.recent_written*，如下图：\n这时，log_writer从当前的buf_ready_for_write_lsn向后遍历log.recent_written，发现这段已经连续：\n因此提升当前的buf_ready_for_write_lsn，并将log.recent_written的tail位置向前滑动，之后的位置清零，供之后循环复用：\n紧接log_writer将连续的内容刷盘并提升write_lsn。\n刷盘 #  log_writer提升write_lsn之后会通知log_flusher线程，log_flusher线程会调用fsync将REDO刷盘，至此完成了REDO完整的写入过程。\n唤醒用户线程 #  为了保证数据正确，只有REDO写完后事务才可以commit，因此在REDO写入的过程中，大量的用户线程会block等待，直到自己的最后一条日志结束写入\n大量的用户线程调用log_write_up_to等待在自己的lsn位置，为了避免大量无效的唤醒，InnoDB将阻塞的条件变量拆分为多个，log_write_up_to根据自己需要等待的lsn所在的block取模对应到不同的条件变量上去。同时，为了避免大量的唤醒工作影响log_writer或log_flusher线程，InnoDB中引入了两个专门负责唤醒用户的线程：log_wirte_notifier和log_flush_notifier，当超过一个条件变量需要被唤醒时，log_writer和log_flusher会通知这两个线程完成唤醒工作。下图是整个过程的示意图：\n多个线程通过一些内部数据结构的辅助，完成了高效的从REDO产生，到REDO写盘，再到唤醒用户线程的流程，下面是整个这个过程的时序图：\n推进checkpoint #  我们知道REDO的作用是避免只写了内存的数据由于故障丢失，那么打Checkpiont的位置就必须保证之前所有REDO所产生的内存脏页都已经刷盘。最直接的，可以从Buffer Pool中获得当前所有脏页对应的最小REDO LSN：lwm_lsn。 但光有这个还不够，因为有一部分min-transaction的REDO对应的Page还没有来的及加入到Buffer Pool的脏页中去，如果checkpoint打到这些REDO的后边，一旦这时发生故障恢复，这部分数据将丢失，因此还需要知道当前已经加入到Buffer Pool的REDO lsn位置：dpa_lsn。取二者的较小值作为最终checkpoint的位置，其核心逻辑如下：\n/* LWM lsn for unflushed dirty pages in Buffer Pool */ lsn_t lwm_lsn = buf_pool_get_oldest_modification_lwm(); /* Note lsn up to which all dirty pages have already been added into Buffer Pool */ const lsn_t dpa_lsn = log_buffer_dirty_pages_added_up_to_lsn(log); lsn_t checkpoint_lsn = std::min(lwm_lsn, dpa_lsn); 为了去掉flush_order_mutex，就需要允许redo log buffer中对应的脏页无序（并发）添加到flush list，即用户写完redo log buffer，就可以把自己的dirty page(s)添加到flush list而无需抢锁。但是这种方法会造成做checkpoint的时候，无法保证flush list最上面的page lsn是最小的，于是引入了log_closer线程定期检查recent_closed（link_buf）是否连续，如果连续就把 recent_closed buffer 向前推进（提升recent_closed的tail，也就是buffer pool脏页的最大LSN（pda_lsn），类似log.recent_written和log_writer），那么checkpoint 的信息也可以往前推进了。需要注意的是，由于这种乱序的存在，lwm_lsn的值并不能简单的获取当前Buffer Pool中的最老的脏页的LSN，保守起见，还需要减掉一个recent_closed的容量大小，也就是最大的乱序范围，简化后的代码如下：\n/* LWM lsn for unflushed dirty pages in Buffer Pool */ const lsn_t lsn = buf_pool_get_oldest_modification_approx(); const lsn_t lag = log.recent_closed.capacity(); lsn_t lwm_lsn = lsn - lag; /* Note lsn up to which all dirty pages have already been added into Buffer Pool */ const lsn_t dpa_lsn = log_buffer_dirty_pages_added_up_to_lsn(log); lsn_t checkpoint_lsn = std::min(lwm_lsn, dpa_lsn); 这里有一个问题，由于lwm_lsn已经减去了recent_closed的capacity，因此理论上这个值一定是小于dpa_lsn的。那么再去比较lwm_lsn和dpa_lsn来获取Checkpoint位置或许是没有意义的。\n在buf_pool_get_oldest_modification_lwm() 还是里面, 会将buf_pool_get_oldest_modification_approx() 获得的 lsn 减去recent_closed buffer 的大小, 这样得到的lsn 可以确保是可以打checkpoint 的, 但是这个lsn 不能保证是最大的可以打checkpoint 的lsn. 而且这个 lsn 不一定是指向一个记录的开始, 更多的时候是指向一个记录的中间, 因为这里会强行减去一个 recent_closed buffer 的size. 而以前在5.6 版本是能够保证这个lsn 是默认一个redo log 的record 的开始位置。\n基本思想是，脏页链表的有序性可以被部分的打破，也就是说，在一定范围内可以无序，但是整体还是有序的。这个无序程度是受控的。假设脏页链表第一个数据页的oldest_modification为A, 在之前的版本中，这个脏页链表后续的page的oldest_modification都严格大于等于A，也就是不存在一个数据页比第一个数据页还老。在MySQL 8.0中，后续的page的oldest_modification并不是严格大于等于A，可以比A小，但是必须大于等于A-L，这个L可以理解为无序度，是一个定值。那么问题来了，如果脏页链表顺序乱了，那么checkpoint怎么确定，或者说是，奔溃恢复后，从哪个checkpoint_lsn开始扫描日志才能保证数据不丢。官方给出的解法是，checkpoint依然由脏页链表中第一个数据页的oldest_modification的确定，但是奔溃恢复从checkpoint_lsn-L开始扫描(有可能这个值不是一个mtr的边界，因此需要调整)。\n参考资料\nMySQL 8.0.11Source Code Documentation: Format of redo log\nMySQL 8.0: New Lock free, scalable WAL design\nHow InnoDB handles REDO logging\nLinkBuf #  在MySQL8.0中增加了一个新的无锁数据结构Link_buf，主要用于redo log buffer以及buffer pool的flush list。\n这个数据结构简单来看就是一个拥有固定大小的数组，而对于InnoDB使用来说里面保存的就是写入redo log buffer或者加入到flush list的数据的大小，数组的每个元素可以被原子的更新。\n由于在MySQL 8.0中redo log buffer会有空洞，因此linkbuf用来track当前log buffer的写入情况，也就是说每次写入的数据大小都会保存在linkbuf中，而每次写入的位置通过start lsn来得到（hash），假设有空洞（某些lsn还没有写入），那么其对应在linkbuf中的值就是0，这样就可以很简单的track空洞。\n最后要注意的是这个数据结构的前提就是LSN是一直增长且不会重复的，因此在InnoDB中只在redo log和flush list（oldest LSN序）中使用。\n源码分析\n无锁ring buffer数据结构\ntemplate \u0026lt;typename Position = uint64_t\u0026gt; class Link_buf { public: typedef Position Distance; // 累计保存 ..................................... */** Capacity of the buffer. */* size_t m_capacity; // linkbuf的大小 */** Pointer to the ring buffer (unaligned). */* std::atomic\u0026lt;Distance\u0026gt; *m_links; // 保存的内容（动态数组，元素是每次写入的长度） */** Tail pointer in the buffer (expressed in original unit). */* alignas(INNOBASE_CACHE_LINE_SIZE) std::atomic\u0026lt;Position\u0026gt; m_tail; // 目前ring buffer的尾部（即第一个空洞的位置，保证之前都是连续的） }; ctor\n根据传递进来的capacity,创建对应大小的数组(m_links)，然后初始化数组的内容（每个slot设为0）\ntemplate \u0026lt;typename Position\u0026gt; Link_buf\u0026lt;Position\u0026gt;::Link_buf(size_t capacity) : m_capacity(capacity), m_tail(0) { if (capacity == 0) { m_links = nullptr; return; } ut_a((capacity \u0026amp; (capacity - 1)) == 0); m_links = UT_NEW_ARRAY_NOKEY(std::atomic\u0026lt;Distance\u0026gt;, capacity); for (size_t i = 0; i \u0026lt; capacity; ++i) { m_links[i].store(0); } } 添加\nadd_link函数保存这次的写入信息(mlog)：计算写入的起始点+长度（按照长度计算slot，slot = hash(len)）\ntemplate \u0026lt;typename Position\u0026gt; inline void Link_buf\u0026lt;Position\u0026gt;::add_link(Position from, Position to) { ut_ad(to \u0026gt; from); ut_ad(to - from \u0026lt;= std::numeric_limits\u0026lt;Distance\u0026gt;::max()); const auto index = slot_index(from); auto \u0026amp;slot = m_links[index]; ut_ad(slot.load() == 0); slot.store(to - from); } 计算slot\n计算方式很简单，起始点和数组的大小取模：\ntemplate \u0026lt;typename Position\u0026gt; inline size_t Link_buf\u0026lt;Position\u0026gt;::slot_index(Position position) const { return position \u0026amp; (m_capacity - 1); } 判断空间\nhas_space函数就是用来判断对应的position是否已经被占据：插入点是否大于尾部+整个空间大小是否overlap（贪吃蛇）\ntemplate \u0026lt;typename Position\u0026gt; inline bool Link_buf\u0026lt;Position\u0026gt;::has_space(Position position) const { return tail() + m_capacity \u0026gt; position; } 更新尾部\ntemplate \u0026lt;typename Position\u0026gt; template \u0026lt;typename Stop_condition\u0026gt; bool Link_buf\u0026lt;Position\u0026gt;::advance_tail_until(Stop_condition stop_condition) { auto position = m_tail.load(); while (true) { Position next; bool stop = next_position(position, next); if (stop || stop_condition(position, next)) { break; } */* Reclaim the slot. */* claim_position(position); position = next; } if (position \u0026gt; m_tail.load()) { m_tail.store(position); return true; } else { return false; } } 读取position位置的slot内容（数据长度），返回下一个位置（next），并判断是否达到空洞（slot未设置，distince == 0）\ntemplate \u0026lt;typename Position\u0026gt; bool Link_buf\u0026lt;Position\u0026gt;::next_position(Position *position*, Position \u0026amp;*next*) { const auto index = slot_index(position); auto \u0026amp;slot = m_links[index]; const auto distance = slot.load(); ut_ad(position \u0026lt; std::numeric_limits\u0026lt;Position\u0026gt;::max() - distance); next = position + distance; return distance == 0; } "},{"id":34,"href":"/docs/MySQL/InnoDB/5_storage/","title":"storage","section":"Inno Db","content":"存储管理是数据库系统最基本的功能，本章将介绍InnoDB存储引擎中数据在外存中的组织形式（即数据文件）、数据文件在内存中的管理方式以及数据在文件层次的读/写。\n存储组织 #  存储管理需要解决以下2个问题：\n 如何在存储设备上表示和组织数据库中的数据 如何在内存和外存上控制数据的存取  一般来说，DBMS不直接使用操作系统提供的文件系统作为直接的存储，而是在文件系统之上封装了一层自己对于存储设备的管理，以保证数据的完整性和存取效率。目前大部分的文件系统即使提供了日志的支持，可以保证写的原子性，但是数据库的数据页可能大于文件系统中的block大小，仍然避免不了出现的半写（partial-write）问题，所以数据库也要解决半写问题。\n存储层次 #  为了解决以上两个问题，根据分层和抽象的思想，存储的组织和管理可以划分为3个层次：\n 文件存储（file storage） 页（page layout） 记录（tuple layout）  暂不讨论raw device，这里假设数据库的文件存放在操作系统提供的文件系统之上。  文件存储负责将文件系统上的一组物理文件进行逻辑组织，按照使用场景（日志、数据、临时数据\u0026hellip;）、以及数据IO特征的不同（日志是append-only的，数据可能是原地写/append-only），将物理文件进行划分，抽象出表空间，并形成统一的数据库层的逻辑文件子系统。\n根据局部性原理，为了I/O的高效，我们需要将数据聚簇并划块，这就形成了页，所以第二层可以理解为页的集合，负责按照页为单位从外存、内存读写数据，并分配使用空间。\n对于用户来说，操作的是数据，也就是记录的集合，那么在微观上需要对记录进行操作。这也是最后一层，我们在这层实现数据的更改和查询。\n所以，存储引擎中内部存储单元（页）和操作系统、以及磁盘上的物理存储单元关系如下图所示：\n文件存储 #  文件存储负责将操作系统文件系统上的数据库相关物理文件组织在一起，形成数据库的逻辑文件子系统，整体结构如下图所示：\n从上图可以看到，不同的物理文件组成了多个表空间，然后一起形成file system子系统。官方表空间的描述在这里。\nInnoDB存储引擎对于文件的管理通过fil_system_t、fil_space_t、fil_node_t进行描述，并定义了四种文件空间（file space）类型：\n FIL_TYPE_TABLESPACE：持久表空间类型，包括系统表空间、独立表空间、undo表空间（innodb_system、user、innodb_undo\u0026lt;000\u0026gt;） FIL_TYPE_LOG：重做日志（ib_logfile\u0026lt;0~100\u0026gt;） FIL_TYPE_TEMPORARY：临时表空间（innodb_temporary） FIL_TYPE_IMPORT：只在导入表空间时使用（导入前为FIL_TYPE_IMPORT，导入完成后为FIL_TYPE_TABLESPACE）  每个文件空间可以包含若干个文件节点（file node）。file node是文件存储的最小单元。逻辑存储模块管理文件系统（file system）下的各个文件空间（file space），并对文件空间下的file node的读/写操作进行管理。\nfil_system_t、fil_space_t、fil_node_t的关系如图所示：\n在file system中的name：\n 在file_space→name为文件名（1）或者文件名统称（n） file_node→name为具体的文件路径+文件名  比如重做日志中的file_space→name和file_node分别为：innodb_redo_log和path/ib_logfile\u0026lt;0 ~ 100\u0026gt;。\n表空间 #  首先先让我们从用户和系统管理的角度来了解表空间的使用场景，以便对表空间有一个直观的认识。\n使用场景 #  在MySQL 5.7中，表空间按照使用场景分为五种：\n 系统表空间 独立表空间 通用表空间 undo表空间 临时表空间  系统表空间 #  系统表空间位于datadir下，存放了InnoDB存储引擎的核心信息，包括数据字典、事务系统信息、double write buffer、change buffer。如果没有配置独立的undo表空间，则undo日志也存放在这里（临时表空间也是如此）。同样，如果没有开启独立表空间（file-per-table），则用户表的数据和索引也存放在这里。因此，系统表空间也被称为共享表空间。\n系统表空间由多个ibdata*的物理文件组成。默认情况下只会创建一个ibdata1文件（初始最小12M），也支持配置多个文件。系统表空间支持自动增长，也就是说，当空间不够时，会自动进行文件的增长（默认每次增长8M，即4个区），可以通过innodb_autoextend_increment设置每次增长的大小。\n系统表空间的最大问题是空间无法收缩。因为其中存放了undo logs，如果遇到大事务，产生的undo logs会造成表空间的膨胀，即使purge undo log后空间也不会收缩（同样还有临时表空间）。同样的，用户表也无法收缩，即使删除了空间也无法释放。只能新建实例→导入数据这样的方式来重建表空间实现收缩。正是因为如此，MySQL随后划分出出独立的用户表空间、undo表空间和临时表空间。\n独立表空间 #  用户表可以存放在独立表空间中，也可以多个表一起存放在通用表空间中（MySQL 5.7引入）。\n独立表空间用于存放用户表的数据、索引和change buffer，物理文件创建在datadir/database/table_name.ibd。\n好处\n 可以把表创建在不同的磁盘上，充分利用不同设备的IO性能 可以使用Barracuda格式，即行格式可以为DYNAMIC/COMPRESSED 可以使用import tablespace直接导入ibd 使用共享表空间，且innodb_flush_method = O_DIRECT时，无法并发写入一个文件  缺点\n fsync在共享表空间下可以合并IO，独立表空间会增加fsync的个数 打开的文件句柄变多 独立表空间的增长每次为4MB，不受innodb_autoextend_increment参数的控制 在删除表时，会扫描buffer pool，这会影响其他操作的效率  通用表空间 #  因为上述独立表空间的缺点，于是由发展出来通用表空间来兼顾独立表空间和共享表空间的优点。通用表空间可以将多个表放置到一个表空间中。支持Antelope/Barracuda格式，同时，同一表空间下可以支持不同的格式和数据页大小。\nCREATE TABLESPACE `ts2` ADD DATAFILE 'ts2.ibd' FILE_BLOCK_SIZE = 8192 Engine=InnoDB; CREATE TABLE t4 (c1 INT PRIMARY KEY) TABLESPACE ts2 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8; 通用表空间不再局限于datadir，可以创建在数据目录以外。在这种情况下，会在datadir下创建一个.isl文件（软链接）来关联实际的ibd文件。\nundo表空间 #  undo log存储在undo段中，多个undo segments组成了undo表空间。\n在共享表空间中，只能创建一个undo表空间。而采用独立的undo表空间后，可以创建多个undo表空间（后面废弃了）。\n只能在MySQL install阶段开启undo表空间，这样才能用到MySQL 5.7引入的online undo truncate。\n首先来看一下undo表空间的相关参数：\n innodb_undo_directory，指定单独存放undo表空间的目录，默认为.（即datadir），可以设置相对路径或者绝对路径。该参数实例初始化之后虽然不可直接改动，但是可以通过先停库，修改配置文件，然后移动undo表空间文件的方式去修改该参数； innodb_undo_tablespaces，指定单独存放的undo表空间个数，例如如果设置为3，则undo表空间为undo001、undo002、undo003，每个文件初始大小默认为10M。该参数我们推荐设置为大于等于3，原因下文将解释。该参数实例初始化之后不可改动； innodb_undo_logs，指定回滚段的个数（早期版本该参数名字是innodb_rollback_segments），默认128个。每个回滚段可同时支持1024个在线事务。这些回滚段会平均分布到各个undo表空间中。该变量可以动态调整，但是物理上的回滚段不会减少，只是会控制用到的回滚段的个数。 innodb_undo_tablespaces\u0026gt;=2。因为truncate undo表空间时，该文件处于inactive状态，如果只有1个undo表空间，那么整个系统在此过程中将处于不可用状态。为了尽可能降低truncate对系统的影响，建议将该参数最少设置为3； innodb_undo_logs\u0026gt;=35（默认128）。因为在MySQL 5.7中，第一个undo log永远在系统表空间中，另外32个undo log分配给了临时表空间，即ibtmp1，至少还有2个undo log才能保证2个undo表空间中每个里面至少有1个undo log； innodb_max_undo_log_size，undo表空间文件超过此值即标记为可收缩，默认1G，可在线修改； innodb_purge_rseg_truncate_frequency,指定purge操作被唤起多少次之后才释放rollback segments。当undo表空间里面的rollback segments被释放时，undo表空间才会被truncate。由此可见，该参数越小，undo表空间被尝试truncate的频率越高。  参数可以设置如下：\ninnodb_max_undo_log_size = 100M innodb_undo_log_truncate = ON innodb_undo_logs = 128 innodb_undo_tablespaces = 3 innodb_purge_rseg_truncate_frequency = 10 临时表空间 #  用户创建的临时表和磁盘临时表都会造成共享表空间的膨胀，由此发展出临时表空间，所有非压缩的临时表都存放在这个表空间中。\n临时表空间支持自动增长，重启后，临时表空间会重新创建。\n对于云服务提供商而言，通过ibtmp文件，可以更好的控制临时文件产生的磁盘存储。\n设计 #  文件系统子系统对于整个的存储管理具有重要的意义，用于保证内存和外存的物理一致性。对于外存（即和文件系统的交互），涉及到文件IO（文件句柄、flush、）、数据一致性、性能；对于内存，涉及到快速查找、并发控制。更为重要和复杂的，则是文件内部数据的组织和管理。\nspace id的分配策略 #  space id的分配策略为：掐头去尾，中间动态分配的方式，并且目前未采用回收重用机制（space_id_reuse_warned = N/A）：\n   0 中间 SRV_LOG_SPACE_FIRST_ID     系统表空间 可分配 日志表空间（0xFFFFFFF0UL）    在InnoDB启动时，space id从系统表空间（space_id = 0的第0个文件的第7个页面，数据字典）中获取，其后在运行时动态产生：\ndict_hdr_get_new_id fil_assign_new_space_id 并且，系统表空间和日志表空间要保证始终打开，以避免死锁（mutex）。\n这是因为，在fil_io的调用中，首先需要持有文件系统的锁，如果在一个线程里，首先写日志，持有了，然后在insert buffer中，需要读取系统表空间，又试图获取锁，这种情况下会产生一个人获取两次，从而产生死锁。\nfil_mutex_enter_and_prepare_for_io if (space_id == 0 || space_id \u0026gt;= SRV_LOG_SPACE_FIRST_ID) { /* We keep log files and system tablespace files always open; this is important in preventing deadlocks in this module, as a page read completion often performs another read from the insert buffer. The insert buffer is in tablespace 0, and we cannot end up waiting in this function. */ return; } 文件空间类型 #  文件空间类型共有4种：\n/** temporary tablespace (temporary undo log or tables) */ FIL_TYPE_TEMPORARY, /** a tablespace that is being imported (no logging until finished) */ FIL_TYPE_IMPORT, /** persistent tablespace (for system, undo log or tables) */ FIL_TYPE_TABLESPACE, /** redo log covering changes to files of FIL_TYPE_TABLESPACE */ FIL_TYPE_LOG 其中除了FIL_TYPE_LOG都属于数据表空间，由函数fil_type_is_data判断。\ntablespace flags #  存储了表空间的元信息标识，比如对于行格式为compact/redundant，该值为0，对于compressed/dynamic，该值有效。\n对于undo page，除了页大小外，都是false。\n其中元信息如下：\n FSP_FLAGS_MASK_POST_ANTELOPE：使用的是antelope行格式 FSP_FLAGS_MASK_ZIP_SSIZE：压缩页的block size（0为表空间不采用压缩） FSP_FLAGS_MASK_ATOMIC_BLOBS：使用的是Compressed/Dynamic行格式 FSP_FLAGS_MASK_PAGE_SSIZE：page size FSP_FLAGS_MASK_DATA_DIR：显式指定了data_dir FSP_FLAGS_MASK_SHARED：是否为共享表空间 FSP_FLAGS_MASK_TEMPORARY：是否是临时表空间 FSP_FLAGS_MASK_ENCRYPTION：是否为加密表空间（MySQL 5.7.11引入）  快速查找 #  从最上面的图上我们可以快速进行以下查找：\n 通过file_system快速查找file_space by id/name：fil_system→spaces、fil_system→name_hash 通过file_system遍历所有的file_space：fil_system→space_list 通过file_system遍历IO操作要flush的file_space：fil_system-\u0026gt;unflushed_spaces 通过file_system遍历所有变化的file_space：fil_system→named_spaces 通过file_system遍历LRU列表：fil_system→LRU 通过file_space遍历所有的file_node：fil_space→chain  LRU #  频繁文件的句柄打开和关闭是高耗时操作，并且，句柄也是珍贵的资源，当一个文件节点上的操作完成时，该文件节点不会立即被关闭，而是加入到file_system的LRU链表中。\nnamed_spaces #  named_spaces用于记录在上一次CP后有哪些file_space修改过，并通过redo log（MLOG_FILE_NAME）记录下这些变化，max_lsn记录了其MLOG_FILE_NAME的lsn点位（每次mtr commit时，都推进max_lsn到其当时的log_sys→lsn）。\n如果在CP时还没有生成这部分信息（MLOG_FILE_NAME），需要在下一次CP时首先生成这部分信息，正常情况下在mtr prepare_write时已写入redo log buffer，同时file_space→max_lsn为0，调用函数为fil_names_dirty_and_write。\n采用MLOG_FILE_NAME记录更改的文件列表是因为在recovery时，需要在apply page变更之前确保打开所有的文件，然后可以进行并行恢复，而不被打断。\nflush #  在page回写磁盘后，需要调用fsync来保证磁盘上数据页的持久性。\n数据文件的IO为O_DIRECT, 但每次修改后依然需要去做fsync来持久化元数据信息，但是对于某些文件系统而言并没有必要做fsync，因此MySQL 5.7加入了新的选项：O_DIRECT_NO_FSYNC，这个需求来自于facebook. 他们也对此做了特殊处理：除非文件size变化，否则不做fsync。（最近在buglist上对这个参数是否安全的讨论也很有意思，官方文档做了新的说明，感兴趣的可以看看 O_DIRECT_NO_FSYNC poss\n因此引入了file_node→flush_size\nfor loop in space-\u0026gt;space_chain { /* Skip flushing if the file size has not changed since last flush was done and the flush mode is O_DIRECT_NO_FSYNC */ if (fbd \u0026amp;\u0026amp; (node-\u0026gt;flush_size == node-\u0026gt;size)) { continue; } }   unflushed list #  因为支持异步IO，所以需要通过unflushed_spaces来跟踪未flush的file_space，即在write时计入（此时modification_counter \u0026gt; flush_counter），在flush（fsync）后移除。\n其中的点位信息有：\n modification_counter 全局点位（推进）：file_system→modification_counter 作为单调递增的IO计数器，当IO操作开始时++，并将该IO点位更新到file_node→modification_counter modification_counter 文件点位（开始）：file_node →modification_counter flush_counter 文件点位（结束）：fil_flush后将flush_counter更新为modification_counter。如果采用Direct IO禁用了fsync（SRV_UNIX_O_DIRECT_NO_FSYNC），当fil_node→flush_size == fil_node→size，则直接更新file_node→flush_counter而无需flush。  file_system和file_node的点位信息无需持久化，在重启后置零，保证在运行时（内存态）单调递增即可  group fsync #  在文件sync时，也采用group fsync的方式来减少fsync的代价。在fil_flush中，首先抢到file_system→mutex的负责group fsync，其他未抢到的则通过node→modification_counter是否和file_node→flush_counter来判断是否已经被leader fsync，从而可以跳过fsync。\n表空间操作的并发控制 #  因为支持异步IO，所以在对表空间的并发控制，需要考虑异步操作（pending operations），所以这里将表空间支持的操作分为两类：\n 维护操作：包括delete、close、truncate、rename和extend 异步操作：读写page，IS统计信息、以及进行io操作（异步io起始结束、flush计数）  首先来看一下异步操作，又可以分为pending ops和pending ios，如下图所示：\npending ops即为file_space.n_pending_ops，具体的+-时机如下：\n在buffer pool从磁盘上同步读取page，或者change buffer修改page时，都需要将file_space的pending ops++-- buf_read_ahead_linear(计算space是否有hole) buf_read_ahead_random(计算space是否有hole) ibuf_merge_or_delete_for_page lock_rec_block_validate(innodb monitor-\u0026gt;lock_print_info_all_transactions) lock_rec_fetch_page(buf_page_get_gen) 以及在IS展示统计信息时，需要将file_space的pending ops++-- pending io的指标体现在读写和flush上。\n接着看一下维护操作，这里的操作指的是对表空间或者物理文件进行的操作，\n 表空间delete 表空间close：只用于import表空间时进行清理 表空间truncate：用于用户表空间和undo表空间 表空间rename 表空间extend：因为是对物理文件进行操作，所以flag设在了file_node上（file_node→being_extended）  在进行维护操作时，需要对操作进行互斥，并发控制如下图所示：\nfile_system子系统 #  文件子系统的整体关系图如下所示：\nfil_system_t #  fil_system_t用于表示文件子系统的逻辑结构，在InnoDB运行期间，内存中只有一个fil_system_t对象，统一管理所有文件的操作。\n   变量 类型 说明     mutex ib_mutex_t 保护fil_system_t   spaces hash_table_t 表空间hash table，实现对表空间按id的快速访问 id, fil_space_t\u0026gt;   name_hash hash_table_t 表空间hash table，实现对表空间按name的快速访问 name, fil_space_t\u0026gt;   LRU UT_LIST_BASE_NODE_T(fil_node_t) 文件系统最近打开的文件节点链表，维护此链表的目的是减少文件节点打开和关闭的次数   unflushed_spaces UT_LIST_BASE_NODE_T(fil_space_t) 在IO操作后记录需要flush的表空间链表   n_open ulint 文件系统当前打开的文件节点数   max_n_open ulint 文件系统最大可以打开的文件节点数（innodb_open_files）   modification_counter int64_t 全局写入点位（单调递增）   max_assigned_id ulint 分配的最大space_id(fil_space_t.id)   space_list UT_LIST_BASE_NODE_T(fil_space_t) 文件空间链表，实现对文件空间的遍历访问   named_spaces UT_LIST_BASE_NODE_T(fil_space_t) 在上一次CP后有哪些file_space产生了变化   space_id_reuse_warned bool /    fil_space_t #  fil_space_t用于表示文件空间的逻辑结构。文件空间是有若干文件节点组成的一个逻辑文件，并不是指单个文件。对于每种文件空间类型，都有一个对应的fil_space_t。\n   变量 类型 说明     变量 类型 说明   name char* file_space名称   id ulint 表空间ID，规则见上   max_lsn lsn_t 与named_spaces相关，已flush为0，否则为MLOG_FILE_NAME的lsn点位   stop_ios bool 在rename表空间时设置，阻止其他操作   stop_new_ops bool 在delete、close、truncate表空间时设置，阻止其他操作   is_being_truncated bool 在truncate表空间时设置，阻止其他操作   redo_skipped_count ulint    purpose fil_type_t 文件空间类型   chain UT_LIST_BASE_NODE_T(fil_node_t) 此文件空间中包含的文件节点链表   size ulint 表空间中所有文件节点的页的总数，文件空间的大小为size * 16K   size_in_header ulint 已使用的页的数量（space header.FSP_SIZE）   free_len ulint 空闲区链表的长度（space header.FSP_FREE）   free_limit ulint 已逻辑分配的位置（space header.FSP_FREE_LIMIT）   flags ulint 表空间flags，可以计算出pageSize(space-\u0026gt;flags)   n_reserved_extents ulint 为表空间操作预留的空闲区个数，比如B+树索引的节点分裂。这些操作会导致表空间的size增加，为了防止增加后超过表空间size的最大值，预先增加这么多个区   n_pending_flushes ulint pending flush计数器   n_pending_ops ulint pending op计数器   hash hash_node_t fil_system_t.spaces的节点   name_hash hash_node_t fil_system_t.name_hash的节点   latch rw_lock_t 对表空间的并发操作进行保护   unflushed_spaces UT_LIST_NODE_T(fil_space_t) fil_system_t.unflushed_spaces链表节点   named_spaces UT_LIST_NODE_T(fil_space_t) fil_system_t.named_spaces链表节点   is_in_unflushed_spaces bool 标记是否未flush，如果该file_space在file_system-\u0026gt;unflushed_spaces中，则为true   space_list UT_LIST_NODE_T(fil_space_t) fil_system_t.space_list链表节点    当系统初始化时，会会为数据表文件、重做日志文件创建对应的fil_space_t，并加入到fil_system_t的space_list、spaces和name_hash中。\n创建的物理文件会通过文件的第一个page（fsp page）中的space id和file space进行关联。\nlatch的作用是对表空间的并发操作进行保护，比如之前介绍的页、区、段的管理，都需要现持有表空间对应的latch（mtr_x_lock_space）。有一点特殊的是，对于临时表空间，因为临时表的创建是用户线程私有的，则不需要持有latch，即latch_t.m_temp_fsp = true。\n新建的独立表空间的tablespace初始只有4个页：\n page 0 is the fsp header and an extent descriptor page, page 1 is an ibuf bitmap page, page 2 is the first inode page, page 3 will contain the root of the clustered index of the table we create here.   fil_node_t #  fil_node_t用于文件节点的管理，以便于对文件节点进行读/写操作。\n注意：对于不同的表空间类型，file_node（物理文件）的数量不同：日志表空间有多个物理文件；系统表空间，用户表空间、undo表空间和临时表空间：都只有一个物理文件对应。\n   变量 类型 说明     变量 类型 说明   space fil_space_t* 文件节点所属的文件空间   name char* 文件节点的名称 \u0026lt;路径+文件名\u0026gt;   is_open bool 文件是否已被打开   handle pfs_os_file_t 文件节点打开后的fd   sync_event os_event_t group fsync时通知其他处于等待中的file node fil_flush()调用者   is_raw_disk bool 是否创建在raw device上（即用OS_FILE_OPEN_RAW创建）   size ulint 文件节点的页数，为表空间前面所有file的size之和+该文件节点的size的相对偏移量（比如20+20+4）==   flush_size ulint 用于支持O_DIRECT_NO_FSYNC，详见【2.2.7 flush】   init_size ulint 初始页数（FIL_IBD_FILE_INITIAL_SIZE = 4）   max_size ulint 最大页数（ULINT_MAX）==   n_pending ulint pending io计数器   n_pending_flushes ulint pending flush的数量，在file node fil_flush时++–，同一时刻可能有其他fil node在fil_flush时等待flush完成   being_extended bool 是否正在扩展表空间   modification_counter int64_t 文件准备写入点位（IO操作前）   flush_counter int64_t 文件完成写入点位（IO操作后）   chain UT_LIST_NODE_T(fil_node_t) fil_space_t.chain链表节点   LRU UT_LIST_NODE_T(fil_node_t) fil_system_t.LRU链表节点   punch_hole bool 文件是否支持打洞   block_size ulint punch_hole开启情况下，洞的大小（即file的sector size）   atomic_write bool 是否支持FusionIO的原子写    一个文件节点必定在一个文件空间的chain链表上，但不一定在文件系统的LRU链表上。当一个文件节点上的操作完成时，该文件节点不会立即被关闭，而是加入到文件系统的LRU链表中。\nfsp0file \u0026amp; fsp0space \u0026amp; fsp0sysspace #  fsp0file中定义了存储的物理表达形式，即物理文件。fsp0space和fsp0sysspace中定义了物理上表空间和系统表空间。\n整体物理文件系统的层次组织如下：\nDatafile #  Datafile中的字段说明：\n   变量 说明     m_name 去掉路径名的文件名   m_filepath 路径+文件名+扩展名   m_filename 文件名+扩展名，指向m_filepath的内存区域   m_handle fd   m_open_flags 打开文件时的flags   m_size 该文件存储使用的page数量   m_order 该文件在tablespace中的顺序（0,1,2\u0026hellip;） 只有系统表空间为n，其他表空间都为0   m_type NOT_RAW 不是raw partition NEW_RAW 需要进行初始化的raw partition OLD_RAW 已完成初始化的raw partition   m_space_id 表空间ID   m_flags 表空间的FSP_SPACE_FLAGS   m_exists 在启动时文件是否就已经存在   m_is_valid 表空间是否有效   m_first_page m_first_page_buf 表空间第0页的内存缓存，分配2倍最大页的空间，两个指针指向到一个区域，只是用第一个指针   m_atomic_write 是否支持原子写   m_last_os_error 读写文件时的最后一次错误信息   m_file_info 文件的inode number   m_encryption_key m_encription_iv 表空间的加密key信息    Remote Datafile #  软链接文件\n在datadir下创建一个软链接文件，即InnoDB Symbolic Link (ISL)，对于共享表空间，软链接文件位于datadir/basename.isl；对于独立表空间，软链接文件位于datadir/database/tablename.isl。\nTablespace #  使用create tablespace都会创建一个tablespace对象，其包括一组Datafile，即vectorm_files。\nSysTablespace #  继承Tablespace，并可以自动增长\n文件IO #  在InnoDB中，对于文件的IO操作分为运维操作和数据读写操作。其中运维操作是指创建、删除等针对文件的操作，而数据读写操作指的是对文件内部的数据进行读写。\n运维操作这里不再详述，我们把精力主要集中在数据读写操作上，以下所提到的IO操作如果不做特殊说明，指的是数据的读写操作。\n在进行数据读写时，用户线程通过同步IO进行读写数据，后台线程通过异步IO的方式来读写数据的。\n同步IO和异步IO都是通过pread/pwrite来保证文件系统的并发读写正确性，区别只是实际的读写线程是用户线程还是IO线程。\n由于早期的操作系统不支持原生的异步IO，所以InnoDB在早期通过模拟的形式来进行异步IO的操作，以提高性能。而在Windows和Linux支持native AIO后，基本上已经不再使用模拟异步IO的方式来读写数据了。\n对于InnoDB来说，前台的用户操作和日志采用同步IO（checkpoint为异步IO），后台的数据操作采用异步IO，并且，异步IO也不是FIFO的模式。\n文件IO的整体处理流如下图所示：\n更详细的函数调用如下图所示：\n异步IO #  存储引擎传入的IO请求放入等待队列，通过唤醒独立的IO线程来同步处理这些IO请求，IO线程实际上通过同步的file read/write读取文件中的数据。这里的异步IOhandler有3种：\n  Windows原生异步IO（Windows native AIO）\n  Linux原生异步IO（Linux native AIO）\n通过libaio的方式进行异步读写\n  模拟异步IO\n通过os_aio_simulated_handler进行模拟异步读写。\n  数据结构如下图所示\n这里可以看到每个io thread都有256个slot，可以认为是每个io thread的并发数控制，如果超过，则后续的异步请求需要等待。\n块设备层也有相应的io并发数控制：\nThe nr_requests is a parameter for block device, it controls maximum requests may be allocated in the block layer for read or write requests, the default value is 128. Occasionally, it may be suggested to adjust the value, generally speaking:\n Increasing the value will improve the I/O throughput, but will also increase the memory usage. Decreasing the value will benefit the real-time applications that are sensitive to latency, but it also decreases the I/O throughput.   函数调用：\nfil_io fil_mutex_enter_and_prepare_for_io 预处理 找到page.space 通过node-\u0026gt;size和page_no找node fil_node_prepare_for_io\tio prepare sync + fil_node_complete_io async + os_aio\t发送aio request os_aio_func OS_AIO_SYNC os_file_read_func/os_file_write_func os_file_pread/pwrite 同步IO OS_AIO_LOG 日志 OS_AIO_IBUF\tchange buffer read OS_AIO_NORMAL\t其他 select_slot_array fill slot wake up IO thread Punch Hole #  punch hole （打洞）功能，就是可以把文件中间的一部分内容释放掉，但是剩余部分的文件偏移不变。\nHole Punch Size on Linux\nOn Linux systems, the file system block size is the unit size used for hole punching. Therefore, page compression only works if page data can be compressed to a size that is less than or equal to the InnoDB page size minus the file system block size. For example, if innodb_page_size=16K and the file system block size is 4K, page data must compress to less than or equal to 12K to make hole punching possible.\n 什么是Punch Hole #  在UNIX文件操作中，文件位移量可以大于文件的当前长度，在这种情况下，对该文件的下一次写将延长该文件，并在文件中构成一个空洞，这一点是允许的。位于文件中但没有写过的字节都被设为 0。\n如果 offset 比文件的当前长度更大，下一个写操作就会把文件“撑大（extend）”。这就是所谓的在文件里创造“空洞（hole）”。没有被实际写入文件的所有字节由重复的 0 表示。空洞是否占用硬盘空间是由文件系统（file system）决定的。大部分文件系统是不占用的。\n怎样获得一个Punch Hole文件 #  以Linux来说，使用lseek或truncate到一个固定位置生成的“空洞文件”是不会占据真正的磁盘空间的。\n空洞文件特点就是offset大于实际大小，也就是说一个文件的两头有数据而中间为空，以‘\\0‘填充。那文件系统会不会不做任何处理的将其存放在硬盘上呢？大部分文件系统是不会将其存放在硬盘上。\n文件预留 #  为什么需要文件预留\n在开发过程中有时候需要为某个文件快速地分配固定大小的磁盘空间，为什么要这样做呢？\n 可以让文件尽可能的占用连续的磁盘扇区，减少后续写入和读取文件时的磁盘寻道开销 迅速占用磁盘空间，防止使用过程中所需空间不足 后面再追加数据的话，不会需要改变文件大小，所以后面将不涉及metadata的修改  前面提到使用lseek或truncate到一个固定位置生成的“空洞文件”是不会占据真正的磁盘空间的。\n快速的为某个文件分配实际的磁盘空间在Linux下可通过fallocate（对应的posix接口为posix_fallocate）系统调用来实现，大部分主流文件系统如ext4，xfs还是支持fallocate。\n文件打洞 #  最近遇到了这样的一种需求，一个大文件中的某段范围的内容已经失效了，想把这段失效的文件部分所占用的磁盘空间还给文件系统。linux下可以通过fallocate实现归还一个文件所占用的部分磁盘空间。\n#include \u0026lt;fcntl.h\u0026gt; int fallocate(int fd, int mode, off_t offset, off_t len); fd就是open产生的文件描述符，offset就是进行fallocate的文件偏移位置，len为fallocate的的长度。offset和len一起构成了要释放的文件范围。\n重点介绍的是mode，它决定了fallocate的行为。\n  Allocating disk space\n这是默认的操作，对应mode等于0。它所作的工作是如果分配从offset开始到offset+len的一段空间，这个是真的分配磁盘空间，不是hole，新分配的空间以0填充数据。当然这个操作一般在offset+len大于现有文件长度时才会起到增加文件数据空间的作用。 一般情况下新增加空间后文件的size也会随着调整，但是有一个特殊情况，就是当FALLOC_FL_KEEP_SIZE出现在mode中时，在增加文件空间后不会改变文件的size。这样的操作算是一种在文件结尾处的预分配，对于后期的append写入操作有优化作用。 （但遗憾的是ubuntu 12.04 ext4文件系统好像并不支持fallocate的预分配）\n  Deallocating file space\n释放文件的某段范围的磁盘空间 （文件打洞）\nFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE\n此mode虽然并不会改变文件的大小，但其实却释放了offset和len所在范围的磁盘块，将它们归还给了文件系统。fallocate成功后，后续对offset和len所在的文件范围进行读操作，将会读到0。\n  FusionIO #  FusionIO公司（2014年被SanDisk收购）在2012年声称他们的基于flush memory的存储产品，可以提供百万IOPS，这款产品和SSD的区别可以参看这篇文章。随后MariaDB也提供了FusionIO设备的支持，即对文件句柄加上DFS_IOCTL_ATOMIC_WRITE_SET的ioctl标记为，就可以启用文件的原子写特性，这样，就不会出现半写的问题，也就不再需要double write。\n在MySQL中，随后也做了相应的支持，代码如下：\n#if !defined(NO_FALLOCATE) \u0026amp;\u0026amp; defined(UNIV_LINUX)  #include \u0026lt;sys/ioctl.h\u0026gt;/** FusionIO atomic write control info */ #define DFS_IOCTL_ATOMIC_WRITE_SET\t_IOW(0x95, 2, uint)  /** Try and enable FusionIO atomic writes. @param[in] file\tOS file handle @return true if successful */ bool fil_fusionio_enable_atomic_write(pfs_os_file_t file) { if (srv_unix_file_flush_method == SRV_UNIX_O_DIRECT) { uint\tatomic = 1; ut_a(file.m_file != -1); if (ioctl(file.m_file, DFS_IOCTL_ATOMIC_WRITE_SET, \u0026amp;atomic) != -1) { return(true); } } return(false); } #endif /* !NO_FALLOCATE \u0026amp;\u0026amp; UNIV_LINUX */Fusion IO的高性能一方面是通过PCI-E通道进行数据的传输，一方面在IO路径上也和传统的存储设备不一样，如下图所示：\nInnoDB文件操作 #  文件的并发访问控制 #  InnoDB需要通过文件系统的文件锁来保证只有一个进程（mysqld）对某个文件进行读写操作（os_file_lock），在实现中使用了建议锁（Advisory locking），而不是强制锁（Mandatory locking），因为强制锁在不少系统上（包括linux）有bug。在非只读模式下，所有文件打开后，都用文件锁锁住，以保护只有mysqld进程对文件的正确访问。\nos_file_lock使用了fcntl+F_WRLCK的方式对文件加排他锁  建目录 #  InnoDB中目录的创建使用递归的方式(os_file_create_subdirs_if_needed和os_file_create_directory)。例如，需要创建/a/b/c/这个目录，先创建c，然后b，然后a，创建目录调用mkdir函数。此外，需要注意创建目录上层需要调用os_file_create_simple_func函数，而不是os_file_create_func。\n临时文件 #  InnoDB也需要临时文件，临时文件的创建逻辑比较简单(os_file_create_tmpfile)，就是在tmp目录下成功创建一个文件后直接使用unlink函数释放掉句柄，这样当进程结束后（不管是正常结束还是异常结束），这个文件都会自动释放。InnoDB创建临时文件，首先复用了server层函数mysql_tmpfile的逻辑，后续由于需要调用server层的函数来释放资源，其又调用dup函数拷贝了一份句柄。\n其他文件操作 #  如果需要获取某个文件的大小，InnoDB并不是去查文件的元数据(stat函数)，而是使用lseek(file, 0, SEEK_END)的方式获取文件大小，这样做的原因是防止元信息更新延迟导致获取的文件大小有误。\nInnoDB会预分配一个大小给所有新建的文件(包括数据和日志文件)，预分配的文件内容全部置为零(os_file_set_size)，当前文件被写满时，再进行扩展。此外，在日志文件创建时，即install_db阶段，会以100MB的间隔在错误日志中输出分配进度。\n表空间的磁盘组织结构 #  对于物理文件，内部也需要通过高效的数据组织来有效的利用文件空间。所以，接下来我们深入文件内部，来看一下内部的数据组织形式。\n从上面可以看到，物理文件隶属于不同的表空间，不同的表空间，除了日志表空间外，都有统一的page layout（具有通用的FIL_HEADER和FIL_TRAIL），固定的page size（对于压缩表，可以在建表时指定block size，但在内存中表现的解压页依旧为统一的页大小），并普遍使用B+树来管理数据。\n在表空间的管理上，首先表空间是由一个或多个物理文件组成的，一个物理文件按照区（extent）来进行管理，区是物理上连续分配的一段空间，由64个页组成，用于存储实际的数据，分配给数据的区逻辑上叫做段（分为non-leaf segment和leaf segment以及碎片页）。\n日志表空间的log block大小为512字节，这时为了保证日志页写入的原子性。因为现代的存储设备（SSD）中的sector size已经为4K了，所以将log block size对齐到4K可以避免read-modify-write现象。MySQL通过innodb_log_write_ahead_size设置来避免read-modify-write，而Percona则直接设置log block size的值。\nMySQL:\nSetting the innodb_log_write_ahead_size value too low in relation to the operating system or file system cache block size results in read-on-write. Setting the value too high may have a slight impact on fsync performance for log file writes due to several blocks being written at once.\nPercona\nEffect from innodb log block size 4096 bytes\nread-modify-write\n当要向硬盘写的数据小于4K时，会先把4k的一个扇区的数据读出，修改相应的部分后在写入。这个过程称之为read-modify-write (RMW)，也叫写放大。\n 页 #  对于InnoDB存储引擎，数据文件最小的存储单位是页（默认16K），在页的基础上逻辑的划分为区（extent）、段（segment），并形成一个逻辑上的统一整体：表空间（tablespace）。\n同时，为了使数据库获得更好的I/O性能，InnoDB存储引擎对于空间的申请不是按照页，而是按照区的方式，一次64页（1MB）的单位申请。这样的目的是：提高空间申请的效率、一定程度上保证磁盘上数据存放的顺序性。\n另外，page size的大小设置也和存储设备的IO性能息息相关，存储设备慢，page size可以设的大一些，以保证IO操作可以读取到更多的数据。\n页是InnoDB访问磁盘的最小I/O单元。页的默认大小是16K（UNIV_PAGE_SIZE）。除去页头和页尾的元数据，页的绝大部分空间用来存储数据，页的结构如图所示：\n从上图中可以看到，页有不同的类型，但都有统一固定的头部（page header 38 bytes）和尾部（page trailer 8 bytes）。通过space_id + page_offset可以定位页的具体位置（FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID + FIL_PAGE_OFFSET）。由于FIL_PAGE_OFFSET的长度是4个字节，所以一个表空间的最大存储空间是64TB（231216K）。索引页之间的逻辑顺序（需要保证有序性：键值顺序）通过双向链表指针连接起来（FIL_PAGE_PREV、FIL_PAGE_NEXT），即内部存储的是前页/后页在表空间中的偏移量。\n页的位置和页之间的位置都通过offset来存储，即相对位置存储的好处是，当表空间数据移动时不会受到影响，如果存储的是绝对位置，则需要进行变更。  FIL_HEADER和FIL_TRAIL的字段如下：\n    字段 大小 说明     FIL_PAGE_DATA（38） FIL_PAGE_SPACE_OR_CHKSUM 4 checksum   FIL_PAGE_OFFSET 4 页号，也是页在表空间中的偏移量    FIL_PAGE_PREV 4 前一个页的偏移量（仅对索引页有效）    FIL_PAGE_NEXT 4 后一个页的偏移量（仅对索引页有效）    FIL_PAGE_LSN 8 页LSN    FIL_PAGE_TYPE 2 页类型    FIL_PAGE_FILE_FLUSH_LSN 8 仅在系统表空间的第1个页（0,0）中使用，用来判断数据库是否正常关闭    FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 8 表空间ID（缓冲池中靠space_id+page_no来标识页）    FIL_PAGE_DATA_END（8） FIL_PAGE_END_LSN_OLD_CHKSUM 8 前4个字节存放checksum，后4个字节存放FIL_PAGE_LSN的后4个字节，用于检测页是否损坏    在数据库正常关闭时，会做一次checkpoint，并把CP lsn写入到页（0,0）中。这样，在数据库恢复时，根据这两个值是否相等来判断是否为正常关闭。  判断页面是否损坏\n通过FIL_PAGE_END_LSN_OLD_CHKSUM来做到\u0026quot;前后呼应\u0026quot;，判断页面的修改是否是完整的：\n 通过后4个字节，检查和FIL_PAGE_LSN的后半部分（后4个字节）是否一致 通过页面内容来计算出checksum，用前4个字节以及页头的checksum（FIL_PAGE_SPACE_OR_CHKSUM）是否一致   在页面头上记录一个标志，如时间戳，然后在页面尾上也记录一个相同的标志，当读取数据页面后，只要检查一下标志是否相同即可证明页面是否存在部分写。需要注意的是每次页面更新后，标志也必须同时进行修改，不能使用原有页面的标志。还有一些代价更大的方法，如生成整个页面的摘要，然后记录到页头中，除了可以检查页面的部分写之外，还能防止页面被恶意篡改，但这对系统资源的消耗比较大。一种热衷的办法是计算部分页面数据的摘要，但要包含页面头和页面尾，这样就可以能检测页面的部分写，也能部分达到防止篡改的目的。  其中页类型如下：\n#define FIL_PAGE_INDEX 17855 /*!\u0026lt; B-tree node */ #define FIL_PAGE_RTREE 17854 /*!\u0026lt; B-tree node */ #define FIL_PAGE_UNDO_LOG 2 /*!\u0026lt; Undo log page */ #define FIL_PAGE_INODE 3 /*!\u0026lt; Index node */ #define FIL_PAGE_IBUF_FREE_LIST 4 /*!\u0026lt; Insert buffer free list */ /* File page types introduced in MySQL/InnoDB 5.1.7 */ #define FIL_PAGE_TYPE_ALLOCATED 0 /*!\u0026lt; Freshly allocated page */ #define FIL_PAGE_IBUF_BITMAP 5 /*!\u0026lt; Insert buffer bitmap */ #define FIL_PAGE_TYPE_SYS 6 /*!\u0026lt; System page */ #define FIL_PAGE_TYPE_TRX_SYS 7 /*!\u0026lt; Transaction system data */ #define FIL_PAGE_TYPE_FSP_HDR 8 /*!\u0026lt; File space header */ #define FIL_PAGE_TYPE_XDES 9 /*!\u0026lt; Extent descriptor page */ #define FIL_PAGE_TYPE_BLOB 10 /*!\u0026lt; Uncompressed BLOB page */ #define FIL_PAGE_TYPE_ZBLOB 11 /*!\u0026lt; First compressed BLOB page */ #define FIL_PAGE_TYPE_ZBLOB2 12 /*!\u0026lt; Subsequent compressed BLOB page */ #define FIL_PAGE_TYPE_UNKNOWN 13 /*!\u0026lt; In old tablespaces, garbage in FIL_PAGE_TYPE is replaced with this value when flushing pages. */ #define FIL_PAGE_COMPRESSED 14 /*!\u0026lt; Compressed page */ #define FIL_PAGE_ENCRYPTED 15 /*!\u0026lt; Encrypted page */ #define FIL_PAGE_COMPRESSED_AND_ENCRYPTED 16/*!\u0026lt; Compressed and Encrypted page */ #define FIL_PAGE_ENCRYPTED_RTREE 17 /*!\u0026lt; Encrypted R-tree page */ 区 #  页是innoDB访问和存储的最小单位，区是InnoDB申请空间的最小单位，一个区由64个连续的页组成，大小为1MB。区的管理和分配由FSP_HDR页（第0页）和XDES页共同完成，其中表空间的元信息存储在FSP_HDR页的space header中，占用112个字节。\n区分为可用区（用于分配给段 extent）和碎片区（frag extent），可用区通过保存在空闲区链表（FSP_FREE）中。InnoDB为了节约存储空间，数据首先保存在32个碎片页中，碎片页从碎片区中分配，超过32个碎片页后，再以区的方式从空闲区链表中申请空间。碎片区不属于任何段，保存在碎片半满区链表（FSP_FREE_FRAG）和碎片全满区链表（FSP_FULL_FRAG）中。\n表空间可以看做是由多个区组成的一个“大文件块”，使用时按照从低到高的页偏移量顺序地进行区空间的申请。FSP_FREE_LIMIT表示当前已经已经申请到的位置。超过FSP_FREE_LIMIT的部分表示区还未进行初始化。\nspace header保存的元信息如下：\n   字段 大小 说明     FSP_SPACE_ID 4 表空间ID，由数据字典分配   FSP_NOT_USED 4 -   FSP_SIZE 4 表空间总的page数量，扩展文件时需要更新（fsp_try_extend_data_file_with_pages），物理分配界限   FSP_FREE_LIMIT 4 未分配的最小page no，该offset之后的都尚未加到空闲区链表（FSP_FREE）上，逻辑分配界限   FSP_SPACE_FLAGS 4 表空间flags   FSP_FRAG_N_USED 4 碎片区（FSP_FREE_FRAG）中已经使用的页的数量，每当从FSP_FREE_FRAG分配一个空闲页出去时，+1，可快速计算表空间的可用碎片页数   FSP_FREE 16 空闲区链表，用户从这里以区为单位申请加入到段链表中   FSP_FREE_FRAG 16 碎片半满区链表，该链表中的区中的页要么属于不同的段，要么还未分配   FSP_FULL_FRAG 16 碎片全满区链表，当由page从该链表的区中释放时，则将该区移回碎片半满区链表   FSP_SEG_ID 8 下一个段的ID，在表空间中，每个段都有唯一的编号，即段ID，每次分配段后+1   FSP_SEG_INODES_FULL 16 已经完全用满的segment inode页链表（也称为段inode全满页链表）   FSP_SEG_INODES_FREE 16 至少存在一个空闲的segment inode entry的segment inode页链表（也称为段inode未满页链表）    对于用户表空间来说，当小于64个页时，FSP_FREE_LIMIT却为64（因为区是分配的最小粒度），但是物理上并没有分配这么多的空间，可能只分配了4个页（初始状态）。  区空间的申请通过函数fsp_fill_free_list实现，如果空间大小允许，每次申请4个区（FSP_FREE_ADD），如果申请的区包含碎片区，则申请5个区。申请的区的信息更新到space header。\n每个区包含了64个页，由区描述符（XDE entry）表示。每 个区（意味着256个XDE entry）存储在一个XDE page里。\n区描述符\n区描述符（XDES extent descriptor 40 bytes）使用位图表示64个页的使用状态，每个页的状态占用2位（XDES_FREE_BIT/XDES_CLEAN_BIT），一共需要64*2 = 128 bits = 16 bytes。区描述符的结构如图所示：\n   字段 大小 说明     XDES_ID 8 如果区已分配给段，则记录其段ID（segment inode entry.FSEG_ID）   XDES_FLST_NODE 12 区所在的链表节点：FSP_FREE、FSP_FREE_FRAG / FSP_FULL_FRAG、或者位于某个B+树的segment inode entry链表中   XDES_STATE 4 区状态XDES_FREE ：空闲区，待分配给段，在FSP_FREE链表中 XDES_FREE_FRAG：碎片半满区，在FSP_FREE_FRAG链表中 XDES_FULL_FRAG：碎片全满区，在FSP_FULL_FRAG链表中 XDES_SEG ：已分配给段，记录段ID   XDES_BITMAP 16 区中64个页的使用状态，用2个bit表示一个页 XDES_FREE_BIT ：该页是否空闲 XDES_CLEAN_BIT：未使用，保留位    一个区描述符占40字节，一页可以保存256个区描述符（40 * 256 = 10240，不会用满，剩余5986字节未用），该XDES页可以描述总共16384个页面的信息（64个页 * 256个区描述符），这样，每隔16384个页分配一个XDES页（第0页，同时也分配第1页IBUF_BITMAP page），即每个XDES页的页号都是16384的倍数，函数xdes_calc_descriptor_page用于计算区描述符所在的页面位置（offset），即XDES entry可以用page no+boffset（在XDES page中的相对位置）描述。\n虽然一个XDES page保存256个XDES，但是第1个用于存放XDES元信息，只用了后面的255个区，也就是说，之后后面的255个区放到了表空间的链表管理中。并且，区描述符的管理不需要链表进行串联，这是因为区是连续分配的，每64个区就有一个XDES page，直接通过*64就可以定位到任意一个XDES page。\n同时由于space header存储在页（0，0）中，所以区描述符保存的开始位置在页的偏移量150字节处（FIL_HEADER + SPACE_HEADER 38 + 112）。同时，如果一个区中的页含有区描述符，则该区为碎片区。\n另外，FSP_HDR页中的剩余5986字节的未使用空间中，可以存储该表空间的加密信息（encryption）：\n   字段 大小 说明     ENCRYPTION_MAGIC_SIZE 4 加密版本，即version ENCRYPTION_KEY_MAGIC_V1 ENCRYPTION_KEY_MAGIC_V2   master_key_id 1 key_id   ENCRYPTION_SERVER_UUID_LEN 36 server_uuid   ENCRYPTION_KEY_LEN 32 key    段 #  段用来保存特定对象的数据。在InnoDB中，表是最常见的对象，同时表中的数据是通过索引组织表的方式组织的，即通过主键值以B+树索引的方式存储数据的。在InnoDB中，每个表至少有两个段，叶子节点段（leaf segment）和非叶子节点段（non leaf segment），段依据区的形式组织存储空间。\n但是为了更有效的管理存储空间，如果表非常小，或者undo段，都不需要一个完整的区来保存数据。那么，在存储数据时，每个段设计了32个碎片页，段中的空间首先保存在这32个碎片页中，如果不够了再以区为单位申请空间。碎片页从空闲碎片区列表（FSP_FREE_FRAG）中的一个碎片区中分配，具体的函数为fsp_alloc_free_page。段中的区从空闲区链表（FSP_FREE）中申请分配。\n一个段可以管理32个独立的页和若干个区。段的这种页+区混合管理的方式可以更有效的节约存储空间。表空间一旦将一块空间（一个页或者一个区）分配给一个段后，就不能再给别人使用了。所以，InnoDB存储引擎管理数据的思路是：从创建表开始，随着表中数据的增加，段每次从表空间中获取一个页，当获取到32个页后，则从表空间中获取一个区，这样，既保证了空间使用率，又兼顾了空间分配效率。\n那么抽象来看，一个段是可以“无限扩展的”，并且段是由若干区+碎片页组成的，是一个逻辑概念，而区是实实在在的物理存储，物理上是连续的。表空间中的若干段都由若干独立的区链接起来，这些链接起来的区链表长短不一，并且磁盘位置是随机的，逻辑上是连续的。\nsegment inode entry用于保存段的信息：\n   字段 大小 说明     FSEG_ID 8 段ID（分配从1开始，0表示未分配）   FSEG_NOT_FULL_N_USED 4 FSEG_NOT_FULL链表中使用的页数量   FSEG_FREE 16 已分配给该段，但完全没有使用的区链表   FSEG_NOT_FULL 16 已分配给该段，全部页未被用满的区链表（也称为段未满区链表）   FSEG_FULL 16 已分配给该段，全部页已被使用的区链表（也称为段已满区链表）   FSEG_MAGIC_N 4 magic number   FSEG_FRAG_ARR 0..31 128 碎片页链表，一共有32个页，保存了每个碎片页在表空间中的偏移量（4），因此总共需要32 * 4个字节    从上面可以看到，一个segment inode entry占用192个字节，这样一个segment inode页可以存储85个segment inode entry，页类型为FIL_PAGE_INODE。\nsegment inode页结构如下：\n   字段 大小 说明     FSEG_INODE_PAGE_NODE 12 segment inode页的链表节点，链表是FSP_SEG_INODES_FULL/FSP_SEG_INODES_FREE   segment inode entry 0 192 segment inode entry   segment inode entry 1 192 segment inode entry   \u0026hellip;     segemnt inode entry 84  segment inode entry    但是和XDES页不同的是，每个segment inode entry的位置不是固定的，不像XDES页都是间隔16384，同时，一个表可能存在多个索引，每个索引有2个段，所以一个表会有多个segment inode entry。为了找到segment inode entry的位置，还需要有一个segment header（10字节），由segment header指向segment inode entry。\n   字段 大小 说明     FSEG_HDR_SPACE 4 segment inode页所在的表空间ID   FSEG_HDR_PAGE_NO 4 segment inode页所在的表空间的偏移量   FSEG_HDR_OFFSET 2 segment inode entry在segment inode页中的偏移量    对于用户表来说，segment header总是保存在其索引的root页中，指向叶子节点段的inode页（leaf segment）和非叶子节点段的inode页（non leaf segment）。但是segment header也并不是总是这一页，比如在change buffer中放在一个单独的页中。\nroot页中的segment info描述了non leaf segment和leaf segment的segment header（10+10字节）：\n   字段 大小 说明     PAGE_BTR_SEG_LEAF 10 (FSEG_HEADER_SIZE) leaf segment在segment inode page中的位置   PAGE_BTR_SEG_TOP 10 (FSEG_HEADER_SIZE) non leaf segment在segment inode page中的位置    当创建一个索引的时候，实际上就是在构建一个新的B+树（btr_create），先为non leaf segment分配一个segment inode entry（从FSP_SEG_INODES_FREE找到一个空闲的segment inode页，从中分配segment inode entry），然后创建root page（根节点页，其位于segment inode entry的第一个碎片页），并将该segment inode entry的位置信息更新到root page上；之后在分配leaf segment的segment inode entry，过程同上。\n表空间 #  从上面可以看到，表空间是一个逻辑概念，由文件系统的物理文件组成，然后组织成页、区、段，如下图所示：\n表空间的逻辑结构如下图所示：\n从上图可以看出，表空间管理的是空闲区、碎片半满区和碎片全满区，segment inode page中的segment inode entry中存放的是具体某一段分配的空闲区、半满区和全满区。\n"},{"id":35,"href":"/docs/MySQL/InnoDB/12_transaction/","title":"transaction","section":"Inno Db","content":"1 事务 #  1.1 概述 #  事务是访问数据库中数据的一个程序执行单元。在一个事务中的操作，要么都做，要么不做，这是事务的目的，也是事务模型区别于文件系统的重要特征之一。\n从理论上来说，事务有着极其严格的定义，必须同时满足ACID四个特性。但是数据库厂商出于各种目的，并没有严格满足事务的ACID标准。比如对于MySQL的NDB Cluster，没有满足D；对于Oracle，默认的事务隔离级别是Read Committed，不满足I。在InnoDB中，默认的事务隔离级别是Read Repeatable，完全遵循和满足事务的ACID特性。\n这里要注意，D保证的是事务系统的高可靠性（High Reliability），而不是高可用性（High Availability）。对于高可用性，事务本身并不能保证，需要一些系统共同配合来完成。\n1.2 分类 #  参见Jim Gray\n1.3 隔离级别 #  令人惊讶的是，大部分数据库系统都没有提供真正的隔离性，最初或许是因为系统实现者并没有真正理解这些问题。如今这些问题已经弄清楚了，但是数据库的实现者在正确性和性能之间做了妥协。ISO和ANSI SQL标准制定了四种事务隔离级别的标准，但是很少有数据库厂商遵循这些标准，比如Oracle不支持Read Uncommitted和Repeatable Read的事务隔离级别。\nSQL标准定义的四个隔离级别为：\n Read Uncommitted Read Committed Repeatable Read Serializable  SQL和SQL2标准的默认事务隔离级别是Serializable。\nInnoDB支持的默认事务隔离级别是Repeatable Read，但是和SQL标准不同的是，InnoDB在此隔离级别下，通过使用next-key locking算法，避免了幻读的产生，即可以完全保证事务的隔离性要求，达到了SQL标准的Serializable隔离级别。在append-only storage上，采用snapshot isolation也是避免幻读的一种实现方式。\n关于事务可以洋洋洒洒写很多，这里主要聚焦InnoDB中的事务子系统的实现，详细事务的概念和事务控制技术这里略过。\n2 事务子系统 #  事务子系统中的数据包括事务信息、undo log、binlog信息以及doublewrite信息。\n这些信息由trx_sys page牵头，存储在各自的segment中：\n   segment 存储内容 segment header     txn segment 事务信息 trx_sys page   rollback segment undo segment、history链表、undo slots rollback segment page   undo segment undo log undo segment page   doublewrite segment doublewrite数据 trx_sys_page    整体层次如下：\ntrx_sys page: → txn_segment header → doublewrite_segment header → rollback slots (page) rollback segment page: → rollback_segment header → history_list → undo slots (page) undo segment page (1st page in undo segment) → undo log undo page → undo log 如果启用了undo表空间，其第3页会存储rollback segment array header（rollback slots），布局和现有介绍不同。  2.1 trx_sys page #  事务系统段在InnoDB第一次启动时创建（trx_sysf_create），trx_sys segment header保存在系統表空間的(0, 5)位置，该页称为trx_sys page，保存以下事务信息：\n 事务相关信息 rollback segment slots MySQL的binlog信息 doublewrite segment信息  trx_sys page如下图所示：\nTRX_SYS_TRX_ID_STORE保存最大事务ID，为了性能考虑，事务ID每隔256才持久化，然后在事务子系统启动时jump同样区间，跳过这个gap实现”健忘性“。\nTRX_SYS_FSEG_HEADER保存事务系统段的segment header（space+page_no+offset）。\nTRX_SYS_RSEGS保存回滚段信息（slot array），一共128个rollback slot（也称为回滚段对象），每个slot用8个字节存储space+page_no。\nTRX_SYS_MYSQL_LOG_INFO保存server层的binlog信息（-1000），用以保证innoDB的日志和server层日志的一致性。\nTRX_SYS_DOUBLEWRITE保存了doublewrite信息（-200）。\n2.2 doublewrite segment #  在buffer pool一章已经介绍，InnoDB在flush page时，为了避免partial-write，而采用doublewrite的方式进行flush，其本质上使用的是shadow page+duplex replica的方式。\ndoublewrite的segment header页保存在trx_sys page中。为了保证其在磁盘的顺序性以便顺序IO，doublewrite segment先一次性申请32个碎片页（不使用），再申请2个区，并将每个区的起始地址更新到BLOCK1、BLOCK2（buf_dblwr_create），因此总共申请了160个页（32 + 64*2），实际使用的是2个区128个页。\n初始化代码如下：\nfor (i = 0; i \u0026lt; 2 * TRX_SYS_DOUBLEWRITE_BLOCK_SIZE + FSP_EXTENT_SIZE / 2; i++) { new_block = fseg_alloc_free_page( fseg_header, prev_page_no + 1, FSP_UP, \u0026amp;mtr); if (new_block == NULL) { ib::error() \u0026lt;\u0026lt; \u0026#34;Cannot create doublewrite buffer: \u0026#34; \u0026#34; you must increase your tablespace size.\u0026#34; \u0026#34; Cannot continue operation.\u0026#34;; return(false); } /* We read the allocated pages to the buffer pool; when they are written to disk in a flush, the space id and page number fields are also written to the pages. When we at database startup read pages from the doublewrite buffer, we know that if the space id and page number in them are the same as the page position in the tablespace, then the page has not been written to in doublewrite. */ ut_ad(rw_lock_get_x_lock_count(\u0026amp;new_block-\u0026gt;lock) == 1); page_no = new_block-\u0026gt;page.id.page_no(); if (i == FSP_EXTENT_SIZE / 2) { ut_a(page_no == FSP_EXTENT_SIZE); mlog_write_ulint(doublewrite + TRX_SYS_DOUBLEWRITE_BLOCK1, page_no, MLOG_4BYTES, \u0026amp;mtr); mlog_write_ulint(doublewrite + TRX_SYS_DOUBLEWRITE_REPEAT + TRX_SYS_DOUBLEWRITE_BLOCK1, page_no, MLOG_4BYTES, \u0026amp;mtr); } else if (i == FSP_EXTENT_SIZE / 2 + TRX_SYS_DOUBLEWRITE_BLOCK_SIZE) { ut_a(page_no == 2 * FSP_EXTENT_SIZE); mlog_write_ulint(doublewrite + TRX_SYS_DOUBLEWRITE_BLOCK2, page_no, MLOG_4BYTES, \u0026amp;mtr); mlog_write_ulint(doublewrite + TRX_SYS_DOUBLEWRITE_REPEAT + TRX_SYS_DOUBLEWRITE_BLOCK2, page_no, MLOG_4BYTES, \u0026amp;mtr); } else if (i \u0026gt; FSP_EXTENT_SIZE / 2) { ut_a(page_no == prev_page_no + 1); } if (((i + 1) \u0026amp; 15) == 0) { /* rw_locks can only be recursively x-locked 2048 times. (on 32 bit platforms, (lint) 0 - (X_LOCK_DECR * 2049) is no longer a negative number, and thus lock_word becomes like a shared lock). For 4k page size this loop will lock the fseg header too many times. Since this code is not done while any other threads are active, restart the MTR occasionally. */ mtr_commit(\u0026amp;mtr); mtr_start(\u0026amp;mtr); doublewrite = buf_dblwr_get(\u0026amp;mtr); fseg_header = doublewrite + TRX_SYS_DOUBLEWRITE_FSEG; } prev_page_no = page_no; } 从这里可以看出，一共申请了64 * 2 +32个页（2 * TRX_SYS_DOUBLEWRITE_BLOCK_SIZE + FSP_EXTENT_SIZE / 2），这正如前面所分析的，doublewrite segment需要保证写入的顺序性。\n3 undo log #  3.1 设计 #  在InnoDB中，undo log有两个用途：\n 用来实现事务的原子性，即当事务由于主动/被动的原因而失败时，可以实现回滚，从而使数据恢复到事务开始运行时的状态 实现一致性非锁定读（consistent non-locking read）  undo log的作用贯穿事务的整个声明周期：故障恢复（crash \u0026amp; txn recovery）和并发控制（concurrency control）。而undo的持久性是靠redo保证的，这也是crash recovery要先做redo apply的原因之一。\n目前绝大多数数据库都通过locking+MV的方式提供事务的并发控制，这样，只有WW阻塞，RW、WR都可以并发，这样极大提高了事务的并发度。\n用于存储数据MV的地方称为version storage，在InnoDB中，采用delta的方式存储（因为其undo log记录的是行的diff），在PostgreSQL中，是在数据的行上实现的MV，即append-only。关于MVCC在后面一节详述。\nInnoDB的一致性非锁定读通过多版本控制的方式（multi-versioning）来读取当前执行期间数据库中行的数据，如果需要读取的行正在执行更新操作，读取操作无需等待行上锁的释放（nonlocking），而是读取行的一个快照（read snapshot），并可以保证数据的读取一致（consistent）。\nInnoDB的一致性非锁定读如下图所示，快照是指该行之前版本的数据，存储在undo log中。同时，因为undo log本身就会用于事务的回滚，因此快照本身是没有额外存储开销的，而且，读取快照也不需要加锁（因为不会变更数据）。\n一致性非锁定读如下图所示：\n在redo log一章我们介绍过，redo log考虑的一个重点是recovery的时间，因此通过多种手段来做出保证，比如可以基于page来做redo log的并行recovery。而undo log和redo log却大不相同，完全不必在crash recovery阶段就完成undo（数据库运行期间就保有undo log），可以在后台异步purge，这样就可以更快的完成启动，提供服务。\nundo log注重的是事务间的并发，以及维护MV的代价，并且与物理存储解耦。因此，undo log采用了逻辑日志的方式。\n一个undo segment在同一时刻只属于一个事务，一个读写事务至少会持有一个undo segment，这样，当大量事务并发时，就需要存在多个undo segment为事务存储undo log，因此，会将多个undo segment组成rollback segment，并通过history链表进行GC。\nundo log的组织有几个维度：\n 事务维度的undo log list，用于快速rollback transaction，一个事务的所有undo log都连续存储在最多2个undo segment中 全局维度的已提交事务undo log list，称为history链表，用于purge GC undo log的物理布局：包括存储在哪个表空间中，undo page，undo log type，undo log的组织  接下来，通过从总体到局部的顺序，依次介绍rollback segment→undo segment→undo page→undo log。\n3.2 物理布局 #  undo log的存储由rollback segment和undo segment共同完成,，并且，这两个段的segment header都保存在各自的segment内。rollback segment中仅保存undo segment page所在页的位置，一个rollback segment一共保存1024个undo segment的信息，加上trx_sys page可以保存128个rollback segment slot，因此理论上可以支持最大128 * 1024/2个并发事务（因为同一时刻只能有一个active事务使用一个undo page，假定每个事务DML有INSERT、UPDATE、DELETE）。\n整体关系如下图所示：\n3.2.1 rollback segment #  trx_sys page描述了128个rollback segment的信息（space+page_no）。这128个rollback segment保存在rollback segment header page中，其中RSEG_0 page位于(0, 6)位置。\n每个rollback segment header page维护了段内的history链表和1024个undo slot位置。\n字段信息如下：\n   字段 大小 说明     TRX_RSEG_MAX_SIZE 4 rollback segment可以拥有的最大undo page数（ULINT_MAX）   TRX_RSEG_HISTORY_SIZE 4 history链表中undo page的数量   TRX_RSEG_HISTORY 16 history链表（committed undo log list）   TRX_RSEG_FSEG_HEADER 10 rollback segment header   TRX_RSEG_UNDO_SLOTS 1024 * 4 undo slot directory（undo segment page位置）    总结一下，rollback segment主要存储三个信息：\n history链表，存储已提交事务的undo log list（按事务提交顺序逆序存放） rolback segment header undo slot directory  在MySQL 5.7中，REG_0在系统表空间中，REG_1 ~ REG_32在临时表空间，REG_33 ~ REG_127如果配置了undo表空间（space_id固定，从1开始）则放置其内，否则在系统表空间。\n3.2.2 undo segment #  每个rollback segment描述了1024个undo segment page，其中每个undo segment又由一系列undo page组成，存储实际的undo log，因此undo segment可以认为是一个\u0026quot;逻辑概念\u0026quot;，由一组undo page构成。\n在每个undo segment中，第一个undo page称为undo segment page，用于保存undo segment header（30字节）和相应的段元信息：\n   字段 大小 说明     TRX_UNDO_STATE 2 UNDO segment状态（5）：TRX_UNDO_ACTIVE：段内有活跃事务的undo logTRX_UNDO_CACHE：cached for quick reuseTRX_UNDO_TO_FREE：存放的都是insert DML，可以快速freeTRX_UNDO_TO_PURGE：不能重用，交由purge线程freeTRX_UNDO_PREPARED：段内有prepared txn的undo log   TRX_UNDO_LAST_LOG 2 最后一个undo log header offset   TRX_UNDO_FSEG_HEADER 10 undo segment header   TRX_UNDO_PAGE_LIST 16 undo page链表（将同一事务的segment page和normal page串在一起）    3.2.3 undo page #  undo segment中的所有page都为undo page，都有undo page header（18字节）：\n   字段 大小 说明     TRX_UNDO_PAGE_TYPE 2 页中保存的undo log类型：TRX_UNDO_INSERT TRX_UNDO_UPDATE   TRX_UNDO_PAGE_START 2 页中最新事务的undo log offset   TRX_UNDO_PAGE_FREE 2 页中的空闲空间offset   TRX_UNDO_PAGE_NODE 12 undo page链表节点（同一事务的undo page串起来）    undo page中其余部分则存储实际的undo log。这样，一个undo page保存以下几部分信息：\n undo page header undo segment header（段中第一页） undo log\u0026hellip;  每个事务在需要记录undo log时都会申请一个或两个undo segment（insert_undo/update_undo分开），同时把事务的第一个undo page放入对应undo segment中，这个页同时也会作为undo segment header page。一个undo segment header page同一时刻只隶属于同一个活跃事务，但是一个undo header page上面存储的undo log可能包含多个已经提交的事务和一个活跃事务。\n当活跃事务产生的undo record超过undo header page容量后，单独再为此事务分配的undo page（trx_undo_add_page）。undo page只隶属于一个事务。\n从上面可以看出，InnoDB会复用undo page存放不同事务的undo log，是因为：\n OLTP事务产生的undo log相对较小 其他事务可能正在引用当前的undo log，事务在提交时并不能直接删除相应的undo log（insert例外）  复用undo page有以下优势：\n 可以减少undo page的分配  我们计算一下，假设OLTP TPS=1000，每个事务产生200字节的undo log，如果undo page不复用，那么一分钟就要产生1000 * 60个undo page，每个undo page 16K，要分配16K * 60000 = 937M的存储空间；并且，如果purge线程每秒只能处理20个undo page，则存储用量会持续上涨。这种设计下磁盘的开销将会非常巨大。\n因此复用undo page是合理的。undo page的复用，是指一个undo page可以存放不同事务的undo log。具体来讲，在事务提交时，首先将undo log放入history链表，然后判断当前undo page是否可以复用：\n undo page的可用空间是否小于3/4（TRX_UNDO_PAGE_REUSE_LIMIT ） 该事务只用了一个undo page  如果满足则复用该undo page，并将undo segment状态设为TRX_UNDO_CACHE，之后的undo log append-only进行追加。\n这里可以看出history链表是undo log串起来的，而undo log又分布在不同的undo page中，因此purge线程在处理history链表时会产生离散IO。\n复用条件的判定：\n/** An update undo segment with just one page can be reused if it has at most this many bytes used; we must leave space at least for one new undo log header on the page */ #define TRX_UNDO_PAGE_REUSE_LIMIT (3 * UNIV_PAGE_SIZE / 4)  trx_undo_set_state_at_finish() { ... if (undo-\u0026gt;size == 1 \u0026amp;\u0026amp; mach_read_from_2(page_hdr + TRX_UNDO_PAGE_FREE) \u0026lt; TRX_UNDO_PAGE_REUSE_LIMIT) { state = TRX_UNDO_CACHED; } ... } 3.2.3 undo log #  undo log是一种逻辑日志，存储的是记录修改的before image（前镜像）。因此事务的rollback意味着是将数据逻辑的恢复到原来的样子。\n但是数据结构和page在回滚之后可能大不相同。因为，在数据库系统中，事务是并发访问的。同一页中的不同行并发，可能已经让page产生分裂/合并进而改变了B+ tree的结构，或者page的布局发生了变化。不能通过apply undo log将page回滚到该事务开始的样子，会影响其他事务在该页上的并发操作。\n另外，redo log是基于LSN实现的，LSN具有单调递增属性，如果回滚页的物理操作，会破坏这个特性，使设计变得复杂。\n每个undo记录由两部分内容构成：\n undo log header undo log record  undo log record又分为insert_undo/update_undo两种，insert操作产生insert undo log record，其他DML操作产生update undo log record，也存在一些特殊情况，后面另行介绍。\nundo log的结构如下图所示：\nundo log header保存每个事务undo日志的通用信息。并且，因为undo页可以重用，因此回滚段的第一个undo页中可以存在多个undo log header（活跃事务的header在最后）。每个undo log header占用46个字节：\n   字段 大小 说明     TRX_UNDO_TRX_ID 8 事务ID（事务开始的逻辑时间）   TRX_UNDO_TRX_NO 8 事务提交顺序，也用来判断是否能purge（事物结束的逻辑时间）   TRX_UNDO_DEL_MARKS 2 undo log中是否有DEL_MARK_REC，避免purge不必要的扫描   TRX_UNDO_LOG_START 2 undo log header结束的offset（也是第一个undo log record的offset）   TRX_UNDO_XID_EXISTS 1 XID   TRX_UNDO_DICT_TRANS 1 是否为DDL事务   TRX_UNDO_TABLE_ID 8 DDL事务修改的表   TRX_UNDO_NEXT_LOG 2 undo log list   TRX_UNDO_PREV_LOG 2 undo log list   TRX_UNDO_HISTORY_NODE 12 history链表节点    在事务每次进行DML操作时，首先分配一个undo log header（trx_undo_create），事务可能包含多个insert/update操作，但是不能存放在一个undo页中。每个类型的undo记录需要分配单独的undo段（UNDO LOG PAGE HEADER.TRX_UNDO_PAGE_TYPE）。这里的insert/update undo log record的删除时机不同：由于插入的记录对其他事务不可见，索引insert undo log record在事务提交后就可以被删除，而update/delete操作产生的update undo log record需要维护MV，只能在purge线程中随后清理。也正是因为这样，分为了两个undo segment，并且这两种undo log record分别称为：insert_undo \u0026amp; update_undo。\nundo log header之后存放实际的undo log record。\n根据产生的undo log就可以将一行记录恢复到之前的版本（trx_undo_prev_version_build），这也是MVCC（或者说非锁定读）的实现过程。\n整个undo log的存储内容如下图所示：\n3.2.3.1 insert undo log record #  insert undo log record通常是指事务在INSERT操作中产生的undo日志。因为insert操作的记录只对事务本身可见，对其他事务不可见（没有历史版本），因此可以在事务提交后直接删除，不需要进行purge操作。\nundo log record的类型为TRX_UNDO_INSERT_REC，只需为rollback做准备，而不需要考虑MVCC，因此只记录行的before image（key fields: col 1\u0026hellip;n len+data）。\ninsert undo log的结构：\nrollback时根据key fields在index中定位record\n3.2.3.2 update undo log record #  update undo log record通常保存的是对DELETE和UPDATE操作产生的undo日志。该undo日志可能需要提供MVCC机制，因此与insert undo log record不同，其不能在事务提交时立即进行删除。当事务提交时，它会放入rollback segment的history链表的头部。然后等待purge。\nupdate undo log record的结构如图所示：\nupdate undo log record相对于insert undo log record要复杂的多，其根据不同的事务操作，会产生不同的update undo log record。primary key是都要存储的，而上图中的最后两部分（update vector、index columns）不一定每个update undo log record都会存储。\nupdate undo log record一共有3种不同的类型：\n TRX_UNDO_UPD_EXIST_REC：更新已经存在的记录 TRX_UNDO_UPD_DEL_REC ：更新已经删除的记录 TRX_UNDO_DEL_MARK_REC：将记录标识为已删除  除了一些\u0026quot;事务信息\u0026quot;，update undo log record主要存储以下3部分的信息：\n 记录的主键值列表（primary key） 发生更新的列（update vector） 索引列  主键值是一定存在的，后面2部分信息取决于操作是否产生这些修改，如果有则记录下来。比如，对于delete操作，只是将记录标记为删除，没有发生更新的列，因此没有第2部分的信息。对于update操作，则需要保存更新前记录的值（upd_t）。\n第3部分保存的是索引列的信息。有两种情况需要保存这部分的信息：\n DELETE MARK操作 更新操作更新了索引列  这样设计的目的是保存这部分信息可以在purge时删除记录所对应的secondary index。\n当一个undo page放不下update undo log record时，会通过将已经产生的部分update undo log record删除（trx_undo_erase_page_end），并以十六进制0xFF进行填充。同时将新的undo page添加到undo segment中，并使用新申请的undo page存放update undo log record。同时，该undo page不可再进行重用。\nTRX_UNDO_UPD_EXIST_REC #  更新现有记录，需要记录update vector和index columns这两部分信息\nTRX_UNDO_DEL_MARK_REC #  因为要支持MVCC多版本，所以需要将DELETE的记录伪删除（更新info_bits），而真正的删除操作由purge线程完成。\n因为没有修改任何列，因此不需要update vector的信息。一般来说，TRX_UNDO_DEL_MARK_REC类型的update undo log record占用的空间最小。\n对于有主键值的更新操作来说，会产生TRX_UNDO_DEL_MARK_REC类型的update undo log record，同样也会产生insert undo log record。这意味着，一个SQL语句可能产生多个undo日志。\nTRX_UNDO_UPD_DEL_REC #  TRX_UNDO_UPD_DEL_REC表示更新已经删除的记录，但是，如果记录真的被删除，由于需要被purge，因此不可以对其进行更新操作。所以，产生该类型的update undo log应该是在同一事务内对记录进行更新操作的（DELETE+INSERT）。\nBEGIN; DELETE FROM t where a = 1; INSERT INTO t values (1, 'm'); 在上面的事务中，首先删除了主键值=1的记录，随后又再次插入了主键值=1的新纪录，这时该事务会产生2个update undo log record。第一个为TRX_UNDO_DEL_MARK_REC，第二个update undo log record需要保存update vector的信息（更新了b列）。另外，由于列b是索引列，因此还需要保存索引列的信息（index columns）。\n从这里可以发现，insert操作不一定都产生insert update undo log，也可能产生TRX_UNDO_UPD_DEL_REC类型的update undo log record。另外，在这个例子中，第二次插入的并非是主键值=1的记录，或者之后插入的记录的字节大小发生了变化，那么就不会产生TRX_UNDO_UPD_DEL_REC的undo日志，其产生的依旧是insert update undo log。\n3.2.4 undo的redo #  在前面已经提到，undo的持久化要通过redo保证，因此undo也有对应的redo log type：\n MLOG_UNDO_INSERT：写入undo log record MLOG_UNDO_ERASE_END：undo log跨undo page时做的抹黑（0xFF） MLOG_UNDO_INIT：undo page初始化 MLOG_UNDO_HDR_DISCARD：废弃undo log header MLOG_UNDO_HDR_REUSE：复用undo log header MLOG_UNDO_HDR_CREATE：创建undo log header  3.3 内存布局 #  在事务子系统中，undo的管理非常重要，我们在上面介绍了物理布局，在内存中，也需要构建其对应的内存对象，来进行undo管理。\n在这里，我们略过undo表空间对象，聚焦在rollback segment和undo log对象上。\nrollback segment内存对象 trx_rseg_t\nstruct trx_rseg_t { ulint id; RsegMutex mutex; ulint space; rollback segment page所在的位置（space+page_no） ulint page_no; page_size_t page_size; 所在表空间的page size ulint max_size; rollback segment可以拥有的最大undo page数 ulint curr_size; 当前undo page数 UT_LIST_BASE_NODE_T(trx_undo_t) update_undo_list; 活跃事务产生的update_undo链表 UT_LIST_BASE_NODE_T(trx_undo_t) update_undo_cached; 之前事务提交后可复用的undo log list（update_undo） UT_LIST_BASE_NODE_T(trx_undo_t) insert_undo_list; 活跃事务产生的insert_undo链表 UT_LIST_BASE_NODE_T(trx_undo_t) insert_undo_cached; 之前事务提交后可复用的undo log list（update_undo） 从trx_undo_t中提取，用于维护history链表的LWN ulint last_page_no; history链表中最后一个没有被purge的undo page no ulint last_offset; 最后一个没有被purge的undo log header offset trx_id_t last_trx_no; 最后一个没有被purge的事务逻辑时间 ibool last_del_marks; 最后一个没有被purge的undo log需要purge ulint trx_ref_count; 活跃事务的引用计数，=0则rollback segment可truncate bool skip_allocation; truncate rollback segment时，trx_sys不分配该段 }; 整体布局如下：\n3.4 undo的一生 #  我们从整个undo的生命周期来完整系统的介绍：\n 分配rollback segment 使用rollback segment 写入undo log 事务提交，事务回滚，MVCC，purge，crash recovery在事务一节介绍  3.4.1 分配rollback segment #  当事务\u0026quot;产生\u0026quot;对数据进行修改时，会相应产生undo log。也就是说，rollback segment在事务第一次\u0026quot;需要\u0026quot;存储undo log时才分配。\n我们知道，事务按读写分为只读事务和读写事务，我们分别介绍这两种事务如何分配rollback segment。\n只读事务 #  涉及到对临时表读写，会从临时表空间分配rollback segment：REG_1 ~ REG_32。（trx_assign_rseg）\n读写事务 #  事务默认都是只读事务，只有当随后判定为读写模式时，才切成读写事务，并分配TrxID和rollback segment（trx_set_rw_mode）。\n分配流程如下：\n 采用round-robin的轮询方式来分配，如果rollback segment被标记为skip_allocation，则跳过 分配后，rseg-\u0026gt;trx_ref_count++ 临时表rseg挂到trx-\u0026gt;rsegs-\u0026gt;m_noredo，普通rseg挂到trx-\u0026gt;rsegs-\u0026gt;m_redo  代码调用链如下：\ntrx_assign_rseg trx_assign_rseg_low(srv_rollback_segments, srv_undo_tablespaces, TRX_RSEG_TYPE_NOREDO) rseg = get_next_noredo_rseg(srv_tmp_undo_logs + 1); trx_set_rw_mode 设置为读写事务 trx_assign_rseg_low(srv_rollback_segments, srv_undo_tablespaces, TRX_RSEG_TYPE_REDO) rseg = get_next_redo_rseg(max_undo_logs, n_tablespaces); 3.4.2 使用rollback segment #  流程如下（trx_undo_report_row_operation）：\n 当前变更如果是临时表（dict_table_is_temporary），则使用临时表rseg（trx-\u0026gt;rsegs.m_noredo）；否则使用普通rseg（trx-\u0026gt;rsegs.m_redo） 判断变更类型  insert_undo，单独分配undo slot update_undo，单独分配undo slot    分配undo slot（undo segment）（trx_undo_assign_undo）\n 首先复用rseg上的cached undo list，  insert_undo：修改seg_hdr（改为active）和page_hdr（start, free），预留XID位置 update_undo：在undo segment page创建undo log header，预留XID位置   初始化trx_undo（设为active） 如果没有cached undo list，从rollback segment分配一个undo slot，并初始化undo page（如果1024分配完了，返回DB_TOO_MANY_CONCURRENT_TRXS错误） 将trx_undo加入active undo list 如果是DDL事务，还要记录元信息（TRX_UNDO_DICT_TRANS=true \u0026amp;\u0026amp; table_id）  这里留给读者一个问题：在#1中对于update_undo，为什么要在undo segment page中创建undo log header  3.4.3 写入undo log record #  写入undo log record流程如下（trx_undo_report_row_operation）：\n 选择undo page（undo-\u0026gt;last_page_no） 按照insert_undo/update_undo进行插入记录 如果写入过程中空间不足，则将已写部分涂黑0xFF（trx_undo_erase_page_end），然后申请新的undo page重新写入 构建rollback pointer，并更新到clustered index record中  4 事务 #  大多数数据库采用STEAL+NO-FORCE的方式，在事务推进国产中，不断写入undo log，然后在事务提交时，保证redo log持久化到磁盘。这样，当发生被动故障时，就可以通过crash recovery将数据恢复到一致的状态。\n4.1 rollback #  4.1.1 rollback pointer #  在record一章我们介绍过，行记录通过roll_ptr将行的历史变化串联起来，实现MV。\n在MVCC中，通过当前行不断apply undo log record，来构造出之前的版本。\nrollback pointer指针占用7个字节：undo segment no需要1个字节，且最高位保存undo log type；undo page需要4个字节；undo log需要2个字节。因此可以通过rseg_id+page_no+offset构造出rollback pointer（trx_undo_build_roll_ptr），也可反解（trx_undo_decode_roll_ptr）。\n4.1.2 回滚操作 #  根据回滚操作的发起者来分类，可以分为用户态的回滚和内核态的回滚。\n用户态的回滚指的是用户通过rollback命令发起。\n内核态的回滚指的是InnoDB内部发起的回滚，一共有两种：\n 仅回滚事务最近的一个语句：比如违反唯一性约束，那么仅回滚最近的一个SQL语句，事务的状态依然是活跃的。 回滚整个事务，和用户态的回滚一样：比如发生死锁时回滚victim的事务。  InnoDB内部定义了三种回滚类型\n TRX_SIG_TOTAL_ROLLBACK：回滚整个事务 TRX_SIG_ROLLBACK_TO_SAVEPT：回滚到最近一个保存点（InnoDB支持保存点的事务，并且，保存点事务也是基于undo实现的（trx_t::undo_no，每次事务写入undo++）） TRX_SIG_ERROR_OCCURRED：当错误发生时进行回滚（没有找到触发的事件）  在进行回滚操作时，InnoDB主要做两件事情：\n 根据trx_id找到undo log record（从undo segment page中根据当前事务的trx_id找到对应的undo log header） 根据undo log record顺序找到最后一个undo log record，逆序回滚记录  若回滚操作进行的是TRX_SIG_TOTAL_ROLLBACK，即重复上述两个操作直到事务中所有的undo log record都已经被回滚。回滚时根据undo log record进行逆序操作（trx_roll_pop_top_rec_of_trx）。当回滚整个事务完成之后，undo log record的空间被释放，可以供之后的事务存放undo log，但需要特别注意的是：回滚后undo log header所占用的空间是不被释放的，但是这并不影响purge。\n当取得undo log record后，需要根据不同类型的undo log record进行处理。\n insert undo log record：进行逆操作DELETE即可，首先删除secondary index中的记录，在删除clustered index中的记录（调用btr_cur_optimistic_delete或btr_cur_optimistic_optimistic，而不是像redo log recovery哪有直接对physical page进行直接操作）。DELETE操作是真正的删除操作，而不是DELETE MARK的伪删除。这是因为事务并没有提交，所以记录对所有其他事务是不可见的，因此可以立即删除，不需要等purge线程来清理。此外，回滚是一个逻辑操作，这和redo log不同。 update undo log record：三种类型，回滚因人而异。但一样是是逻辑操作，处理顺序也是secondary index→clustered index（row_undo_mod）  从update_undo中解析出update vector回退seconary index：去除delete mark标记，或者用update vector中的diff信息修改成之前的值（row_undo_mod_del_unmark_sec_and_undo_update） 同样用update vector中的diff信息修改clustered index（row_undo_mod_clust）    总结一下，逆向操作可以分为以下几种：\n delete mark = 1改为0 in-place更新直接改成旧值 对于插入操作，依次删除seoncdary index和clustered index  row_undo_ins_remove_sec_rec row_undo_ins_remove_clust_rec    当回滚处理完最后一个undo log record后，释放之前undo log record所占用的空间（trx_undo_truncate_end），但是并不回收undo log header。\n回滚同样需要提交，因为与正常提交一样，需要释放事务所持有的资源：undo segment、lock、read view，并flush redo log buffer。\n4.2 commit #  事务提交分为两种：隐式提交和显式提交。\n当执行DDL，或者采用事务控制语句（BEGIN，START TRANSACTION时），为隐式提交。\n显式提交，发送COMMIT。\n另外，MySQL因为架构分为server层和engine层，为了保证两层日志的一致性，\n事务提交时，为了和server层的binlog做XA，InnoDB的commit分为2个阶段（2PC）：\n prepare：  将事务状态从active-\u0026gt;prepared，即将所有undo slot中的undo segment page的seg_hdr.TRX_UNDO_STATE=TRX_UNDO_PREPARED（trx_undo_set_state_at_prepare） 填充XID（执行事务的第一条SQL时，就会注册XA，并根据thd-\u0026gt;query_id分配XID）（trans_register_ha）   commit：  将所有undo slot设为完成态（CACHED/TO_FREE/TO_PURGE）  如果undo segment iff 1 undo page \u0026amp;\u0026amp; 使用量小于3/4，复用该undo segment（将该undo segment状态设为TRX_UNDO_CACHE） 含有insert undo log record，事务提交后快速free掉（将该undo segment状态设为TRX_UNDO_TO_FREE） 含有update undo log record，undo segment交由purge free（将该undo segment状态设为TRX_UNDO_TO_PURGE）   将rseg加入purge队列（purge_sys-\u0026gt;purge_queue） 更新rseg-\u0026gt;last_*四元组 串到cached undo list（insert_undo知道事务释放完全部资源才free） rseg-\u0026gt;trx_ref_count\u0026ndash; 如果开启了binlog（trx-\u0026gt;mysql_log_file_name != NULL \u0026amp;\u0026amp; trx-\u0026gt;mysql_log_file_name[0] != \u0026lsquo;\\0\u0026rsquo;），在trx_sys page写入binlog信息 清理read view对象 flush redo log buffer（WAL） 将trx从事务列表中删除    redo log中并没有一个\u0026quot;特殊\u0026quot;的redo log record表示事务结束：事务结束的标志是事务的undo log放入rollback segment的history链表。这个操作将redo log写入mtr中，mtr.commit后在事务提交时flush redo log buffer完成。\n调用链如下：\ntrx_commit_low trx_write_serialisation_history trx_undo_set_state_at_finish 这里留给读者2个思考：\n为什么不直接设置一个特殊的redo log标识事务结束？\n为什么可以事务结束的标识放到history链表中？\n 4.3 MVCC #  从前面的undo chain图上可以很直观的看到，undo记录包含了行数据的历史版本，因此，MVCC可以：\n 根据rollback pointer可以回溯undo log，进而可以构建出那个版本的数据 根据trx_id判断可见性  简单来说就是对于RC及以上隔离级别（RU直接读取当前行记录），当前事务只能看到其他已经提交的事务的数据，不能看到active事务修改中的数据。\n因为trx_id是单调递增的，因此，我们可以在一个线性序列上依托于trx_id构建可见性，因此：\n trx_id：事务开始的逻辑时间，在事务开始时分配 trx_no：事务结束的逻辑时间，在事务提交commit阶段分配，这个用于构建purge的可见性 trx_sys-\u0026gt;rw_trx_ids：当前active的读写事务列表，开启读写事务时加入，读写事务提交时移除  以及基于以上构建readview\n读写异常是因为本事务的R和其他事务的R+W有交集，MVCC也是本着这个本质来解决问题的，解决问题的核心是定序。\n只读事务的trx_id始终为0（不分配），读写事务分配trx_id，另外，readview都分配，\n4.3.1 readview #  readview的生命周期：\n 分配：在显式start transaction with consistent snapshot或RR时，事务开始就分配（在事务期间不变，保证可见性不变）；其他情况读取数据时才分配（trx_assign_read_view） 回收：事务commit后，清理资源阶段进行回收（view_close）  全局的readview通过一个readview链表维护（trx_sys-\u0026gt;mvcc-\u0026gt;readview-\u0026gt;m_views），在头部插入新的readview，因此，链表中最后一个还没有close的readview就是oldest readview（这是purge的基础）。\nreadview由以下几部分构成：\n m_low_limit_id：高水位，分配时取trx_sys::max_trx_id，也就是取当前还没有被分配的事务ID m_up_limit_id：低水位，如果m_ids不为空，取其最小值，否则取trx_sys::max_trx_id m_ids：当前活跃事务列表（不包括自己） m_creator_trx_id：本事务（自己）  本质就是读取时获得一个快照版本，active的，自己的可见，别人的不可见；（高水位）未来的不可见，（低水位）过去的可见。\n活跃事务：\n trx_id \u0026gt; read_view::m_low_limit_id read_view::m_up_limit_id \u0026lt; trx_id \u0026lt; read_view::m_low_limit_id，并且 trx_id 属于 trx_t::read_view::m_ids  已提交事务：\n trx id \u0026lt; read_view::m_up_limit_id read_view::m_up_limit_id \u0026lt; trx id \u0026lt; read_view::m_low_limit_id，并且 trx id 不属于 trx_t::read_view::m_ids  4.3.2 数据可见性 #  事务为T，trx_id为记录R中的DATA_TRX_ID：\n trx_id \u0026gt; 高水位，T 不可见 R trx_id \u0026lt; 低水位，T 可见 R 在高低之间，如果是自己，T可见R，否则不可见  比如下图示例：\n这里要理解insert_undo为什么事务提交后可以抛弃，readview的遍历方式。\n4.3.3 构建版本 #  因为在clustered index的行上存储了trx_id和roll_ptr，因此可以通过trx_id判断可见性，如果不可见，则通过roll_ptr继续向前构建，直至可见或者达到尾部。（row_vers_build_for_consistent_read）。\n这也是线性系统的一种体现。\n因为构建MV需要持有page latch，如果active事务很多，则undo chain可能会很长，要注意latch的持有时长。\nundo log record如果是insert_undo，不用查看undo，因为未提取事务的新插入记录，对其他事务一定不可见。\n对于secondary index记录，如果page上的最大事务ID（PAGE_MAX_TRX_ID）对当前事务是可见的，也无需构建MV，否则还要去走clustered index record构建。\n 4.4 purge #  purge操作负责清理已提交的不再被使用的数据，包括记录和日志。\n清理这些数据依托于undo log，更进一步，依托于构建在提交序上的点位（trx_no），再结合SQL graph，展开实际的清理。\n4.4.1 清理操作 #  purge清理属于事后打扫，这一方面保证forward fast，另一方面为了MVCC的GC，因此，清理工作包括：\n 清理记录：清理delete mark的行记录或者secondary index record（update undo log record中标记修改了secondary index相应值） 清理undo log：清理undo log，如果undo page中的所有undo log都清理后，则删除对应的undo segment。在清理undo log时，需要判断当前是否有其他事务正在进行MVCC。  undo有几个序：\n undo申请序：事务在进行DML操作时申请undo page用于存放undo log，在一个undo segment中一个事务会占用1\u0026hellip;n个undo page，但在undo segment是无序的。 undo使用序：undo page可以复用，存储不同事务的undo log，在page维度的存储序是committed-committed-active。 提交序：history链表标识已提交事务序，这可以关联到事务中的undo log序，这个undo log序分布在同一个undo segment中的不同undo page中，通过undo log header串联，history和各个undo log header串联起来已提交序。  总结一下：\n undo segment整体是无序的 undo page按事务状态排序 提交序离散分布在rollback segment中的各个undo page中  purge的执行序列通过构建SQL execution graph来实现。\n这里注意：\n 事务的提交序和执行序不一样，因此在purge undo和purge记录时有依赖关系，需要构建graph  并且，在进行purge操作时，会产生大量的随机读，这可能会产生性能问题。另外，如果出现了长事务正在使用mvcc（比如最新的active事务），则会导致history链表最头部的已提交事务的undo log的回收无法进行。\n4.4.2 purge实现 #  purge_sys全局对象（trx_purge_t）保存当前purge的位置和信息。\npurge流程如下：\n  通过purge point确认可见性（purge_sys::view）\n通过clone oldest active readview（clone_oldest_view），来保证purge point之前的事务变更都是可以清理的\n具体解释一下，在事务子系统中，committing事务保存在serialisation_list中，并按照提交序排列（trx_no）。在事务prepare时加入，事务commit时移除\n并且，在readview中还会维护一个serialisation_list LWN（read_view::m_low_limit_no：取自serialisation_list trx_no最小值），其含义是如果其他事务的trx_no比其小，一定已提交。 这样，一个事务的readview的LWN就确定了，那么全局的LWN即为所有readview的最小点（trx_no，直接按提交序倒排第一个即是），这个点即为purge point\n  从各个rollback segment中的history链表中依次（从后）读取最多300个undo log（trx_purge_fetch_next_rec递归），分配到各个purge线程的工作队列中（vector thr-\u0026gt;child-\u0026gt;undo_recs）\n这些history链表已经在事务提交时加入puge_sys::purge_queue中了，即可以将该queue理解为以trx_no为key的优先级队列\n因此，purge_sys::view和所有histroy链表已经存在交集。\n因此这里可以理解成两层关系，purge_queue维护了提交序，负责推进purge，其来源来自于rollback segment的history链表，。\n  实际purge（细节见下）：分为undo purge和undo truncate\n  清理history list（每128）\ninsert_undo的undo segment事务提交后就释放，update undo在做完history后，也到了释放时机。但因为undo page的复用，一个undo segment是离散分布的，所以需要全局推进。因此，每做完128次purge后，进行一次purge history链表。\n从history链表中遍历释放undo segment，因为history是按照提交序排列的，所以遇到一个还未purge的undo log（trx_no比当前purge point大）则停止。\n  第三步的purge函数调用栈如下：\nrow_purge_step // purge single undo log record(全部清理完成返回node，即purge线程) row_purge // fetch \u0026amp; purge (300),从purge_queue取第一个rseg，从history链表中取最老的还还没有purge的undo log header，一次读取该事务的undo log record，依次往复，直到300或没有 row_purge_parse_undo_rec // parse update_undo row_purge_record // undo记录（secondary-\u0026gt;clustered） row_purge_del_mark 1. delete mark 1-\u0026gt;0 物理删除所有涉及的记录 undo purge row_purge_upd_exist_or_extern 2. clustered index in-place update, 可能更新secondary index（delete+insert），清理, undo truncate 当一个undo segment段中的最后一个undo log被清理完毕后，则将该undo segment删除（trx_purge_free_segment）。这里有两个细节需要特别注意：\n  如果一个undo segment包含多个undo page时，当进行undo slot的回收时，首先删除最后一个undo页，并进行mtr_commit。\n如果在这时，即undo segment的最后一个undo log所在的最后一个undo page从段中被删除时，数据库crash，那么重启后，重新进行purge时，会导致最后一个undo log的最后一部分undo log record可能会出现不一致的问题（因为回收的undo page可能已经分配给其他段使用），这时，再进行purge操作会导致错误发生。因此，InnoDB在设计时对每个undo log header，设置了一个TRX_UNDO_DEL_MARKS标记。当最后一个undo log删除时，首先将其置为true，那么crash recovery而重新purge时，由于该变量已置为true，所以不需要继续进行purge操作\n  rollback segment的history链表的更新与undo segment的删除需要在一个mtr中。否则，当数据库宕机时，可能发生rollback segment的history链表已经将undo log删除，而undo segment还存在的情况。在这种情况下，undo segment将没有机会被删除，却需要一直占用存储空间。\n  purge_is_running用于保护DROP TABLE的操作，当进行purge undo日志时，需要持有该对象的x-latch；当需要进行DROP TABLE时，需要持有该对象的s-latch，以此保证当进行purge操作时，表不会被删除。\nlatch用于保证删除undo日志的正确性，保证删除时没有其他事务正在引用该undo日志。当开启purge操作时，首先需要持有该x-latch，然后通过read_view_oldest_copy_or_open_new判断哪些undo日志可以被清理（注意：purge是一个特殊的事务，事务类型为TRX_PURGE，其他都是用户事务）。当用户事务需要通过undo日志进行多版本并发控制（一致性非锁定读）时，需要首先获得该对象的s-latch。\n4.5 crash recovery #  crash recovery的恢复分为两个阶段：\n  forward，恢复到内存态，即将redo log从checkpoint到最新，进行apply，将磁盘上的数据恢复到crash时的内存态，其中也包括undo log。\n  backward：因为buffer pool的STEAL+NO-FORCE，需要将active STEAL的数据rollback掉，即找出活跃事务，将其rollback，也称为failure atomic。\n具体来讲，遍历所有rollback segment，读取其undo segment中的undo segment page中TRX_UNDO_STATE，可以得知其事务状态（还记得前面说过的吗，一个undo segment只会最多有一个active transaction），如果为活跃事务，则需要遍历该事务的undo来rollback，以及构建出事务子系统的内存布局：trx_sys，trx_t，trx_rseg_t和trx_undo_t\n  这里还需要考虑XA事务，即保证server层的binlog和InnoDB数据的一致，通过2PC我们可以知道，只要过了commit point，就可以forward。这样，我们就可以通过判断binlog是否已经记录（扫描最后一个binlog文件，拿binlog的XID和InnoDB的XID做比对，如果binlog有，则提交InnoDB事务，否则回滚InnoDB事务）。\n总结一下，数据库系统整体的恢复节奏如下：\n redo recovery 数据字典子系统初始化 事务子系统初始化\u0026amp;重建 XA事务恢复  redo recovery详细的函数调用栈如下：\ninnobase_start_or_create_for_mysql recv_sys_var_init(); recv_sys_create(); malloc recv_sys create mutex/writer_mutex recv_sys_init(buf_pool_get_curr_size()); 创建一个MEM_HEAP_FOR_RECV_SYS的heap 用于存放 log records和file create flush_start/flush_end event recovery时在buffer pool instance中预留好的frame数量 512 recv_n_pool_free_frames malloc recv_sys-\u0026gt;buf (parser buffer) 2M malloc recv_sys-\u0026gt;addr_hash 大小为 buffer_pool_size/512 recv_sys-\u0026gt;last_block_buf_start = 2 * 512 malloc recv_sys-\u0026gt;dblwr recv_recovery_from_checkpoint_start(flushed_lsn); recv_sys-\u0026gt;dblwr.pages.clear(); recv_apply_hashed_log_recs(TRUE);\t从log records hash table应用到page 在buffer pool中， recv_recover_page -\u0026gt; recv_parse_or_apply_log_rec_body 不在内存中，recv_read_in_area buf_read_recv_pages buf_read_page_low buf_page_io_complete recv_recovery_from_checkpoint_finish(); recv_recovery_from_checkpoint_start 创建buffer pool instances中的flush list（红黑树）buf_flush_init_flush_rbt 在redo log file group中查找最新的CP的file和其CP位置 recv_find_max_checkpoint 从file的CP位置读取CP信息放入 log_sys-\u0026gt;checkpoint_buf 从CP信息中解析出 checkpoint_lsn checkpoint_no contiguous_lsn = checkpoint_lsn 从contiguous_lsn开始扫描，把redo log放入hash table recv_group_scan_log_recs ... recv_synchronize_groups();\t更新group-\u0026gt;lsn, lsn_offset，做checkpoint 更新log_sys-\u0026gt;点位 recv_group_scan_log_recs end_lsn = contiguous_lsn = 512对齐后的checkpoint_lsn do { start_lsn = end_lsn end_lsn += 4 * 16K = 128 log block log_group_read_log_seg(log_sys-\u0026gt;buf, group, start_lsn, end_lsn);\t从redo log file中将start_lsn - end_lsn的数据读入log_sys-\u0026gt;buf } while (!recv_scan_log_recs( available_mem,\tpage_size * (buffer pool总page-512*instance) \u0026amp;store_to_hash,\tSTORE_YES / STORE_IF_EXISTS redo log record是否需要存在hash table里 log_sys-\u0026gt;buf, RECV_SCAN_SIZE,\t128 log block checkpoint_lsn, start_lsn, contiguous_lsn, \u0026amp;group-\u0026gt;scanned_lsn) recv_scan_log_recs\t解析128个log block Scans log from a buffer and stores new log data to the parsing buffer. recv_sys_add_to_parsing_buf\t处理每个log block，一直处理完128 log block，拷贝到recv_sys-\u0026gt;buf Adds data from a new log block to the parsing buffer of recv_sys recv_parse_log_recs\t处理recv_sys-\u0026gt;buf中的mtr record single page/ recv_parse_log_rec\tparse a single log record recv_add_to_hash_table\t将mtr record放入hash table，元素为recv_t，规则为space_id,page_no multiple page loop in recv_parse_log_rec （多次） loop in recv_parse_log_rec + recv_add_to_hash_table recv_parse_or_apply_log_rec_body\t在recv_parse_log_rec中只分析 从log records hash table应用到page recv_sys_justify_left_parsing_buf "}]